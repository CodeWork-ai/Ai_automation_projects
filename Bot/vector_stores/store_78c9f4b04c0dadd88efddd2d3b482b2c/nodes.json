[
  {
    "text": "=== PAGE: moonshotai/Kimi-K2-Instruct-0905 ¬∑ Hugging Face (https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905) === moonshotai/Kimi-K2-Instruct-0905 ¬∑ Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up moonshotai / Kimi-K2-Instruct-0905 like 297 Follow Moonshot AI 2.91k Text Generation Transformers Safetensors kimi_k2 conversational custom_code fp8 License: modified-mit Model card Files Files and versions xet Community 5 Train Deploy Use this model 1. Model Introduction Key Features 2. Model Summary 3. Evaluation Results 4. Deployment 5. Model Usage Chat Completion Tool Calling 6. License 7. Third Party Notices 7. Contact Us üì∞ Tech Blog | üìÑ Paper 1. Model Introduction Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2. It is a state-of-the-art mixture-of-experts (MoE) language model, featuring 32 billion activated parameters and a total of 1 trillion parameters. Key Features Enhanced agentic coding intelligence: Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks. Improved frontend coding experience: Kimi K2-Instruct-0905 offers advancements in both the aesthetics and practicality of frontend programming. Extended context length: Kimi K2-Instruct-0905‚Äôs context window has been increased from 128k to 256k tokens, providing better support for long-horizon tasks. 2.",
    "id": "c0c4429c-131c-426c-89df-4ad1752821c7",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Improved frontend coding experience: Kimi K2-Instruct-0905 offers advancements in both the aesthetics and practicality of frontend programming. Extended context length: Kimi K2-Instruct-0905‚Äôs context window has been increased from 128k to 256k tokens, providing better support for long-horizon tasks. 2. Model Summary Architecture Mixture-of-Experts (MoE) Total Parameters 1T Activated Parameters 32B Number of Layers (Dense layer included) 61 Number of Dense Layers 1 Attention Hidden Dimension 7168 MoE Hidden Dimension (per Expert) 2048 Number of Attention Heads 64 Number of Experts 384 Selected Experts per Token 8 Number of Shared Experts 1 Vocabulary Size 160K Context Length 256K Attention Mechanism MLA Activation Function SwiGLU 3. Evaluation Results Benchmark Metric K2-Instruct-0905 K2-Instruct-0711 Qwen3-Coder-480B-A35B-Instruct GLM-4.5 DeepSeek-V3.1 Claude-Sonnet-4 Claude-Opus-4 SWE-Bench verified ACC 69.2 ¬± 0.63 65.8 69.6 64.2 66.0 72.7 72.5 SWE-Bench Multilingual ACC 55.9 ¬± 0.72 47.3 54.7 52.7 54.5 53.3 - Multi-SWE-Bench ACC 33.5 ¬± 0.28 31.3 32.7 31.7 29.0 35.7 - Terminal-Bench ACC 44.5 ¬± 2.03 37.5 37.5 39.9 31.3 36.4 43.2 SWE-Dev ACC 66.6 ¬± 0.72 61.9 64.7 63.2 53.3 67.1 - All K2-Instruct-0905 numbers are reported as mean ¬± std over five independent, full-test-set runs.",
    "id": "2ecede74-859d-4fa7-9636-38f5080afda2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Before each run we prune the repository so that every Git object unreachable from the target commit disappears; this guarantees the agent sees only the code that would legitimately be available at that point in history. Except for Terminal-Bench (Terminus-2), every result was produced with our in-house evaluation harness. The harness is derived from SWE-agent, but we clamp the context windows of the Bash and Edit tools and rewrite the system prompt to match the task semantics. All baseline figures denoted with an asterisk () are excerpted directly from their official report or public leaderboard; the remaining metrics were evaluated by us under conditions identical to those used for K2-Instruct-0905. For SWE-Dev we go one step further: we overwrite the original repository files and delete any test file that exercises the functions the agent is expected to generate, eliminating any indirect hints about the desired implementation. 4. Deployment You can access Kimi K2's API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you. The Anthropic-compatible API maps temperature by real_temperature = request_temperature  0.6 for better compatible with existing applications. Our model checkpoints are stored in the block-fp8 format, you can find it on Huggingface. Currently, Kimi-K2 is recommended to run on the following inference engines: vLLM SGLang KTransformers TensorRT-LLM Deployment examples for vLLM and SGLang can be found in the Model Deployment Guide. 5. Model Usage Chat Completion Once the local inference service is up, you can interact with it through the chat endpoint: def simple_chat(client: OpenAI, model_name: str): messages =  for field status updates and consider signing up for text alerts for the latest information. Conclusion: Don't miss this fantastic opportunity for your child to engage in soccer training with experienced coaches in a supportive and enjoyable environment. With its focus on technical skills and fun, our Friday open training sessions are the perfect way for young soccer enthusiasts to develop their passion for the game.",
    "id": "196c9055-9aa9-49b3-ab2a-a2b40f391895",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Conclusion: Don't miss this fantastic opportunity for your child to engage in soccer training with experienced coaches in a supportive and enjoyable environment. With its focus on technical skills and fun, our Friday open training sessions are the perfect way for young soccer enthusiasts to develop their passion for the game. Register now and join us every Friday for an exciting experience! Tim Mattson, a 2002 alumnus of Evergreen, has made a significant mark in the entertainment industry with his company, Sweet Victory Entertainment, LLC. Founded in September 2008, Sweet Victory Entertainment is a film and video production company that encompasses all aspects of entertainment, from big-screen productions to internet content. With its headquarters located at 181 Broadway, New York, NY, the company has established itself as a key player in the industry. At the heart of Tim's success is his experience at Evergreen, which played a pivotal role in preparing him for his professional life. Evergreen's unique approach to learning, which empowers students to take charge of their educational journey, proved to be instrumental in Tim's development as an entrepreneur. The institution's emphasis on self-directed learning allowed Tim to explore his interests in media and entertainment deeply, fostering a sense of autonomy and responsibility that would later become essential in his career. Evergreen's facilities and tools were another critical factor in Tim's preparation. The institution offers state-of-the-art equipment and resources that enable media students to experiment with and produce high-quality content. For Tim, having access to these resources was a game-changer. It allowed him to bring his ideas to life, learn from his mistakes, and develop the practical skills necessary to succeed in the competitive entertainment industry. One of the most valuable skills Tim acquired during his time at Evergreen was the ability to adapt and innovate. The self-directed learning environment encouraged him to think creatively and approach problems from different angles, skills that have been indispensable in the fast-paced world of entertainment.",
    "id": "686a4eda-8bd6-4417-a5da-bdf66e6b0e7e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "For Tim, having access to these resources was a game-changer. It allowed him to bring his ideas to life, learn from his mistakes, and develop the practical skills necessary to succeed in the competitive entertainment industry. One of the most valuable skills Tim acquired during his time at Evergreen was the ability to adapt and innovate. The self-directed learning environment encouraged him to think creatively and approach problems from different angles, skills that have been indispensable in the fast-paced world of entertainment. Moreover, the experience of working on various projects and collaborating with peers from diverse backgrounds helped Tim develop strong communication and project management skills, which are crucial for managing a production company. In reflecting on his time at Evergreen, Tim notes that the institution's learning process mirrors the real world, where success is largely dependent on one's ability to push themselves and seize opportunities. Evergreen provided Tim with the ideal environment to hone his skills, explore his passion for entertainment, and build a strong foundation for his future endeavors. Today, as the founder of Sweet Victory Entertainment, LLC, Tim embodies the spirit of innovation and entrepreneurship that Evergreen fosters. His journey is a testament to the power of self-directed learning and the impact that the right educational environment can have on one's professional trajectory. By combining a passion for entertainment with the skills and mindset gained at Evergreen, Tim has achieved success in his field and continues to push the boundaries of what is possible in the world of film and video production. Understanding Evolution: The Effects of Mutations Mutations are a fundamental driver of evolutionary change, and understanding their effects is crucial for comprehending the evolutionary process. A single germ line mutation can have a range of effects on an organism's phenotype, which can be categorized into three main types: no change, small change, and big change. No Change in Phenotype Some mutations don't have any noticeable effect on the phenotype of an organism. This can happen in several situations: perhaps the mutation occurs in a stretch of DNA with no function, or perhaps the mutation occurs in a protein-coding region, but ends up not affecting the amino acid sequence of the protein.",
    "id": "864cd06a-d526-44ae-ba7f-51b4f74d8280",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "No Change in Phenotype Some mutations don't have any noticeable effect on the phenotype of an organism. This can happen in several situations: perhaps the mutation occurs in a stretch of DNA with no function, or perhaps the mutation occurs in a protein-coding region, but ends up not affecting the amino acid sequence of the protein. For example, a mutation in a non-coding region of DNA may not have any noticeable effect on the organism's phenotype. Small Change in Phenotype A single mutation can cause a small change in an organism's phenotype. For instance, a cat with a curled-ear mutation has ears that curl backwards slightly due to a single mutation. This type of mutation can have a noticeable effect on the organism's appearance or physiology, but it may not have a significant impact on its overall fitness or survival. Big Change in Phenotype A single mutation can also cause a big change in an organism's phenotype. This type of mutation can have a significant impact on the organism's appearance, physiology, or behavior, and may affect its overall fitness or survival. For example, a mutation that affects the development of an organism's limbs or sensory organs can have a profound impact on its ability to survive and reproduce. The molecular mechanisms underlying each type of effect are complex and involve multiple factors. For example, a mutation in a protein-coding region can affect the structure and function of the resulting protein, which can in turn affect the organism's phenotype. Additionally, environmental factors can influence the effects of a mutation, and may interact with the mutation to produce a particular phenotype. In conclusion, understanding the effects of mutations is essential for comprehending the evolutionary process. By categorizing the effects of a single germ line mutation into three main types ‚Äì no change, small change, and big change ‚Äì we can better understand the complex relationships between mutations and phenotypic changes. Further research and exploration of the molecular mechanisms underlying each type of effect can provide valuable insights into the evolutionary process and the diversity of life on Earth.",
    "id": "94548ae5-6ad6-478b-a229-78bcb89dc456",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "In conclusion, understanding the effects of mutations is essential for comprehending the evolutionary process. By categorizing the effects of a single germ line mutation into three main types ‚Äì no change, small change, and big change ‚Äì we can better understand the complex relationships between mutations and phenotypic changes. Further research and exploration of the molecular mechanisms underlying each type of effect can provide valuable insights into the evolutionary process and the diversity of life on Earth. As a seasoned software engineer with over 10 years of experience, Fabien Warniez has developed a strong expertise in designing and implementing web-based applications in PHP, .Net, and Java, as well as iOS applications. With a solid background in computer engineering and a passion for innovation, Fabien has consistently delivered high-quality solutions that meet and exceed expectations. Professional Experience Fabien's professional experience is characterized by a series of challenging and rewarding roles that have allowed him to grow and develop as a software engineer. Some of his notable achievements include:  Software Engineer ‚Äî Backend, Electronic Arts, Inc.: Fabien designed and delivered the backend of all major features on the site for Star Wars Battlefront, Mirror's Edge: Catalyst, and The Sims 4. He also implemented a tagging system for news articles for Dragon Age: Inquisition. His work on these projects demonstrated his ability to work on large-scale applications and deliver high-quality results under tight deadlines.  Software Developer, Team Lead, Atimi Software: Fabien led the development of new features for a major financial software company's flagship iOS application and implemented new features and bug fixes for the iOS version of their popular digital magazine. He also started implementing an improved version of the iOS framework for Windows Phone 8 before the project's funding was cut.  Software Development Lead, Overinteractive Media / dimeRocker: Fabien led the development of the Film Accelerator for CineCoup and built a multi-million row social data harvesting platform for Social Farm. He also led the development of a social platform to distribute grants for local community TV programming for TELUS STORYHIVE.",
    "id": "d9f76eb1-deba-4d69-95b0-207e399e7776",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "He also started implementing an improved version of the iOS framework for Windows Phone 8 before the project's funding was cut.  Software Development Lead, Overinteractive Media / dimeRocker: Fabien led the development of the Film Accelerator for CineCoup and built a multi-million row social data harvesting platform for Social Farm. He also led the development of a social platform to distribute grants for local community TV programming for TELUS STORYHIVE. Education Fabien holds a Master of Science in Computer Engineering from EPITA, Paris, France, where he developed a range of skills, including programming languages, database design, and project management. He also completed a Baccalaur√©at S (Sciences) with honors and is an Adobe Certified Expert in Flex with AIR. Skills Fabien is proficient in a range of technologies, including PHP, Symfony, Objective-C, Swift, Java, Play!, MySQL, C, ASP.Net MVC, Microsoft SQL Server, HTML/CSS, JavaScript, and Git. He is also interested in functional programming, big data, NoSQL, Swift, and Scala. Personal Projects In his free time, Fabien enjoys working on personal projects, such as his iOS app Vie, a take on the popular Conway Game of Life, and his community website TopButPES.com, where fans of Pro Evolution Soccer can watch, share, and vote for their favorite videos. He is also a contributor to Twitterizer, a .Net client library for the Twitter API. Overall, Fabien Warniez is a highly skilled and experienced software engineer with a strong passion for innovation and a commitment to delivering high-quality solutions. His expertise in web-based applications, iOS development, and software engineering makes him a valuable asset to any organization. Introduction to Prime Legionaries Prime Legionaries are elite members of the Legion, a major faction in the Fallout: New Vegas game, set in the Mojave Wasteland in 2281. They are distinguished by their superior training and equipment compared to their recruit counterparts, marking them as a formidable force within the Legion's infantry.",
    "id": "6a4f776f-da4b-4725-b8d1-eeb224392106",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "His expertise in web-based applications, iOS development, and software engineering makes him a valuable asset to any organization. Introduction to Prime Legionaries Prime Legionaries are elite members of the Legion, a major faction in the Fallout: New Vegas game, set in the Mojave Wasteland in 2281. They are distinguished by their superior training and equipment compared to their recruit counterparts, marking them as a formidable force within the Legion's infantry. Characteristics of Prime Legionaries Prime Legionaries are easily recognizable due to their distinctive crimson-colored armor and unique visor that extends over their eyes as part of their covered helmets. This attire not only signifies their elite status but also provides them with enhanced protection in combat. They are primarily equipped with hunting rifles, although they can occasionally be found wielding more advanced weaponry such as Marksman carbines, particularly in key locations like Hoover Dam or as part of specialized squads like the Legionary Assassins. Interactions with the Player Character Prime Legionaries play a significant role in the game's world, often encountered in Legion-controlled areas such as The Fort, Cottonwood Cove, and Nelson. They usually travel in squads that may include higher-ranking Legion members, such as Veteran Legionaries or Decani. While they do not offer services like trading or repair, interactions with them can be pivotal in understanding the Legion's dynamics and motivations within the game's narrative. Behind the Scenes: Historical Inspiration The concept of Prime Legionaries draws inspiration from the Roman Republic's military structure, specifically the Principes. The Principes were more experienced and better equipped than the Hastati, forming the second line in the Roman legion's formation. This historical backdrop adds depth to the game's world-building, reflecting the developers' attention to detail in creating immersive and believable factions within the Fallout universe. Conclusion Prime Legionaries are a fascinating aspect of Fallout: New Vegas, offering insights into the game's rich lore and complex faction dynamics. Through their distinctive appearance, advanced equipment, and integral role in the Legion's structure, they embody the elite infantry of the Caesar's Legion.",
    "id": "699b0881-2a38-4bde-9beb-6f0c3e59e722",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "This historical backdrop adds depth to the game's world-building, reflecting the developers' attention to detail in creating immersive and believable factions within the Fallout universe. Conclusion Prime Legionaries are a fascinating aspect of Fallout: New Vegas, offering insights into the game's rich lore and complex faction dynamics. Through their distinctive appearance, advanced equipment, and integral role in the Legion's structure, they embody the elite infantry of the Caesar's Legion. Understanding Prime Legionaries enhances the overall gaming experience, providing players with a deeper appreciation for the game's narrative and the strategic considerations involved in navigating the Mojave Wasteland. The concept of being \"governed by imperatives\" is a fascinating one, and it's something that I've been thinking about a lot lately. For me, imperatives are those things that I feel like I must know, do, or have in order to live a fulfilling and meaningful life. They can be big or small, significant or trivial, but they all share one thing in common: they're essential to who I am and what I want to achieve. My imperatives are a diverse and ever-changing list, and they reflect my interests, values, and priorities. Some of them are self-improving, like learning new languages or developing my social skills. Others are more indulgent, like collecting stationery or trying out new beauty routines. And then there are the imperatives that are simply a part of my daily life, like grading papers or writing research proposals. One of the challenges of being governed by imperatives is that it can be overwhelming. There are always so many things that I feel like I need to do or accomplish, and it can be hard to prioritize and manage my time. This is especially true when I have a lot of different imperatives competing for my attention, like during RU's fall break. With a six-day weekend stretching out before me, I have to navigate a long list of tasks and responsibilities, from grading papers to replacing my windshield. Despite the chaos and complexity of my imperatives, I'm grateful to have them.",
    "id": "60c554f4-993b-416d-a6a9-5888db1d7681",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "This is especially true when I have a lot of different imperatives competing for my attention, like during RU's fall break. With a six-day weekend stretching out before me, I have to navigate a long list of tasks and responsibilities, from grading papers to replacing my windshield. Despite the chaos and complexity of my imperatives, I'm grateful to have them. They give my life structure and purpose, and they help me to stay focused and motivated. They also remind me of what's truly important to me, and what I want to achieve in the short and long term. Of course, being governed by imperatives isn't always easy. There are times when I feel like I'm spreading myself too thin, or when I'm not living up to my own expectations. But overall, I think that my imperatives are a net positive, and they help me to live a more intentional and fulfilling life. As I look back on my imperatives, I realize that they're not just about what I want to achieve or acquire. They're also about the relationships I have with others, and the ways in which I connect and interact with the world around me. My readers, for example, are an important part of my imperatives, and I value the conversations and connections that we have through this blog. In the end, being governed by imperatives is a complex and multifaceted experience, and it's one that I'm still navigating and exploring. But I'm grateful for the challenges and opportunities that it presents, and I'm excited to see where my imperatives will take me in the future. Area Forecast Discussion Issued by NWS Quad Cities, IA IL FXUS63 KDVN 270439 Area Forecast Discussion National Weather Service Quad Cities IA IL 1139 PM CDT TUE JUL 26 2016 Short-Term Forecast (This Evening through Wednesday) Quiet and dry conditions will prevail across the area late this afternoon and through Wednesday morning as high pressure moves into the Ohio Valley. Light winds and clear skies will allow patchy fog to develop late tonight and continue through sunrise.",
    "id": "538b3433-c146-4845-ac63-03b2bc3f92d1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Light winds and clear skies will allow patchy fog to develop late tonight and continue through sunrise. There is a slight chance of isolated showers and thunderstorms in far northwest Buchanan county late Wednesday afternoon, but confidence is low. Long-Term Forecast (Wednesday Night through Tuesday) The main forecast concern for the long term is the chances of rain and thunderstorms Thursday as a low-pressure system slides across the area, dragging a cold front with it. There is uncertainty in the forecast, with differences between the model solutions, but the latest guidance suggests a high chance of precipitation across the area. A shortwave trough and wave at H5 will lead to the development of a low in the models, which will move across the CWA on Thursday. Showers and thunderstorms are possible along this front, with the highest chances in the afternoon and evening. As the low moves out of the area, a back door cold front will slide in from the Great Lakes, leading to cooler temperatures and a quiet weekend. The ECMWF model maintains mostly dry conditions for the area, but the blend suggests low-end chance to slight chance pops associated with the convergence on the lake-induced cold front. Aviation Near calm winds and mainly clear skies, along with a strengthening low-level inversion, will foster at least patchy fog development toward daybreak. Expect MVFR conditions with any fog, with a possibility of IFR/LIFR/VLIFR conditions. Outside of any fog, anticipate VFR conditions. Some shower and storm chances will exist by Wednesday afternoon through evening ahead of a cold front, but confidence is low. DVN Watches/Warnings/Advisories None at this time. Probability Forecast There is a 30% chance of showers and thunderstorms on Thursday, with the highest chances in the afternoon and evening. There is a 20% chance of precipitation on Friday, with mostly dry conditions expected over the weekend. This forecast is subject to change as new model data and forecast guidance become available. Stay tuned for updates and revisions.",
    "id": "f96c759f-6ee1-4a74-8092-bde7148635aa",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "DVN Watches/Warnings/Advisories None at this time. Probability Forecast There is a 30% chance of showers and thunderstorms on Thursday, with the highest chances in the afternoon and evening. There is a 20% chance of precipitation on Friday, with mostly dry conditions expected over the weekend. This forecast is subject to change as new model data and forecast guidance become available. Stay tuned for updates and revisions. The Great Bedroom Refurbishment Debacle As I stand amidst the chaos of my daughter's bedroom refurbishment, I am reminded of the wise words: \"be careful what you wish for.\" What started as a simple idea to give my daughter's room a fresh new look has rapidly descended into a nightmare of epic proportions. The room, once a tranquil oasis, is now a war zone of debris and destruction. Furniture is disassembled, walls are bare, and the floor is a sea of dust and chaos. And at the center of it all is my daughter, clinging to her beloved possessions like a shipwreck survivor clinging to a liferaft. I must admit, I underestimated the sheer volume of \"treasures\" that my daughter had accumulated over the years. From scribbly sketches to soft toys, every item is precious and irreplaceable in her eyes. And as I gaze upon the mountain of clutter, I am struck by the realization that I may have created a monster ‚Äì a mini-hoarder in the making. But I digress. The task at hand is to transform this chaotic mess into a beautiful and functional space that my daughter will love. And so, I embark on a journey of paint swatches, furniture assembly, and decorative decisions. It's a daunting task, but someone has to do it. As I navigate the treacherous waters of DIY decorating, I find myself drawing inspiration from our recent trip to Edinburgh. The city's vibrant atmosphere and rich history are a far cry from the chaos of my daughter's bedroom, but they remind me of the importance of creating a space that is both beautiful and functional.",
    "id": "f00fb18c-080f-4de4-be95-46ce173243e3",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "And so, I embark on a journey of paint swatches, furniture assembly, and decorative decisions. It's a daunting task, but someone has to do it. As I navigate the treacherous waters of DIY decorating, I find myself drawing inspiration from our recent trip to Edinburgh. The city's vibrant atmosphere and rich history are a far cry from the chaos of my daughter's bedroom, but they remind me of the importance of creating a space that is both beautiful and functional. Our visit to the National Museum and the Real Mary King's Close were particular highlights, and I find myself drawing parallels between the historic architecture and the modern-day chaos of my daughter's room. And of course, no trip to Edinburgh would be complete without a visit to the iconic Ikea, where my daughter's eyes grew wide with wonder at the endless possibilities of Swedish design. As I reflect on the experience, I realize that the great bedroom refurbishment debacle is not just about decorating a room ‚Äì it's about creating a space that reflects my daughter's personality and style. It's about navigating the challenges of parenthood and finding a balance between functionality and aesthetics. And it's about surviving the chaos and emerging victorious, with a beautiful new room and a newfound appreciation for the art of DIY decorating. To integrate Spring Webflow 2 with Commons Validator and validate forms based on event IDs, you can follow these steps: 1. Create a custom validator that extends the DefaultBeanValidator and overrides the validate method to take into account the event ID. You can use the `ValidationContext` object to access the event ID and apply exclusions dynamically. ```java public class CustomValidator extends DefaultBeanValidator { @Override public void validate(Object obj, Errors errors) { // Get the event ID from the ValidationContext String eventId = ((ValidationContext) errors).getUserEvent(); // Apply exclusions based on the event ID if (eventId.equals(\"next\")) { // Skip validation for the \"next\" event return; } // Call the default validation method super.validate(obj, errors); } } ``` 2.",
    "id": "f5a4d392-0838-41df-8def-2c326cc603e1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "```java public class CustomValidator extends DefaultBeanValidator { @Override public void validate(Object obj, Errors errors) { // Get the event ID from the ValidationContext String eventId = ((ValidationContext) errors).getUserEvent(); // Apply exclusions based on the event ID if (eventId.equals(\"next\")) { // Skip validation for the \"next\" event return; } // Call the default validation method super.validate(obj, errors); } } ``` 2. Use the validation.xml file to define the validations and exclusions for each field. You can use the `ValidatorFactory` to configure the custom validator and validation.xml file. ```xml <bean id=\"validatorFactory\" class=\"org.springmodules.validation.commons.DefaultValidatorFactory\"> <property name=\"validationConfigLocations\"> <list> <value>classpath:validation.xml</value> </list> </property> </bean> <bean id=\"customValidator\" class=\"CustomValidator\"> <property name=\"validatorFactory\" ref=\"validatorFactory\"/> </bean> ``` 3. Intercept the validation process to apply exclusions dynamically based on the event ID. You can use the `ValidationInterceptor` to intercept the validation process and apply exclusions. ```java public class ValidationInterceptor implements MethodInterceptor { @Override public Object invoke(MethodInvocation invocation) throws Throwable { // Get the event ID from the MethodInvocation String eventId = (String) invocation.getArguments()[0]; // Apply exclusions based on the event ID if (eventId.equals(\"next\")) { // Skip validation for the \"next\" event return null; } // Call the default validation method return invocation.proceed(); } } ``` 4. Configure the validatorFactory to use the custom validator and validation.xml file. You can use the `ValidatorFactory` to configure the custom validator and validation.xml file.",
    "id": "c53253cd-3290-4b48-bcf2-5a36d4a8dab9",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Configure the validatorFactory to use the custom validator and validation.xml file. You can use the `ValidatorFactory` to configure the custom validator and validation.xml file. ```xml <bean id=\"validatorFactory\" class=\"org.springmodules.validation.commons.DefaultValidatorFactory\"> <property name=\"validationConfigLocations\"> <list> <value>classpath:validation.xml</value> </list> </property> <property name=\"validator\" ref=\"customValidator\"/> </bean> ``` By following these steps, you can integrate Spring Webflow 2 with Commons Validator and validate forms based on event IDs. Note that this solution requires a combination of custom validation, validation.xml configuration, and interception of the validation process. You may need to modify the code snippets and configuration files to fit your specific use case. Additionally, you can use the `ValidatorFactory` to configure multiple validators and validation.xml files for different forms. You can also use the `ValidationInterceptor` to intercept the validation process and apply exclusions dynamically based on the event ID. I hope this helps! Let me know if you have any further questions or need more assistance. The original poster is seeking a feature in Lightwave Layout that allows them to plot a path for a character to follow while performing a walk cycle, similar to the \"footsteps\" feature in 3DMAX. To address this question, I will break down the problem into smaller parts and explore possible solutions. First, let's assume that the character's walk cycle is already animated. In this case, the focus is solely on plotting the path for the character to follow. Lightwave provides a motion path tool that can be used to guide the character's movement. However, if this tool is not sufficient, there are third-party plugins available that provide more advanced path-following capabilities, such as ACS4 or Messiah:Animate 4. To determine the best approach, let's consider the key assumptions underlying this problem. Is the character's walk cycle already animated, or does the solution need to include animating the walk cycle as well?",
    "id": "1d2bfc82-f3ff-4929-a8ec-82448c161136",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Lightwave provides a motion path tool that can be used to guide the character's movement. However, if this tool is not sufficient, there are third-party plugins available that provide more advanced path-following capabilities, such as ACS4 or Messiah:Animate 4. To determine the best approach, let's consider the key assumptions underlying this problem. Is the character's walk cycle already animated, or does the solution need to include animating the walk cycle as well? Are there any specific requirements for the path, such as it being a straight line, a curve, or a complex trajectory? If the walk cycle is already animated, then the solution can focus solely on plotting the path. In this case, the motion path tool in Lightwave can be used to create a path for the character to follow. However, if the walk cycle needs to be animated, then that adds an additional layer of complexity to the problem. In Lightwave, the motion path tool can be used to create a path for the character to follow. To do this, the user can create a motion path and then assign it to the character. The character will then follow the motion path, allowing the user to control its movement. Alternatively, third-party plugins like ACS4 or Messiah:Animate 4 can be used to provide more advanced path-following capabilities. These plugins can be used to create complex motion paths and to animate the character's movement along those paths. Another possible approach is to use a scripting language to animate the character's movement along a path. This can be done using Lightwave's built-in scripting language, LScript, or using a third-party scripting language like Python. In conclusion, there are several possible solutions to the problem of plotting a path for a character to follow while performing a walk cycle in Lightwave. The best approach will depend on the specific requirements of the project and the user's level of experience with Lightwave and its plugins. Here is a step-by-step plan to implement the solution: 1. Verify that the character's walk cycle is already animated. 2.",
    "id": "e535f588-bb23-4195-a8cc-8882f506fee8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "In conclusion, there are several possible solutions to the problem of plotting a path for a character to follow while performing a walk cycle in Lightwave. The best approach will depend on the specific requirements of the project and the user's level of experience with Lightwave and its plugins. Here is a step-by-step plan to implement the solution: 1. Verify that the character's walk cycle is already animated. 2. Explore the motion path tool in Lightwave to determine its capabilities and limitations. 3. Research third-party plugins that provide advanced path-following capabilities, such as ACS4 or Messiah:Animate 4. 4. If necessary, use a scripting language to animate the character's movement along a path. By following this plan, users should be able to find a suitable solution for plotting a path for their character to follow while performing a walk cycle in Lightwave. To achieve an Ajax-enabled file upload feature in .Net 2.0, we can use an iframe-based approach. This involves creating an iframe that contains the FileUpload control and using JavaScript to handle the upload process and update the progress bar. First, we need to create an ASP.NET page that contains the FileUpload control and an iframe. The iframe will be used to upload the file, while the main page will display the progress bar. ```asp <%@ Page Language=\"C\" %> <script runat=\"server\"> protected void Page_Load(object sender, EventArgs e) { // Initialize the iframe iframe1.Attributes[\"src\"] = \"upload.aspx\"; } </script> <html> <body> <form id=\"form1\" runat=\"server\"> <asp:FileUpload ID=\"FileUpload1\" runat=\"server\" /> <iframe id=\"iframe1\" runat=\"server\" width=\"0\" height=\"0\"></iframe> <img id=\"progressBar\" src=\"progress.gif\" style=\"display: none;\" /> </form> </body> </html> ``` Next, we need to create a separate ASP.NET page (upload.aspx) that will handle the file upload process.",
    "id": "446fb284-4398-43f5-8bd0-72f929492e82",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "This page will contain the FileUpload control and will use the iframe to upload the file. ```asp <%@ Page Language=\"C\" %> <script runat=\"server\"> protected void Page_Load(object sender, EventArgs e) { // Handle the file upload process if (FileUpload1.HasFile) { // Upload the file FileUpload1.SaveAs(\"C:\\\\\\\\upload\\\\\" + FileUpload1.FileName); // Update the progress bar ScriptManager.RegisterStartupScript(this, this.GetType(), \"UpdateProgressBar\", \"document.getElementById('progressBar').style.display = 'none';\", true); } } </script> <html> <body> <form id=\"form1\" runat=\"server\"> <asp:FileUpload ID=\"FileUpload1\" runat=\"server\" /> <asp:Button ID=\"Button1\" runat=\"server\" Text=\"Upload\" OnClick=\"Page_Load\" /> </form> </body> </html> ``` To update the progress bar, we can use client-side scripting to show and hide the animated GIF during the upload process. ```javascript function UpdateProgressBar() { document.getElementById('progressBar').style.display = 'block'; } ``` We can call this function when the user clicks the upload button. ```asp <asp:Button ID=\"Button1\" runat=\"server\" Text=\"Upload\" OnClientClick=\"UpdateProgressBar()\" OnClick=\"Page_Load\" /> ``` By using this iframe-based approach, we can achieve an Ajax-enabled file upload feature in .Net 2.0, with a progress bar that updates in real-time. Note that this solution requires additional JavaScript code to handle the upload process and update the progress bar. However, it provides a seamless and interactive experience for the user, with a progress bar that updates in real-time. End of preview.",
    "id": "1e30a435-8f76-4c86-b08d-e41c7c4db38a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Note that this solution requires additional JavaScript code to handle the upload process and update the progress bar. However, it provides a seamless and interactive experience for the user, with a progress bar that updates in real-time. End of preview. Expand in Data Studio Previous 1 2 3 ... 16,320 Next Dataset Details Dataset Description Dataset Sources Uses Direct Use Dataset Creation Curation Rationale Source Data Citation Dataset Card Contact Dataset Card for Recycling-The-Web Synthetic Data We release 44.4B tokens of high-quality, model-filtered synthetic texts obtained via our REcycling the Web with guIded REwrite (REWIRE) approach. The generation process involves taking all documents that are of moderate quality (i.e., having passed some rule-based filters), using an LLM (Llama-3.3-70B-Instruct) to identify the purpose of the text content, and then asking the LLM to come up with an improved document conditioned on chain-of-thought reasoning. Our approach specifically targets the vast quantity of low-quality documents that are somewhat informative but still not considered high-quality by existing filters. We use LLM‚Äôs knowledge and reasoning capabilities to recycle these discarded documents and add them back to the training pool. Dataset Details Dataset Description Curated by: Thao Nguyen Language(s): Mostly English texts License: CC-by-NC The texts are outputs of Llama 3.3 and subject to the Llama 3.3 license (https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE). If you use the data to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name. Third party content pulled from other locations are subject to its own licenses and you may have other legal obligations or restrictions that govern your use of that content.",
    "id": "c41f09d6-4a4b-42e4-b720-eb233aabb27c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "If you use the data to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name. Third party content pulled from other locations are subject to its own licenses and you may have other legal obligations or restrictions that govern your use of that content. Dataset Sources Paper: Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models, COLM 2025 Uses Direct Use The data is intended for pre-training large language models (LLMs), and designed with the goal of complementing existing web-scraped texts. Dataset Creation Curation Rationale The data was obtained by using an LLM to rewrite low-quality documents that have been discarded by quality filters, in order to make them useful for pre-training. This helps increase the token availability and address the impending data bottleneck, as the growth of public human-generated texts has been lagging behind the increase in model capacity and training token budget. Across different model scales, we find that mixing our synthetic data and high-quality web data consistently outperforms training on only the latter. Source Data Data Collection and Processing We first gathered raw web documents from DCLM-RefinedWeb (Li et al., 2024), Common Crawl data that has passed the rule-based quality filters from RefinedWeb (Penedo et al., 2023) (e.g., repetition filter, page length filter, URL filter, etc.) and global deduplication, but has not gone through model-based filtering. We then prompted Llama-3.3-70B-Instruct (Grattafiori et al., 2024) to perform chain-of-thought reasoning on the original web documents, such as identifying the task or purpose of the text, reasoning about the steps needed to achieve the purpose, etc. before generating an improved version of the documents. Refer to our paper for the full prompt we used. We applied this rewriting process to all documents in the starting pool (DCLM-RefinedWeb).",
    "id": "257b1768-8e36-4f09-80e7-9959706f5e2f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "before generating an improved version of the documents. Refer to our paper for the full prompt we used. We applied this rewriting process to all documents in the starting pool (DCLM-RefinedWeb). To control the quality of the generations, we further performed model-based filtering using a fastText classifier, following DCLM (Li et al., 2024). This data release contains only the rewritten outputs that have been filtered, i.e. those in the top 10% of the generations based on fastText scores. Citation If you use data from Recyling The Web, please cite with the following BibTex entry: @article{nguyen2025recycling, title={Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models}, author={Nguyen, Thao and Li, Yang and Golovneva, Olga and Zettlemoyer, Luke and Oh, Sewoong and Schmidt, Ludwig and Li, Xian}, journal={arXiv preprint arXiv:2506.04689}, year={2025} } Dataset Card Contact Thao Nguyen (thaottn@cs.washington.edu) Downloads last month5,956 Use this dataset Size of the auto-converted Parquet files (First 5GB): 2.8 GBNumber of rows (First 5GB): 1,631,961Estimated number of rows: 60,305,490 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Models compatible with the ONNX library ‚Äì Hugging Face (https://huggingface.",
    "id": "6f508470-f32f-43dc-818b-0e11b72abcb2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "washington.edu) Downloads last month5,956 Use this dataset Size of the auto-converted Parquet files (First 5GB): 2.8 GBNumber of rows (First 5GB): 1,631,961Estimated number of rows: 60,305,490 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Models compatible with the ONNX library ‚Äì Hugging Face (https://huggingface.co/models?library=onnx) === Models compatible with the ONNX library ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks Libraries 1 Languages Licenses Other Tasks Text Generation Any-to-Any Image-Text-to-Text Image-to-Text Image-to-Image Text-to-Image Text-to-Video Text-to-Speech + 42Parameters Reset Parameters < 1B 6B 12B 32B 128B > 500B < 1B > 500B Libraries Reset Libraries PyTorch google-tensorflow TensorFlow JAX Transformers Diffusers Safetensors ONNX GGUF Transformers.js MLX Keras + 41 Apps vLLM TGI llama.cpp MLX LM LM Studio Ollama Jan + 13 Inference Providers Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale + 6 Apply filters Models 24,466 Full-text search Edit filters Sort: Trending Active filters: onnx Clear all stepfun-ai/Step-Audio-2-mini Any-to-Any ‚Ä¢ 8B ‚Ä¢ Updated 4 days ago ‚Ä¢ 3.02k ‚Ä¢ 208 Nikity/lille-130m-instruct Text Generation ‚Ä¢ 0.1B ‚Ä¢ Updated 4 days ago ‚Ä¢ 1.64k ‚Ä¢ 88 stabilityai/stable-diffusion-xl-base-1.0 Text-to-Image ‚Ä¢ Updated Oct 30, 2023 ‚Ä¢ 2.14M ‚Ä¢ ‚Ä¢ 6.92k onnx-community/FastVLM-0.",
    "id": "17c3786c-902b-4d83-b1a6-ae9f699a4564",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "02k ‚Ä¢ 208 Nikity/lille-130m-instruct Text Generation ‚Ä¢ 0.1B ‚Ä¢ Updated 4 days ago ‚Ä¢ 1.64k ‚Ä¢ 88 stabilityai/stable-diffusion-xl-base-1.0 Text-to-Image ‚Ä¢ Updated Oct 30, 2023 ‚Ä¢ 2.14M ‚Ä¢ ‚Ä¢ 6.92k onnx-community/FastVLM-0.5B-ONNX Image-Text-to-Text ‚Ä¢ Updated 6 days ago ‚Ä¢ 14.3k ‚Ä¢ 64 sentence-transformers/all-MiniLM-L6-v2 Sentence Similarity ‚Ä¢ 0.0B ‚Ä¢ Updated Mar 6 ‚Ä¢ 88.9M ‚Ä¢ ‚Ä¢ 3.85k BAAI/bge-m3 Sentence Similarity ‚Ä¢ Updated Jul 3, 2024 ‚Ä¢ 5.89M ‚Ä¢ ‚Ä¢ 2.33k HancomInSpaceAI/HiEmbed_base_onnx_v1 Sentence Similarity ‚Ä¢ Updated 1 day ago ‚Ä¢ 7 ‚Ä¢ 16 onnx-community/embeddinggemma-300m-ONNX Sentence Similarity ‚Ä¢ Updated 4 days ago ‚Ä¢ 4.51k ‚Ä¢ 15 UmeAiRT/ComfyUI-Auto_installer 8B ‚Ä¢ Updated 15 days ago ‚Ä¢ 312k ‚Ä¢ 114 Nikity/lille-130m-base Text Generation ‚Ä¢ 0.1B ‚Ä¢ Updated 4 days ago ‚Ä¢ 43 ‚Ä¢ 11 google-bert/bert-base-uncased Fill-Mask ‚Ä¢ 0.1B ‚Ä¢ Updated Feb 19, 2024 ‚Ä¢ 53.8M ‚Ä¢ ‚Ä¢ 2.4k sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 Sentence Similarity ‚Ä¢ 0.1B ‚Ä¢ Updated Mar 6 ‚Ä¢ 10.2M ‚Ä¢ ‚Ä¢ 1k openai-community/gpt2 Text Generation ‚Ä¢ 0.1B ‚Ä¢ Updated Feb 19, 2024 ‚Ä¢ 11M ‚Ä¢ 2.93k BAAI/bge-small-en-v1.5 Feature Extraction ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 22,",
    "id": "28024df0-7be7-41cd-8ad3-20ef6ed683c1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "1B ‚Ä¢ Updated Mar 6 ‚Ä¢ 10.2M ‚Ä¢ ‚Ä¢ 1k openai-community/gpt2 Text Generation ‚Ä¢ 0.1B ‚Ä¢ Updated Feb 19, 2024 ‚Ä¢ 11M ‚Ä¢ 2.93k BAAI/bge-small-en-v1.5 Feature Extraction ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 22, 2024 ‚Ä¢ 4.3M ‚Ä¢ ‚Ä¢ 367 jinaai/jina-embeddings-v3 Feature Extraction ‚Ä¢ 0.6B ‚Ä¢ Updated Feb 24 ‚Ä¢ 4.17M ‚Ä¢ 1.06k jinaai/ReaderLM-v2 Text Generation ‚Ä¢ 2B ‚Ä¢ Updated Mar 4 ‚Ä¢ 22.3k ‚Ä¢ ‚Ä¢ 698 ds4sd/SmolDocling-256M-preview Image-Text-to-Text ‚Ä¢ 0.3B ‚Ä¢ Updated 14 days ago ‚Ä¢ 55.6k ‚Ä¢ 1.56k KittenML/kitten-tts-mini-0.1 Updated about 10 hours ago ‚Ä¢ 6 FacebookAI/xlm-roberta-large-finetuned-conll03-english Token Classification ‚Ä¢ 0.6B ‚Ä¢ Updated Feb 19, 2024 ‚Ä¢ 200k ‚Ä¢ ‚Ä¢ 177 answerdotai/ModernBERT-base Fill-Mask ‚Ä¢ 0.1B ‚Ä¢ Updated Jan 15 ‚Ä¢ 1.12M ‚Ä¢ 928 Kim2091/UltraSharpV2 Image-to-Image ‚Ä¢ Updated Jun 5 ‚Ä¢ 52 KittenML/kitten-tts-nano-0.1 Updated 9 days ago ‚Ä¢ 83.5k ‚Ä¢ 476 cross-encoder/ms-marco-MiniLM-L6-v2 Text Ranking ‚Ä¢ 0.0B ‚Ä¢ Updated 10 days ago ‚Ä¢ 3.94M ‚Ä¢ 143 sentence-transformers/all-mpnet-base-v2 Sentence Similarity ‚Ä¢ 0.1B ‚Ä¢ Updated 20 days ago ‚Ä¢ 17M ‚Ä¢ ‚Ä¢ 1.15k uwg/upscaler Updated Dec 23, 2024 ‚Ä¢ 635 BAAI/bge-large-en-v1.5 Feature Extraction ‚Ä¢ 0.",
    "id": "2cad539a-93c8-4e20-a25d-b4bd1766acac",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "0B ‚Ä¢ Updated 10 days ago ‚Ä¢ 3.94M ‚Ä¢ 143 sentence-transformers/all-mpnet-base-v2 Sentence Similarity ‚Ä¢ 0.1B ‚Ä¢ Updated 20 days ago ‚Ä¢ 17M ‚Ä¢ ‚Ä¢ 1.15k uwg/upscaler Updated Dec 23, 2024 ‚Ä¢ 635 BAAI/bge-large-en-v1.5 Feature Extraction ‚Ä¢ 0.3B ‚Ä¢ Updated Feb 21, 2024 ‚Ä¢ 3.57M ‚Ä¢ ‚Ä¢ 569 BAAI/bge-reranker-large Feature Extraction ‚Ä¢ 0.6B ‚Ä¢ Updated May 11, 2024 ‚Ä¢ 562k ‚Ä¢ ‚Ä¢ 416 intfloat/multilingual-e5-large-instruct Feature Extraction ‚Ä¢ 0.6B ‚Ä¢ Updated Jul 10 ‚Ä¢ 1.86M ‚Ä¢ ‚Ä¢ 556 nomic-ai/nomic-embed-text-v1.5 Sentence Similarity ‚Ä¢ 0.1B ‚Ä¢ Updated Jul 21 ‚Ä¢ 2.34M ‚Ä¢ 698 mixedbread-ai/mxbai-embed-large-v1 Feature Extraction ‚Ä¢ 0.3B ‚Ä¢ Updated Mar 13 ‚Ä¢ 2.26M ‚Ä¢ ‚Ä¢ 720 Previous 1 2 3 ... 100 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Safetensors (https://huggingface.co/docs/safetensors) === Safetensors Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Safetensors documentation Safetensors Safetensors üè° View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm Search documentation mainv0.5.0-rc.",
    "id": "ddc93cdf-3b6a-4c94-b9e7-0ffcfd4778c5",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "jsInference Endpoints (dedicated)Inference ProvidersLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm Search documentation mainv0.5.0-rc.0v0.3.2v0.2.9 EN Getting started ü§ó Safetensors Speed Comparison Tensor Sharing in Pytorch Metadata Parsing Convert weights to safetensors API Torch API Tensorflow API PaddlePaddle API Flax API Numpy API You are viewing main version, which requires installation from source. If you'd like regular pip install, checkout the latest stable version (v0.5.0-rc.0). Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Safetensors Safetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). Safetensors is really fast üöÄ.",
    "id": "f3e43b40-a13a-4941-bbe7-24613e8ee825",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Safetensors Safetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). Safetensors is really fast üöÄ. Installation with pip: Copied pip install safetensors with conda: Copied conda install -c huggingface safetensors Usage Load tensors Copied from safetensors import safe_open tensors = {} with safe_open(\"model.safetensors\", framework=\"pt\", device=0) as f: for k in f.keys(): tensors[k] = f.get_tensor(k) Loading only part of the tensors (interesting when running on multiple GPU) Copied from safetensors import safe_open tensors = {} with safe_open(\"model.safetensors\", framework=\"pt\", device=0) as f: tensor_slice = f.get_slice(\"embedding\") vocab_size, hidden_dim = tensor_slice.get_shape() tensor = tensor_slice[:, :hidden_dim] Save tensors Copied import torch from safetensors.torch import save_file tensors = { \"embedding\": torch.zeros((2, 2)), \"attention\": torch.zeros((2, 3)) } save_file(tensors, \"model.safetensors\") Format Let‚Äôs say you have safetensors file named model.safetensors, then model.safetensors will have the following internal format: Featured Projects Safetensors is being used widely at leading AI enterprises, such as Hugging Face, EleutherAI, and StabilityAI.",
    "id": "f87c4e1f-cd76-488d-9a4b-fc22677ebf48",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Here is a non-exhaustive list of projects that are using safetensors: huggingface/transformers ml-explore/mlx huggingface/candle AUTOMATIC1111/stable-diffusion-webui Llama-cpp microsoft/TaskMatrix hpcaitech/ColossalAI huggingface/pytorch-image-models CivitAI huggingface/diffusers coreylowman/dfdx invoke-ai/InvokeAI oobabooga/text-generation-webui Sanster/lama-cleaner PaddlePaddle/PaddleNLP AIGC-Audio/AudioGPT brycedrennan/imaginAIry comfyanonymous/ComfyUI LianjiaTech/BELLE alvarobartt/safejax MaartenGr/BERTopic rachthree/safestructures justinchuby/onnx-safetensors < > Update on GitHub Speed Comparison‚Üí Safetensors Installation Usage Load tensors Save tensors Format Featured Projects === PAGE: DeepSite v2 - a Hugging Face Space by enzostvs (https://huggingface.co/spaces/enzostvs/deepsite) === DeepSite v2 - a Hugging Face Space by enzostvs Spaces enzostvs / deepsite like 12.9k Running App Files Files Community 369 Fetching metadata from the HF Docker repository... Refreshing === PAGE: apple/FastVLM-0.5B ¬∑ Hugging Face (https://huggingface.co/apple/FastVLM-0.5B) === apple/FastVLM-0.5B ¬∑ Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up apple / FastVLM-0.5B like 259 Follow Apple 4.85k Text Generation Safetensors Transformers ml-fastvlm llava_qwen2 conversational custom_code arxiv: 2412.",
    "id": "e111e5bc-7511-4f5b-810b-5618b2a9988c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "co/apple/FastVLM-0.5B) === apple/FastVLM-0.5B ¬∑ Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up apple / FastVLM-0.5B like 259 Follow Apple 4.85k Text Generation Safetensors Transformers ml-fastvlm llava_qwen2 conversational custom_code arxiv: 2412.13303 License: apple-amlr Model card Files Files and versions xet Community 7 Use this model FastVLM: Efficient Vision Encoding for Vision Language Models Citation FastVLM: Efficient Vision Encoding for Vision Language Models FastVLM was introduced in FastVLM: Efficient Vision Encoding for Vision Language Models. (CVPR 2025) Highlights We introduce FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Our smallest variant outperforms LLaVA-OneVision-0.5B with 85x faster Time-to-First-Token (TTFT) and 3.4x smaller vision encoder. Our larger variants using Qwen2-7B LLM outperform recent works like Cambrian-1-8B while using a single image encoder with a 7.9x faster TTFT.",
    "id": "4397d347-19e8-47a6-80a2-3d59e6d4c3d4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Our smallest variant outperforms LLaVA-OneVision-0.5B with 85x faster Time-to-First-Token (TTFT) and 3.4x smaller vision encoder. Our larger variants using Qwen2-7B LLM outperform recent works like Cambrian-1-8B while using a single image encoder with a 7.9x faster TTFT. Evaluations Benchmark FastVLM-0.5B FastVLM-1.5B FastVLM-7B Ai2D 68.0 77.4 83.6 ScienceQA 85.2 94.4 96.7 MMMU 33.9 37.8 45.4 VQAv2 76.3 79.1 80.8 ChartQA 76.0 80.1 85.0 TextVQA 64.5 70.4 74.9 InfoVQA 46.4 59.7 75.8 DocVQA 82.5 88.3 93.2 OCRBench 63.9 70.2 73.1 RealWorldQA 56.1 61.2 67.2 SeedBench-Img 71.0 74.2 75.4 Usage Example To run inference of PyTorch checkpoint, follow the instruction in the official repo: Download the model huggingface-cli download apple/FastVLM-0.5B Run inference using predict.py from the official repo. python predict.py --model-path /path/to/checkpoint-dir \\ --image-file /path/to/image.png \\ --prompt \"Describe the image.\"",
    "id": "6c817128-203b-404c-9610-00a71d70acd9",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "python predict.py --model-path /path/to/checkpoint-dir \\ --image-file /path/to/image.png \\ --prompt \"Describe the image.\" Run inference with Transformers (Remote Code) To run inference with transformers we can leverage trust_remote_code along with the following snippet: import torch from PIL import Image from transformers import AutoTokenizer, AutoModelForCausalLM MID = \"apple/FastVLM-0.5B\" IMAGE_TOKEN_INDEX = -200 what the model code looks for Load tok = AutoTokenizer.from_pretrained(MID, trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained( MID, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, device_map=\"auto\", trust_remote_code=True, ) Build chat -> render to string (not tokens) so we can place <image> exactly messages = [ {\"role\": \"user\", \"content\": \"<image>\\nDescribe this image in detail.\"} ] rendered = tok.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) pre, post = rendered.split(\"<image>\", 1) Tokenize the text around the image token (no extra specials!)",
    "id": "9b438f41-bd7e-4202-ad64-dc2594933b89",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "] rendered = tok.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) pre, post = rendered.split(\"<image>\", 1) Tokenize the text around the image token (no extra specials!) pre_ids = tok(pre, return_tensors=\"pt\", add_special_tokens=False).input_ids post_ids = tok(post, return_tensors=\"pt\", add_special_tokens=False).input_ids Splice in the IMAGE token id (-200) at the placeholder position img_tok = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype) input_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(model.device) attention_mask = torch.ones_like(input_ids, device=model.device) Preprocess image via the model's own processor img = Image.open(\"test-2.jpg\").convert(\"RGB\") px = model.get_vision_tower().image_processor(images=img, return_tensors=\"pt\")[\"pixel_values\"] px = px.to(model.device, dtype=model.dtype) Generate with torch.no_grad(): out = model.generate( inputs=input_ids, attention_mask=attention_mask, images=px, max_new_tokens=128, ) print(tok.decode(out[0], skip_special_tokens=True)) Citation If you found this model useful, please cite the following paper: @InProceedings{fastvlm2025, author = {Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari}, title = {FastVLM: Efficient Vision Encoding for Vision Language Models}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2025}, } Downloads last month18,181 Safetensors Model size 759M params Tensor type BF16 ¬∑ Chat template Files info Inference Providers NEW Text Generation This model isn't deployed by any Inference Provider.",
    "id": "236b417e-70b1-4fb2-baff-d7375e7c7de8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "üôã 3 Ask for provider support Model tree for apple/FastVLM-0.5B Finetunes 2 models Quantizations 2 models Spaces using apple/FastVLM-0.5B 4 ‚ö° umint/searchgpt üèÜ rahul7star/FastVLM-ImageToText üê® Grinding/Vizionary üèÜ akhaliq/FastVLM-0.5B-gradio Collection including apple/FastVLM-0.5B FastVLM Collection Efficient Vision Encoding for Vision Language Models ‚Ä¢ 9 items ‚Ä¢ Updated 6 days ago ‚Ä¢ 91 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Models ‚Äì Hugging Face (https://huggingface.co/models?other=llama.cpp) === Models ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks Libraries Languages Licenses Other 1 Apps llama.cpp LM Studio Jan Backyard AI Draw Things DiffusionBee Jellybox RecurseChat Msty Sanctum Invoke JoyFusion LocalAI vLLM node-llama-cpp Ollama TGI MLX LM Docker Model Runner Lemonade Inference Providers Select all Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale SambaNova fal Featherless AI Cohere Replicate HF Inference API Misc Reset Misc llama.cpp Inference Endpoints text-generation-inference Eval Results Merge 4-bit precision custom_code 8-bit precision text-embeddings-inference Mixture of Experts Carbon Emissions Apply filters Models 288 Full-text search Edit filters Sort: Trending Active filters: llama.cpp Clear all LiquidAI/LFM2-VL-1.6B-GGUF Image-Text-to-Text ‚Ä¢ 1B ‚Ä¢ Updated 21 days ago ‚Ä¢ 12.3k ‚Ä¢ 33 sugiv/cardvaultplus-500m-gguf Image-to-Text ‚Ä¢ 0.",
    "id": "0e6a3e04-25bb-4425-8056-1464d3a660d7",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "cpp Clear all LiquidAI/LFM2-VL-1.6B-GGUF Image-Text-to-Text ‚Ä¢ 1B ‚Ä¢ Updated 21 days ago ‚Ä¢ 12.3k ‚Ä¢ 33 sugiv/cardvaultplus-500m-gguf Image-to-Text ‚Ä¢ 0.4B ‚Ä¢ Updated Jul 22 ‚Ä¢ 169 ‚Ä¢ 2 mistralai/Devstral-Small-2507_gguf Text Generation ‚Ä¢ 24B ‚Ä¢ Updated Jul 23 ‚Ä¢ 4.04k ‚Ä¢ 37 LiquidAI/LFM2-350M-GGUF Text Generation ‚Ä¢ 0.4B ‚Ä¢ Updated 4 days ago ‚Ä¢ 3.41k ‚Ä¢ 27 JonathanMiddleton/Qwen3-Embedding-8B-GGUF 8B ‚Ä¢ Updated Jul 14 ‚Ä¢ 808 ‚Ä¢ 3 theprint/TiTan-Qwen2.5-0.5B-GGUF Text Generation ‚Ä¢ 0.5B ‚Ä¢ Updated 26 days ago ‚Ä¢ 894 ‚Ä¢ 2 LiquidAI/LFM2-VL-450M-GGUF Image-Text-to-Text ‚Ä¢ 0.4B ‚Ä¢ Updated 21 days ago ‚Ä¢ 4.41k ‚Ä¢ 22 Novaciano/L3.2-2025_SuperUncensored_Q5-GGUF Text Generation ‚Ä¢ 1B ‚Ä¢ Updated Jan 12 ‚Ä¢ 20 ‚Ä¢ 2 Mozzipa/ko_vicuna_7b_ggml_q4 Updated Apr 20, 2023 ‚Ä¢ 4 ManthanKulakarni/JQL_LLaMa_GGML Text Generation ‚Ä¢ Updated Jun 26, 2023 ‚Ä¢ 2 mys/ggml_llava-v1.5-7b 7B ‚Ä¢ Updated Oct 9, 2023 ‚Ä¢ 2.02k ‚Ä¢ 112 mys/ggml_llava-v1.5-13b 13B ‚Ä¢ Updated Oct 10, 2023 ‚Ä¢ 692 ‚Ä¢ 53 mys/ggml_bakllava-1 7B ‚Ä¢ Updated Oct 19,",
    "id": "45e58571-7921-4aba-ba9b-38c7db617143",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2023 ‚Ä¢ 2 mys/ggml_llava-v1.5-7b 7B ‚Ä¢ Updated Oct 9, 2023 ‚Ä¢ 2.02k ‚Ä¢ 112 mys/ggml_llava-v1.5-13b 13B ‚Ä¢ Updated Oct 10, 2023 ‚Ä¢ 692 ‚Ä¢ 53 mys/ggml_bakllava-1 7B ‚Ä¢ Updated Oct 19, 2023 ‚Ä¢ 716 ‚Ä¢ 75 wasmedge/llama2 Text Generation ‚Ä¢ 13B ‚Ä¢ Updated Nov 11, 2023 ‚Ä¢ 186 ‚Ä¢ 8 alvarobartt/lince-zero-7b-GGUF Text Generation ‚Ä¢ 7B ‚Ä¢ Updated Nov 1, 2023 ‚Ä¢ 41 vietgpt/dama-2-7b-chat-gguf Text Generation ‚Ä¢ 7B ‚Ä¢ Updated Nov 17, 2023 ‚Ä¢ 29 ‚Ä¢ 1 FlexingD/yarn-mistral-7B-64k-instruct-alpaca-cleaned-GGUF 7B ‚Ä¢ Updated Dec 4, 2023 ‚Ä¢ 94 ‚Ä¢ 5 jdluzen/Mistral-7B-Instruct-v0.2-GGUF 7B ‚Ä¢ Updated Dec 24, 2023 ‚Ä¢ 32 jacobhoffmann/CodeLlama-13B-TestGen-Dart_v0.2-GGUF Text Generation ‚Ä¢ 13B ‚Ä¢ Updated Dec 11, 2024 ‚Ä¢ 67 mostafaamiri/persian-llama-7b-GGUF-Q4 7B ‚Ä¢ Updated Jan 13, 2024 ‚Ä¢ 3.17k ‚Ä¢ 9 ayoubkirouane/Mistral-Depth-UP-Scaled-9B-AlpacaInstruct-gguf Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Jan 24, 2024 ‚Ä¢ 24 ehristoforu/LLMs Text Generation ‚Ä¢ 7B ‚Ä¢ Updated Apr 16,",
    "id": "f6b87820-f61e-408d-8e89-7c25ba72aec7",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2024 ‚Ä¢ 3.17k ‚Ä¢ 9 ayoubkirouane/Mistral-Depth-UP-Scaled-9B-AlpacaInstruct-gguf Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Jan 24, 2024 ‚Ä¢ 24 ehristoforu/LLMs Text Generation ‚Ä¢ 7B ‚Ä¢ Updated Apr 16, 2024 ‚Ä¢ 13 ‚Ä¢ 4 osanseviero/DareVox-7B-AWQ 1B ‚Ä¢ Updated Feb 7, 2024 ‚Ä¢ 9 IndiaBuild/GGUF_Navarna_v0_1_OpenHermes_Hindi 7B ‚Ä¢ Updated Feb 15, 2024 ‚Ä¢ 5 ahmetkca/trendyol-7B-v1.0-f16-gguf 7B ‚Ä¢ Updated Feb 15, 2024 ‚Ä¢ 7 ahmetkca/trendyol-7B-v1.0-f32-gguf 7B ‚Ä¢ Updated Feb 15, 2024 ‚Ä¢ 3 google/gemma-7b-it-GGUF 9B ‚Ä¢ Updated Aug 14, 2024 ‚Ä¢ 165 ‚Ä¢ 43 google/gemma-7b-GGUF 9B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 158 ‚Ä¢ 19 google/gemma-2b-it-GGUF 3B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 217 ‚Ä¢ 17 google/gemma-2b-GGUF 3B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 189 ‚Ä¢ 15 Previous 1 2 3 ... 10 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Hugging Face ‚Äì The AI community building the future. (https://huggingface.co/datasets) === Hugging Face ‚Äì The AI community building the future.",
    "id": "c77d58f0-1fc8-4608-b038-2f8f05e694a5",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2024 ‚Ä¢ 189 ‚Ä¢ 15 Previous 1 2 3 ... 10 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Hugging Face ‚Äì The AI community building the future. (https://huggingface.co/datasets) === Hugging Face ‚Äì The AI community building the future. Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Datasets filters Main Tasks Libraries Languages Licenses Other Modalities 3D Audio Document Geospatial Image Tabular Text Time-series Video Size (rows) Reset Size < 1K > 1TFormat json csv parquet imagefolder soundfolder webdataset text arrow Apply filters Datasets 492,542 Full-text search Add filters Sort: Trending HuggingFaceFW/finepdfs Preview ‚Ä¢ Updated about 4 hours ago ‚Ä¢ 5.06k ‚Ä¢ 249 HuggingFaceM4/FineVision Viewer ‚Ä¢ Updated 4 days ago ‚Ä¢ 24.2M ‚Ä¢ 49.6k ‚Ä¢ 240 data-agents/jupyter-agent-dataset Viewer ‚Ä¢ Updated 4 days ago ‚Ä¢ 95.8k ‚Ä¢ 1.81k ‚Ä¢ 108 fka/awesome-chatgpt-prompts Viewer ‚Ä¢ Updated Jan 6 ‚Ä¢ 203 ‚Ä¢ 42.8k ‚Ä¢ 9k facebook/recycling_the_web Viewer ‚Ä¢ Updated 12 days ago ‚Ä¢ 60.3M ‚Ä¢ 5.96k ‚Ä¢ 59 Pageshift-Entertainment/LongPage Viewer ‚Ä¢ Updated 4 days ago ‚Ä¢ 300 ‚Ä¢ 2.82k ‚Ä¢ 29 SandboxAQ/SAIR Viewer ‚Ä¢ Updated 10 days ago ‚Ä¢ 8.8M ‚Ä¢ 579 ‚Ä¢ 30 openai/healthbench Preview ‚Ä¢ Updated 12 days ago ‚Ä¢ 776 ‚Ä¢ 85 syncora/developer-productivity-simulated-behavioral-data Viewer ‚Ä¢ Updated 4 days ago ‚Ä¢ 2k ‚Ä¢ 1.",
    "id": "f75951e0-81d8-40b1-9f60-c95f7bdaffd1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "82k ‚Ä¢ 29 SandboxAQ/SAIR Viewer ‚Ä¢ Updated 10 days ago ‚Ä¢ 8.8M ‚Ä¢ 579 ‚Ä¢ 30 openai/healthbench Preview ‚Ä¢ Updated 12 days ago ‚Ä¢ 776 ‚Ä¢ 85 syncora/developer-productivity-simulated-behavioral-data Viewer ‚Ä¢ Updated 4 days ago ‚Ä¢ 2k ‚Ä¢ 1.12k ‚Ä¢ 83 JDhruv14/Bhagavad-Gita_Dataset Viewer ‚Ä¢ Updated about 5 hours ago ‚Ä¢ 700 ‚Ä¢ 102 ‚Ä¢ 21 nvidia/Nemotron-CC-v2 Viewer ‚Ä¢ Updated 13 days ago ‚Ä¢ 5.81B ‚Ä¢ 59.3k ‚Ä¢ 70 Lk123/InfoSeek Preview ‚Ä¢ Updated 1 day ago ‚Ä¢ 211 ‚Ä¢ 18 continuedev/instinct-data Viewer ‚Ä¢ Updated 4 days ago ‚Ä¢ 9.04k ‚Ä¢ 291 ‚Ä¢ 17 camel-ai/loong Viewer ‚Ä¢ Updated 5 days ago ‚Ä¢ 9.65k ‚Ä¢ 1.27k ‚Ä¢ 50 ASLP-lab/WenetSpeech-Yue Updated about 15 hours ago ‚Ä¢ 479 ‚Ä¢ 17 HuggingFaceFW/fineweb Viewer ‚Ä¢ Updated Jul 11 ‚Ä¢ 52.5B ‚Ä¢ 277k ‚Ä¢ 2.35k m-a-p/Inverse_IFEval Viewer ‚Ä¢ Updated about 7 hours ago ‚Ä¢ 1.01k ‚Ä¢ 287 ‚Ä¢ 14 nvidia/OpenScienceReasoning-2 Viewer ‚Ä¢ Updated Jul 31 ‚Ä¢ 803k ‚Ä¢ 1.57k ‚Ä¢ 38 facebook/meta-active-reading Viewer ‚Ä¢ Updated 24 days ago ‚Ä¢ 1.83B ‚Ä¢ 26.3k ‚Ä¢ 31 iGeniusAI/FinReflectKG Viewer ‚Ä¢ Updated 6 days ago ‚Ä¢ 17.5M ‚Ä¢ 195 ‚Ä¢ 11 Nikity/Kyoto-Corpus Viewer ‚Ä¢ Updated 8 days ago ‚Ä¢ 1.84M ‚Ä¢ 861 ‚Ä¢ 12 roneneldan/TinyStories Viewer ‚Ä¢ Updated Aug 12, 2024 ‚Ä¢ 2.14M ‚Ä¢ 39.",
    "id": "ff4f7a5a-725e-40fb-b042-0fe5419ecc50",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "83B ‚Ä¢ 26.3k ‚Ä¢ 31 iGeniusAI/FinReflectKG Viewer ‚Ä¢ Updated 6 days ago ‚Ä¢ 17.5M ‚Ä¢ 195 ‚Ä¢ 11 Nikity/Kyoto-Corpus Viewer ‚Ä¢ Updated 8 days ago ‚Ä¢ 1.84M ‚Ä¢ 861 ‚Ä¢ 12 roneneldan/TinyStories Viewer ‚Ä¢ Updated Aug 12, 2024 ‚Ä¢ 2.14M ‚Ä¢ 39.3k ‚Ä¢ 738 Josephgflowers/Finance-Instruct-500k Viewer ‚Ä¢ Updated Mar 1 ‚Ä¢ 518k ‚Ä¢ 2.23k ‚Ä¢ 159 nvidia/Nemotron-Pretraining-SFT-v1 Viewer ‚Ä¢ Updated 13 days ago ‚Ä¢ 358M ‚Ä¢ 3.56k ‚Ä¢ 19 syncora/synthetic-healthcare-admissions Viewer ‚Ä¢ Updated 13 days ago ‚Ä¢ 100k ‚Ä¢ 603 ‚Ä¢ 41 yahma/alpaca-cleaned Viewer ‚Ä¢ Updated Apr 10, 2023 ‚Ä¢ 51.8k ‚Ä¢ 25.1k ‚Ä¢ 723 gaia-benchmark/GAIA Updated Feb 13 ‚Ä¢ 14.5k ‚Ä¢ 436 xai-org/RealworldQA Viewer ‚Ä¢ Updated Apr 18, 2024 ‚Ä¢ 765 ‚Ä¢ 1.48k ‚Ä¢ 106 open-thoughts/OpenThoughts-114k Viewer ‚Ä¢ Updated 9 days ago ‚Ä¢ 228k ‚Ä¢ 31.3k ‚Ä¢ 754 peteromallet/high-quality-midjouney-srefs Viewer ‚Ä¢ Updated Jul 16 ‚Ä¢ 3.92k ‚Ä¢ 704 ‚Ä¢ 15 Previous 1 2 3 ... 100 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: PEFT (https://huggingface.",
    "id": "e815652a-0bab-478e-a249-7ae9bda17be2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "3k ‚Ä¢ 754 peteromallet/high-quality-midjouney-srefs Viewer ‚Ä¢ Updated Jul 16 ‚Ä¢ 3.92k ‚Ä¢ 704 ‚Ä¢ 15 Previous 1 2 3 ... 100 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: PEFT (https://huggingface.co/docs/peft) === PEFT Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up PEFT documentation PEFT PEFT üè° View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm Search documentation mainv0.17.0v0.16.0v0.15.0v0.14.0v0.13.0v0.12.0v0.11.0v0.10.0v0.9.0v0.8.2v0.7.1v0.6.2 EN Get started ü§ó PEFT Quicktour Installation Tutorial Configurations and models Integrations PEFT method guides Prompt-based methods LoRA methods IA3 Developer guides Model merging Quantization LoRA Custom models Adapter injection Mixed adapter types torch.",
    "id": "6e0cda17-4990-4691-9682-9d40c4801c99",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "14.0v0.13.0v0.12.0v0.11.0v0.10.0v0.9.0v0.8.2v0.7.1v0.6.2 EN Get started ü§ó PEFT Quicktour Installation Tutorial Configurations and models Integrations PEFT method guides Prompt-based methods LoRA methods IA3 Developer guides Model merging Quantization LoRA Custom models Adapter injection Mixed adapter types torch.compile Contribute to PEFT Troubleshooting PEFT checkpoint format ü§ó Accelerate integrations DeepSpeed Fully Sharded Data Parallel Conceptual guides Adapters Soft prompts IA3 OFT/BOFT API reference Main classes AutoPeftModel PEFT model PEFT types Configuration Tuner Adapters AdaLoRA IA3 Llama-Adapter LoHa LoKr LoRA X-LoRA LyCORIS Multitask Prompt Tuning OFT BOFT Polytropon P-tuning Prefix tuning Prompt tuning Layernorm tuning VeRA FourierFT VB-LoRA HRA CPT Bone Trainable Tokens RandLora SHiRA C3A MiSS Utilities Model merge Helpers Hotswapping adapters Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started PEFT ü§ó PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model‚Äôs parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware. PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference.",
    "id": "af867519-f0a3-46ee-8cfb-c89f2b374a78",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware. PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference. Quicktour Start here if you're new to ü§ó PEFT to get an overview of the library's main features, and how to train a model with a PEFT method. How-to guides Practical guides demonstrating how to apply various PEFT methods across different types of tasks like image classification, causal language modeling, automatic speech recognition, and more. Learn how to use ü§ó PEFT with the DeepSpeed and Fully Sharded Data Parallel scripts. Conceptual guides Get a better theoretical understanding of how LoRA and various soft prompting methods help reduce the number of trainable parameters to make training more efficient. Reference Technical descriptions of how ü§ó PEFT classes and methods work. < > Update on GitHub Quicktour‚Üí PEFT === PAGE: Brand assets - Hugging Face (https://huggingface.co/brand) === Brand assets - Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Hugging Face ¬∑ Brand assets HF Logos .svg .png .ai .svg .png .ai .svg .png .ai HF Colors FFD21E FF9D00 6B7280 HF Bio Hugging Face is the collaboration platform for the machine learning community. The Hugging Face Hub works as a central place where anyone can share, explore, discover, and experiment with open-source ML. HF empowers the next generation of machine learning engineers, scientists, and end users to learn, collaborate and share their work to build an open and ethical AI future together.",
    "id": "510987f8-6392-46a0-a0f1-34ad49c29ec7",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The Hugging Face Hub works as a central place where anyone can share, explore, discover, and experiment with open-source ML. HF empowers the next generation of machine learning engineers, scientists, and end users to learn, collaborate and share their work to build an open and ethical AI future together. With the fast-growing community, some of the most used open-source ML libraries and tools, and a talented science team exploring the edge of tech, Hugging Face is at the heart of the AI revolution. Copy to clipboard HF Universe Find other assets available for use from the Hugging Face brand universe here. System theme Website Models Datasets Spaces Changelog Inference Endpoints HuggingChat Company About Brand assets Terms of service Privacy Jobs Press Resources Learn Documentation Blog Forum Service Status Social GitHub Twitter LinkedIn Discord === PAGE: Diffusers (https://huggingface.co/docs/diffusers) === Diffusers Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Diffusers documentation Diffusers Diffusers üè° View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm Search documentation mainv0.35.1v0.34.0v0.33.1v0.32.2v0.31.0v0.30.3v0.29.2v0.28.2v0.27.2v0.26.3v0.25.1v0.24.0v0.23.1v0.22.3v0.21.0v0.20.0v0.19.3v0.18.2v0.17.1v0.16.0v0.15.",
    "id": "642fc410-34fe-4b37-9820-1ebe5cfa4131",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "31.0v0.30.3v0.29.2v0.28.2v0.27.2v0.26.3v0.25.1v0.24.0v0.23.1v0.22.3v0.21.0v0.20.0v0.19.3v0.18.2v0.17.1v0.16.0v0.15.0v0.14.0v0.13.0v0.12.0v0.11.0v0.10.2v0.9.0v0.8.0v0.7.0v0.6.0v0.5.1v0.4.1v0.3.0v0.2.4 ENJAKOPTZH Get started Diffusers Installation Quickstart Basic performance DiffusionPipeline Adapters Inference Inference optimization Hybrid Inference Modular Diffusers Training Quantization Model accelerators and hardware Specific pipeline examples Resources API Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Diffusers Diffusers is a library of state-of-the-art pretrained diffusion models for generating videos, images, and audio. The library revolves around the DiffusionPipeline, an API designed for: easy inference with only a few lines of code flexibility to mix-and-match pipeline components (models, schedulers) loading and using adapters like LoRA Diffusers also comes with optimizations - such as offloading and quantization - to ensure even the largest models are accessible on memory-constrained devices. If memory is not an issue, Diffusers supports torch.compile to boost inference speed. Get started right away with a Diffusers model on the Hub today! Learn If you‚Äôre a beginner, we recommend starting with the Hugging Face Diffusion Models Course.",
    "id": "4f56cceb-05ac-4e19-b534-1bb36ce3e70b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "If memory is not an issue, Diffusers supports torch.compile to boost inference speed. Get started right away with a Diffusers model on the Hub today! Learn If you‚Äôre a beginner, we recommend starting with the Hugging Face Diffusion Models Course. You‚Äôll learn the theory behind diffusion models, and learn how to use the Diffusers library to generate images, fine-tune your own models, and more. < > Update on GitHub Installation‚Üí Diffusers Learn === PAGE: HuggingFaceM4/FineVision ¬∑ Datasets at Hugging Face (https://huggingface.co/datasets/HuggingFaceM4/FineVision) === HuggingFaceM4/FineVision ¬∑ Datasets at Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Datasets: HuggingFaceM4 / FineVision like 240 Follow HuggingFaceM4 615 Modalities: Image Text Formats: parquet Size: 10M - 100M Libraries: Datasets Dask Croissant + 1 Dataset card Data Studio Files Files and versions xet Community 12 Dataset Viewer (First 5GB) Auto-converted to Parquet API Embed Data Studio Subset (185) CoSyn_400k_chart ¬∑ 117k rows CoSyn_400k_chart (117k rows)CoSyn_400k_chemical (8.94k rows)CoSyn_400k_circuit (10.5k rows)CoSyn_400k_diagram (35k rows)CoSyn_400k_document (71.3k rows)CoSyn_400k_graphic (27k rows)CoSyn_400k_math (66.7k rows)CoSyn_400k_music (12k rows)CoSyn_400k_nutrition (6.93k rows)CoSyn_400k_table (46.5k rows)DoclingMatix (1.",
    "id": "25c9944e-3d54-48c7-9cf9-a9412d9329b2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "5k rows)CoSyn_400k_diagram (35k rows)CoSyn_400k_document (71.3k rows)CoSyn_400k_graphic (27k rows)CoSyn_400k_math (66.7k rows)CoSyn_400k_music (12k rows)CoSyn_400k_nutrition (6.93k rows)CoSyn_400k_table (46.5k rows)DoclingMatix (1.27M rows)LLaVA_Instruct_150K (158k rows)SynthChartNet (500k rows)SynthCodeNet (500k rows)SynthFormulaNet (500k rows)Unichart (612k rows)a_okvqa (54.6k rows)aguvis-stage-1 (459k rows)ai2d_merged (4.86k rows)alfworldgpt (45.1k rows)allava_laion (469k rows)allava_vflan (177k rows)aokvqa (16.5k rows)art (5.49k rows)arxivqa (100k rows)bentham (10.8k rows)blockdiagramcomputerized (502 rows)blockdiagramhandwritten (1.03k rows)cambrian(filtered)_processed (83.1k rows)captcha (113k rows)chart2text (27k rows)chartqa (18.3k rows)chinesememe (54.2k rows)chrome_writting (8.83k rows)clevr (70k rows)clevr_math (70k rows)clevr_math(mathv360k) (5.28k rows)coco_colors (118k rows)cocoqa (46.3k rows)cocotext (16.2k rows)ctw (~23.8k rows, showing the first 1.1k)datik (221k rows)datikz (47.4k rows)densefusion_1m (1.",
    "id": "0792069c-a5e8-4556-b6a8-8d4c786972d8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "28k rows)coco_colors (118k rows)cocoqa (46.3k rows)cocotext (16.2k rows)ctw (~23.8k rows, showing the first 1.1k)datik (221k rows)datikz (47.4k rows)densefusion_1m (1.06M rows)diagram_image_to_text (300 rows)docvqa (10.2k rows)drivelm (~3.78k rows, showing the first 605)dvqa (200k rows)est_vqa (19.4k rows)face_emotion (797 rows)figureqa (100k rows)figureqa(mathv360k) (17.6k rows)finqa (5.28k rows)funsd (194 rows)geo170k(align) (35.3k rows)geo170k(qa) (12.1k rows)geo3k (2.09k rows)geometry3k(mathv360k) (9.72k rows)geomverse (9.3k rows)geoqa+(mathv360k) (17.2k rows)geos(mathv360k) (498 rows)google_landmarks (300k rows)groundui (13.5k rows)handwriting_forms (1.4k rows)hateful_memes (8.5k rows)hitab (2.5k rows)hme100k (74.5k rows)hw_squad (20.5k rows)iam (5.66k rows)iconqa (27.3k rows)iconqa(mathv360k) (22.6k rows)idk (11.1k rows)iiit5k (1.99k rows)image_textualization(filtered) (99.6k rows)imgur5k (5.93k rows)indoor_qa (3.35k rows)infographic(gpt4v) (1.98k rows)infographic_vqa (4.",
    "id": "0035c042-1602-4f21-a25a-429192743f47",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "3k rows)iconqa(mathv360k) (22.6k rows)idk (11.1k rows)iiit5k (1.99k rows)image_textualization(filtered) (99.6k rows)imgur5k (5.93k rows)indoor_qa (3.35k rows)infographic(gpt4v) (1.98k rows)infographic_vqa (4.39k rows)infographic_vqa_llava_format (2.11k rows)intergps (1.28k rows)invoices_receipts (3.01k rows)k12_printing (257k rows)laion_gpt4v (9.3k rows)latex_handwritten (39.6k rows)latexformulas (552k rows)llavar_gpt4_20k (19.8k rows)lnqa (303k rows)localized_narratives (200k rows)lrv_chart (1.78k rows)lrv_normal(filtered) (10.5k rows)lvis_instruct4v (223k rows)mapqa (37.4k rows)mapqa(mathv360k) (5.23k rows)maptext (200 rows)mathwriting-google (300k rows)mavis_math_metagen (87.3k rows)mavis_math_rule_geo (100k rows)memotion (6.99k rows)mimic_cgd (70.9k rows)mmc_instruct (168k rows)mmevol (160k rows)mmra (1.02k rows)mmsoc_memotion (6.99k rows)multihiertt (7.62k rows)nlvr2 (50.4k rows)objects365_qa (1.67M rows)ocrvqa (166k rows)olmOCR-mix-0225-books (15.2k rows)olmOCR-mix-0225-documents (229k rows)oodvqa (8.",
    "id": "743082ef-8b01-4da2-a0b5-36ece69bccb2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "02k rows)mmsoc_memotion (6.99k rows)multihiertt (7.62k rows)nlvr2 (50.4k rows)objects365_qa (1.67M rows)ocrvqa (166k rows)olmOCR-mix-0225-books (15.2k rows)olmOCR-mix-0225-documents (229k rows)oodvqa (8.49k rows)orand_car_a (2k rows)pathvqa (32.6k rows)pdfvqa (8.59k rows)plotqa (157k rows)pmc_vqa(mathv360k) (35.9k rows)raven (42k rows)rendered_text (10k rows)robut_sqa (8.51k rows)robut_wikisql (75k rows)robut_wtq (38.2k rows)scienceqa (4.98k rows)scienceqa(nona_context) (19.2k rows)screen2words (15.7k rows)screenqa (80.8k rows)sharegpt4o (57.3k rows)sharegpt4v(coco) (50k rows)sharegpt4v(knowledge) (1.99k rows)sharegpt4v(llava) (30k rows)sharegpt4v(sam) (~8.72k rows, showing the first 1.51k)sketchyvqa (8k rows)slidevqa (1.92k rows)spark (3.9k rows)spatialsense (10.4k rows)spot_the_diff (8.57k rows)sroie (33.6k rows)st_vqa (17.2k rows)sujet_finance (9.8k rows)super_clevr(mathv360k) (8.64k rows)svrd (4.4k rows)synthdog (500k rows)tabmwp (22.",
    "id": "2156fd38-5f2d-4a3f-ac95-eb8b477e1b49",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "92k rows)spark (3.9k rows)spatialsense (10.4k rows)spot_the_diff (8.57k rows)sroie (33.6k rows)st_vqa (17.2k rows)sujet_finance (9.8k rows)super_clevr(mathv360k) (8.64k rows)svrd (4.4k rows)synthdog (500k rows)tabmwp (22.7k rows)tabmwp(mathv360k) (22.5k rows)tal_ocr_eng (257k rows)tallyqa (98.7k rows)tat_dqa (2.21k rows)tat_qa (2.2k rows)text_OpenMathInstruct-2 (1M rows)text_code_feedback (66.4k rows)text_codefeedback_filtered_instruction (157k rows)text_infinitymath (101k rows)text_mathinstruct (262k rows)text_mathqa (395k rows)text_mathstepdpo10k (10.8k rows)text_numinamath_cot (859k rows)text_openhermes_2_5 (1M rows)text_openorca (4.23M rows)text_orcamath (200k rows)text_pythoncode25k (49.6k rows)text_pythoncodealpaca (18.6k rows)text_ruozhiba (1.5k rows)text_theoremqa (800 rows)text_wizardlm_evol (70k rows)textcaps (21.9k rows)textocr(gpt4v) (25.1k rows)textvqa (21.9k rows)tqa (2.75k rows)unigeo(mathv360k) (11.9k rows)ureader_cap (91.2k rows)ureader_ie (17.3k rows)ureader_kg_processed (37.",
    "id": "435467ca-28eb-4b92-879e-b2335a4085d8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "9k rows)textocr(gpt4v) (25.1k rows)textvqa (21.9k rows)tqa (2.75k rows)unigeo(mathv360k) (11.9k rows)ureader_cap (91.2k rows)ureader_ie (17.3k rows)ureader_kg_processed (37.6k rows)ureader_qa_processed (253k rows)vision_flan(filtered) (176k rows)vistext (9.97k rows)visual7w (14.4k rows)visualmrc (3.03k rows)visualwebinstruct(filtered) (264k rows)vizwiz(mathv360k) (6.6k rows)vqaonbd (40k rows)vqarad (313 rows)vqav2 (82.8k rows)vsr (2.16k rows)websight (10k rows)wildvision (333 rows)wordart (4.8k rows)yesbut (4.32k rows) Split (1) train ¬∑ 117k rows train (117k rows) SQL Console images images listlengths 1 1 texts listlengths 1 15 source stringclasses 1 value relevance_ratings listlengths 1 15 relevance_min int64 1 5 visual_dependency_ratings listlengths 1 15 visual_dependency_min int64 1 5 image_correspondence_ratings listlengths 1 15 image_correspondence_min int64 1 5 formatting_ratings listlengths 1 15 formatting_min int64 1 5 [ { \"user\": \"What's the title of the figure?\", \"assistant\": \"The title of the figure is usually placed at the top. In this Ridgeline plot, it clearly reads \\\"Expatriate Population Trends in Major UAE Cities\\\".\\nAnswer: Expatriate Population Trends in Major UAE Cities\" }, { \"user\": \"Which city had the highest expatriate population in 2022?",
    "id": "6010c892-5e88-42dd-86c2-58a7ba6d252e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title of the figure is usually placed at the top. In this Ridgeline plot, it clearly reads \\\"Expatriate Population Trends in Major UAE Cities\\\".\\nAnswer: Expatriate Population Trends in Major UAE Cities\" }, { \"user\": \"Which city had the highest expatriate population in 2022?\", \"assistant\": \"To find the city with the highest expatriate population in 2022, we look at the topmost line in the Ridgeline plot for the year 2022. Dubai is at the top with the highest value.\\nAnswer: Dubai\" }, { \"user\": \"How did the expatriate population trend for Abu Dhabi change from 2008 to 2022?\", \"assistant\": \"To see the trend, trace the line corresponding to Abu Dhabi from 2008 to 2022. The line shows a steady increase from 0.62 million in 2008 to 1.47 million in 2022.\\nAnswer: Steady increase\" }, { \"user\": \"Compare the expatriate population in Sharjah and Ajman in 2014. Which city had more expats?\", \"assistant\": \"Find the lines corresponding to Sharjah and Ajman for the year 2014. Sharjah's population is higher than Ajman's, with values of 0.96 million and 0.39 million, respectively.\\nAnswer: Sharjah\" }, { \"user\": \"What's the average expatriate population in Ajman between 2008 and 2022?\", \"assistant\": \"First, sum the values for Ajman from 2008 to 2022: 0.21 + 0.28 + 0.34 + 0.39 + 0.44 + 0.49 + 0.53 + 0.56 = 3.24 million.",
    "id": "c4bae302-0dfe-41d8-ada5-2d9ae60d1072",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"First, sum the values for Ajman from 2008 to 2022: 0.21 + 0.28 + 0.34 + 0.39 + 0.44 + 0.49 + 0.53 + 0.56 = 3.24 million. There are 8 data points, so the average is 3.24 / 8.\\nAnswer: 0.405 million\" }, { \"user\": \"What is the general trend in expatriate populations across all cities from 2008 to 2022?\", \"assistant\": \"By examining all lines from left (2008) to right (2022), we can observe that all cities show an increasing trend in expatriate populations over time.\\nAnswer: Increasing trend\" }, { \"user\": \"How much did the expatriate population of Ras Al Khaimah grow from 2010 to 2022?\", \"assistant\": \"The population in Ras Al Khaimah in 2010 was 0.15 million and 0.32 million in 2022. The growth can be calculated as 0.32 - 0.15 = 0.17 million.\\nAnswer: 0.17 million\" }, { \"user\": \"Identify the city with the least noticeable growth in expatriate population from 2008 to 2022.\", \"assistant\": \"The city with the relatively smallest change over the years can be identified by comparing the vertical shifts in the baseline. Ras Al Khaimah shows the least growth, increasing from 0.12 million in 2008 to 0.32 million in 2022.\\nAnswer: Ras Al Khaimah\" }, { \"user\": \"Which year shows the highest number of expatriates in Dubai, and how many were there?\", \"assistant\": \"The year with the highest number is at the far right end for Dubai.",
    "id": "7b19ffc0-7cd3-4443-a9ae-1ef0bd66c319",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Ras Al Khaimah shows the least growth, increasing from 0.12 million in 2008 to 0.32 million in 2022.\\nAnswer: Ras Al Khaimah\" }, { \"user\": \"Which year shows the highest number of expatriates in Dubai, and how many were there?\", \"assistant\": \"The year with the highest number is at the far right end for Dubai. In 2022, Dubai's expatriate population reached 3.05 million.\\nAnswer: 2022, 3.05 million\" }, { \"user\": \"What was the difference in expatriate populations between Sharjah and Abu Dhabi in 2016?\", \"assistant\": \"For 2016, Sharjah had 1.05 million, and Abu Dhabi had 1.15 million. The difference is 1.15 - 1.05 = 0.10 million.\\nAnswer: 0.10 million\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 4, 5, 5, 5, 5, 5 ] 4 [ 5, 2, 2, 4, 2, 4, 2, 2, 4, 2 ] 2 [ 5, 5, 5, 5, 4, 5, 4, 5, 4, 5 ] 4 [ 5, 5, 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What proportion of genetic diseases are inherited through Autosomal dominant patterns?\", \"assistant\": \"From the pie chart, find the sector labeled \\\"Autosomal dominant\\\" and read the percentage value presented.\\nAnswer: 45%\" }, { \"user\": \"Which inheritance pattern has the smallest representation in the pie chart?\", \"assistant\": \"Look for the sector with the smallest percentage value.",
    "id": "c060c218-6321-4a91-ab02-531520e03f40",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"From the pie chart, find the sector labeled \\\"Autosomal dominant\\\" and read the percentage value presented.\\nAnswer: 45%\" }, { \"user\": \"Which inheritance pattern has the smallest representation in the pie chart?\", \"assistant\": \"Look for the sector with the smallest percentage value. The smallest slice is labeled \\\"Y-linked\\\" and \\\"X-linked dominant\\\", both contributing 0.5%.\\nAnswer: Y-linked and X-linked dominant\" }, { \"user\": \"How do the proportions of Autosomal recessive and Multifactorial diseases compare?\", \"assistant\": \"Identify the percentages for both Autosomal recessive (25%) and Multifactorial (8%). Autosomal recessive has a higher proportion than Multifactorial.\\nAnswer: Autosomal recessive is greater\" }, { \"user\": \"What are the combined percentages of X-linked recessive and Mitochondrial inheritance patterns?\", \"assistant\": \"Add the percentages of X-linked recessive (15%) and Mitochondrial (5%). 15% + 5% = 20%.\\nAnswer: 20%\" }, { \"user\": \"How much larger is the Autosomal dominant proportion compared to the X-linked recessive proportion?\", \"assistant\": \"Subtract the percentage of X-linked recessive (15%) from Autosomal dominant (45%). 45% - 15% = 30%.\\nAnswer: 30%\" }, { \"user\": \"Which inheritance pattern is represented by a blue slice in the pie chart?\", \"assistant\": \"Identify the blue slice in the pie chart associated with a specific inheritance pattern label. The blue slice corresponds to \\\"Autosomal recessive\\\".\\nAnswer: Autosomal recessive\" }, { \"user\": \"Add the proportions of all inheritance patterns except Autosomal dominant. What is the total?",
    "id": "14c01f10-21b7-4acc-aa72-3c51e4a84125",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Identify the blue slice in the pie chart associated with a specific inheritance pattern label. The blue slice corresponds to \\\"Autosomal recessive\\\".\\nAnswer: Autosomal recessive\" }, { \"user\": \"Add the proportions of all inheritance patterns except Autosomal dominant. What is the total?\", \"assistant\": \"Add percentages of Autosomal recessive (25%), X-linked recessive (15%), Mitochondrial (5%), Multifactorial (8%), Y-linked (0.5%), Codominant (1%), and X-linked dominant (0.5%). 25% + 15% + 5% + 8% + 0.5% + 1% + 0.5% = 55%.\\nAnswer: 55%\" }, { \"user\": \"Which inheritance pattern shares an equal proportion of 0.5% in genetic disease inheritance?\", \"assistant\": \"Identify the slices that each represent 0.5% and their associated labels. Both Y-linked and X-linked dominant have equal proportions of 0.5%.\\nAnswer: Y-linked and X-linked dominant\" }, { \"user\": \"Is the percentage of Mitochondrial inheritance greater than that of Codominant inheritance?\", \"assistant\": \"Compare the percentage values of Mitochondrial (5%) and Codominant (1%).",
    "id": "e1390de6-4c4c-46ef-ad7a-b683a1a69e0d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Identify the slices that each represent 0.5% and their associated labels. Both Y-linked and X-linked dominant have equal proportions of 0.5%.\\nAnswer: Y-linked and X-linked dominant\" }, { \"user\": \"Is the percentage of Mitochondrial inheritance greater than that of Codominant inheritance?\", \"assistant\": \"Compare the percentage values of Mitochondrial (5%) and Codominant (1%). 5% is greater than 1%.\\nAnswer: Yes\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 4, 2, 2, 4, 4, 4, 4, 2 ] 2 [ 5, 5, 4, 4, 5, 5, 5, 4, 4 ] 4 [ 4, 4, 4, 4, 4, 4, 5, 4, 4 ] 4 [ { \"user\": \"What is the most common architectural style among the historic buildings?\", \"assistant\": \"The figure shows the proportions of different architectural styles. Gothic has the largest segment.\\nAnswer: Gothic\" }, { \"user\": \"Which architectural style has a proportion that is more than twice that of Norman?\", \"assistant\": \"Check the percentage for Norman, which is 8%. Gothic (35%) and Tudor (22%) are more than double (8% x 2 = 16%).\\nAnswer: Gothic, Tudor\" }, { \"user\": \"How much more common is Gothic compared to Victorian?\", \"assistant\": \"Comparing the percentages: Gothic is 35%, and Victorian is 12%. The difference is 35% - 12%.\\nAnswer: 23%\" }, { \"user\": \"If you combine the proportions of Georgian and Victorian styles, what is their total percentage?",
    "id": "07f2523d-0dcf-4af2-88a3-cf9b7fc5accb",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Comparing the percentages: Gothic is 35%, and Victorian is 12%. The difference is 35% - 12%.\\nAnswer: 23%\" }, { \"user\": \"If you combine the proportions of Georgian and Victorian styles, what is their total percentage?\", \"assistant\": \"Add Georgian (18%) and Victorian (12%) percentages together: 18% + 12%.\\nAnswer: 30%\" }, { \"user\": \"Which architectural style is represented by the smallest segment in the pie chart?\", \"assistant\": \"Look at the smallest segment in the pie chart, which corresponds to Arts and Crafts (2%).\\nAnswer: Arts and Crafts\" }, { \"user\": \"Is the proportion of Tudor greater than the combined total of Norman and Jacobean?\", \"assistant\": \"Check the percentages: Tudor is 22%, Norman is 8%, and Jacobean is 3%. Combined total of Norman and Jacobean is 8% + 3% = 11%. 22% > 11%.\\nAnswer: Yes\" }, { \"user\": \"What is the range of the percentages shown in the pie chart?\", \"assistant\": \"Take the highest percentage, Gothic (35%), and the lowest, Arts and Crafts (2%), and find the difference: 35% - 2%.\\nAnswer: 33%\" }, { \"user\": \"What proportion of historic buildings have either Gothic or Arts and Crafts styles?\", \"assistant\": \"Add the percentages for Gothic (35%) and Arts and Crafts (2%) together: 35% + 2%.\\nAnswer: 37%\" }, { \"user\": \"By how much does the Georgian style exceed the Norman style in percentage?",
    "id": "fd6b1e55-6d37-4c4e-ad21-81bbdde5583b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Add the percentages for Gothic (35%) and Arts and Crafts (2%) together: 35% + 2%.\\nAnswer: 37%\" }, { \"user\": \"By how much does the Georgian style exceed the Norman style in percentage?\", \"assistant\": \"Subtract the Norman percentage (8%) from the Georgian percentage (18%): 18% - 8%.\\nAnswer: 10%\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 4, 4, 2, 3, 5, 4, 5, 3, 2 ] 2 [ 5, 5, 4, 4, 5, 5, 5, 5, 4 ] 4 [ 4, 4, 4, 4, 4, 5, 4, 4, 4 ] 4 [ { \"user\": \"What is the title of the plot?\", \"assistant\": \"The title is typically positioned at the top of the figure. In this case, it clearly mentions \\\"Comparative Analysis of Evacuation Times from High-Risk Locations.\\\"\\nAnswer: Comparative Analysis of Evacuation Times from High-Risk Locations\" }, { \"user\": \"How many subplots are there in the figure?\", \"assistant\": \"The figure is divided into sections, each containing its own graph. The titles of the subplots suggest there are four distinct subplots.\\nAnswer: 4\" }, { \"user\": \"Which location has the highest risk level?\", \"assistant\": \"Looking at the bar chart in the bottom right subplot (Risk Level by Location), the bar for Mogadishu_Somalia is the tallest, indicating the highest risk level.\\nAnswer: Mogadishu_Somalia\" }, { \"user\": \"What is the average evacuation time for Tripoli_Libya?",
    "id": "d315e29b-c706-404d-8868-e273ac4da247",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Looking at the bar chart in the bottom right subplot (Risk Level by Location), the bar for Mogadishu_Somalia is the tallest, indicating the highest risk level.\\nAnswer: Mogadishu_Somalia\" }, { \"user\": \"What is the average evacuation time for Tripoli_Libya?\", \"assistant\": \"Referring to the scatter plots (especially \\\"Evacuation Time vs Risk Level\\\" or \\\"Evacuation Time vs Population Density\\\"), hover info or the y-axis position for Tripoli_Libya can be noted.\\nAnswer: 75 minutes\" }, { \"user\": \"Which location has the lowest population density?\", \"assistant\": \"Check the values on the y-axis of the \\\"Population Density vs Risk Level\\\" scatter plot and locate the lowest point. Based on the data, this should be Juba_South_Sudan.\\nAnswer: Juba_South_Sudan\" }, { \"user\": \"What is the risk level for Caracas_Venezuela?\", \"assistant\": \"Using the bar chart in the \\\"Risk Level by Location\\\" subplot, find Caracas_Venezuela and read off its corresponding bar height.\\nAnswer: 7.6\" }, { \"user\": \"Compare the evacuation times between Juba_South_Sudan and Port-au-Prince_Haiti and identify which one has a longer time.\", \"assistant\": \"Locate both Juba_South_Sudan and Port-au-Prince_Haiti in the \\\"Evacuation Time vs Risk Level\\\" or \\\"Evacuation Time vs Population Density\\\" scatter plots, then compare their y-values. Juba_South_Sudan (~150 minutes) has a longer evacuation time than Port-au-Prince_Haiti (~95 minutes).\\nAnswer: Juba_South_Sudan\" }, { \"user\": \"Is there a general trend between risk level and average evacuation time?",
    "id": "acb5942c-3825-48bc-98d5-effb1df1ee5d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Juba_South_Sudan (~150 minutes) has a longer evacuation time than Port-au-Prince_Haiti (~95 minutes).\\nAnswer: Juba_South_Sudan\" }, { \"user\": \"Is there a general trend between risk level and average evacuation time?\", \"assistant\": \"Observing the \\\"Evacuation Time vs Risk Level\\\" subplot, one might note that higher risk levels generally correspond to higher evacuation times, forming a positive correlation overall.\\nAnswer: Positive correlation\" }, { \"user\": \"Which location with a risk level above 8 has the shortest evacuation time?\", \"assistant\": \"Examine the locations in the risk level by location bar chart with risk levels above 8 and identify the shortest evacuation time from the relevant scatter plots. Tripoli_Libya with a risk level of 7.9 was close but not above 8.\\nAnswer: Baghdad_Iraq\" }, { \"user\": \"In the \\\"Evacuation Time vs Population Density\\\" subplot, what is the population density for Caracas_Venezuela?\", \"assistant\": \"Locate Caracas_Venezuela in the scatter plot, then trace its position on the x-axis to find the population density value.\\nAnswer: 4900\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 3, 5 ] 3 [ 5, 5, 4, 2, 2, 2, 4, 2, 4, 4 ] 2 [ 5, 5, 5, 4, 4, 5, 4, 5, 5, 4 ] 4 [ 4, 5, 4, 4, 4, 4, 5, 4, 4, 4 ] 4 [ { \"user\": \"What's the package with the highest frequency of security updates?",
    "id": "f82591f0-119b-4597-adae-5189995ff81b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The package with the highest frequency of security updates is represented by the longest bar in the chart. By identifying which package bars extend the farthest to the right, we can find the package with the highest updates per year. In this case, it is GitLab.\\nAnswer: GitLab\" }, { \"user\": \"Which packages receive fewer updates per year than Kubernetes?\", \"assistant\": \"To answer this, we first locate Kubernetes on the y-axis, which corresponds to an update frequency of 12 per year. Then, by visually comparing the length of bars to the left, we identify the packages that have bars shorter than 12 updates. These packages are: Python (4), MySQL (4), PostgreSQL (4), Nginx (3), Django (3), Drupal (4), Spring Framework (4).\\nAnswer: Python, MySQL, PostgreSQL, Nginx, Django, Drupal, Spring Framework\" }, { \"user\": \"What is the difference in update frequency per year between Docker and TensorFlow?\", \"assistant\": \"Docker has an update frequency of 24 per year, while TensorFlow has 6. The difference can be calculated by subtracting 6 from 24.\\nAnswer: 18\" }, { \"user\": \"Which package has the same update frequency as Node.js?\", \"assistant\": \"Looking at the bar lengths, Node.js has an update frequency of 12. We then check for other packages with a 12 updates per year bar length. These packages are Kubernetes and Vue.js.\\nAnswer: Kubernetes, Vue.js\" }, { \"user\": \"How many packages receive 6 or more updates per year?\", \"assistant\": \"We count the number of bars that extend to or beyond the 6 updates per year mark.",
    "id": "ac0b656d-cb85-4a32-b463-394d0efebf7b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Looking at the bar lengths, Node.js has an update frequency of 12. We then check for other packages with a 12 updates per year bar length. These packages are Kubernetes and Vue.js.\\nAnswer: Kubernetes, Vue.js\" }, { \"user\": \"How many packages receive 6 or more updates per year?\", \"assistant\": \"We count the number of bars that extend to or beyond the 6 updates per year mark. These packages are Linux Kernel (8), OpenSSL (6), Node.js (12), Docker (24), Kubernetes (12), Apache HTTP Server (6), TensorFlow (6), React (24), Angular (18), Ruby on Rails (6), WordPress (12), GitLab (52).\\nAnswer: 12\" }, { \"user\": \"What is the combined update frequency per year for Angular, React, and Vue.js?\", \"assistant\": \"Angular has 18 updates per year, React has 24, and Vue.js has 12. Adding these together gives us 18 + 24 + 12.\\nAnswer: 54\" }, { \"user\": \"Is there any package that has an equal number of updates per year to both MySQL and PostgreSQL?\", \"assistant\": \"MySQL and PostgreSQL both have 4 updates per year. By checking the chart, we find that Python, Drupal, and Spring Framework also have 4 updates per year.\\nAnswer: Python, Drupal, Spring Framework\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 3 ] 3 [ 2, 4, 2, 4, 4, 2, 4 ] 2 [ 5, 5, 5, 4, 5, 5, 3 ] 3 [ 4, 4, 4, 4, 4, 5, 4 ] 4 [ { \"user\": \"what is the title of the plot?",
    "id": "2fff8de8-cfb3-462f-8d8c-3be0a4cd643b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title is prominently displayed at the top of the plot. It reads \\\"Statistical Significance of Ian Somerhalder's Roles.\\\"\\nAnswer: Statistical Significance of Ian Somerhalder's Roles\" }, { \"user\": \"How is the y-axis labeled?\", \"assistant\": \"The y-axis label is written vertically along the left side of the plot. It reads \\\"-log10(p-value)\\\".\\nAnswer: -log10(p-value)\" }, { \"user\": \"Which axis represents the filmography timeline?\", \"assistant\": \"The x-axis represents the filmography timeline. This can be inferred from its label \\\"Filmography Timeline\\\" at the bottom of the plot.\\nAnswer: x-axis\" }, { \"user\": \"What color line indicates the significance threshold?\", \"assistant\": \"The significance threshold is indicated by a red dashed line. This can be seen running horizontally across the plot.\\nAnswer: red dashed line\" }, { \"user\": \"Which role has the lowest p-value and what is it?\", \"assistant\": \"The role with the lowest p-value is Damon Salvatore (The Vampire Diaries) with a p-value represented as the highest -log10(p) value. This can be found by identifying the highest point along the y-axis.\\nAnswer: Damon Salvatore (The Vampire Diaries)\" }, { \"user\": \"How many roles have p-values below the significance threshold (p < 0.05)?\", \"assistant\": \"To determine this, we count the number of data points that fall above the red threshold line on the y-axis.\\nAnswer: 12\" }, { \"user\": \"Compare the -log10(p-value) for Damon Salvatore in \\\"The Vampire Diaries\\\" and \\\"The Vampire Diaries: A Darker Truth.\\\" Which one is higher?\", \"assistant\": \"Identify both roles on the scatter plot and observe their corresponding -log10(p) values.",
    "id": "78c6c76c-d42f-4c2b-9951-ed7d37d22bed",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Identify both roles on the scatter plot and observe their corresponding -log10(p) values. The one placed higher along the y-axis has a higher value.\\nAnswer: The Vampire Diaries: A Darker Truth\" }, { \"user\": \"What is the approximate range of the x-axis?\", \"assistant\": \"The x-axis ranges can be estimated by observing the minimum and maximum values at the x-axis ends. From the given data, it starts from 0 and extends a little beyond the largest Position value.\\nAnswer: 0 to approximately 71000\" }, { \"user\": \"Which role falls closest to the significance threshold line?\", \"assistant\": \"Identifying the role closest to the red dashed line involves finding the data point nearest to the threshold along the y-axis but not necessarily above it.\\nAnswer: Aaron Corbett (Fallen)\" }, { \"user\": \"What is the -log10(p-value) for Hamilton Fleming in \\\"Young Americans\\\"?\", \"assistant\": \"Find the data point marked with this role and observe its position along the y-axis. This value is directly given by its y position, translating the height into the -log10(p-value).\\nAnswer: approximately 2.054\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 3, 5, 3, 5, 3, 5 ] 3 [ 5, 5, 4, 4, 4, 5, 4, 4, 4, 4 ] 4 [ 5, 3, 4, 4, 4, 4, 1, 3, 4, 2 ] 1 [ 4, 4, 4, 5, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"How many family support programs are included in the visualization?\", \"assistant\": \"The figure shows data categorized under different family support programs.",
    "id": "ab2dd0d3-2d70-483d-8f2b-b52ad08254b6",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The figure shows data categorized under different family support programs. By counting the distinct labels on the x-axis, we can identify that there are three family support programs: Family Visitation Program, Parenting Skills Workshop, and Family Counseling Sessions.\\nAnswer: Three\" }, { \"user\": \"What is the range of recidivism rates for inmates participating in the Family Counseling Sessions?\", \"assistant\": \"The recidivism rates for the Family Counseling Sessions vary from the minimum data point (22%) to the maximum data point (30%).\\nAnswer: 22%-30%\" }, { \"user\": \"Which program shows the highest average recidivism rate?\", \"assistant\": \"By observing the dashed lines representing mean recidivism rates for each program, we can identify that the \\\"No Program Participation\\\" group has the highest average recidivism rate among all the programs.\\nAnswer: No Program Participation\" }, { \"user\": \"By roughly how much does the average recidivism rate differ between those who participated in the Family Visitation Program and those who did not participate in any program?\", \"assistant\": \"First, we identify the mean recidivism rates for both the Family Visitation Program and No Program Participation from the dashed lines. The figure shows that the Family Visitation Program has an average around 30%, while the No Program Participation group has an average around 49%. The difference is approximately 49% - 30% = 19%.\\nAnswer: 19%\" }, { \"user\": \"Based solely on the figure, which family support program has the lowest individual recidivism rate recorded, and what is that rate?\", \"assistant\": \"The lowest individual recidivism rate is found in the Family Counseling Sessions program, where the lowest point is 22%.\\nAnswer: Family Counseling Sessions, 22%\" }, { \"user\": \"How does participation in family support programs overall affect recidivism rates in comparison to non-participation, as shown in the visualization?",
    "id": "c5ec9139-e8de-4059-ac4b-ba0276cda9a8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The lowest individual recidivism rate is found in the Family Counseling Sessions program, where the lowest point is 22%.\\nAnswer: Family Counseling Sessions, 22%\" }, { \"user\": \"How does participation in family support programs overall affect recidivism rates in comparison to non-participation, as shown in the visualization?\", \"assistant\": \"Comparing the clustered points and their respective mean lines, it is clear that all three family support programs (Family Visitation Program, Parenting Skills Workshop, and Family Counseling Sessions) have lower recidivism rates than the No Program Participation category.\\nAnswer: Family support programs lower recidivism rates\" }, { \"user\": \"How many individual data points are plotted for each family support program, and how does this compare to those not in any program?\", \"assistant\": \"Count the number of points displayed for each category on the x-axis. Each family support program (Family Visitation Program, Parenting Skills Workshop, Family Counseling Sessions) has five data points, while the No Program Participation category also has five data points.\\nAnswer: Five each\" }, { \"user\": \"Does the Visualization specify the exact recidivism rates for each data point?\", \"assistant\": \"By examining the figure, it is evident that individual points (represented as dots) display the distribution of data but do not specify the exact recidivism rates without hovering or additional data reference.\\nAnswer: No\" }, { \"user\": \"Which family support program has data points closest to the 50% recidivism rate, and are there any points above this rate?\", \"assistant\": \"Observing the positions of the data points on the y-axis relative to the 50% mark, we see that none of the family support program categories have data points near or above the 50% recidivism rate.",
    "id": "0f3de2c2-581a-48a2-99e3-c30b773174c9",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Observing the positions of the data points on the y-axis relative to the 50% mark, we see that none of the family support program categories have data points near or above the 50% recidivism rate. Only the No Program Participation group approaches and surpasses this level.\\nAnswer: No Program Participation, Yes\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 2, 4, 4, 5, 5, 4, 4, 4 ] 2 [ 5, 4, 5, 4, 5, 5, 4, 3, 5 ] 3 [ 4, 4, 4, 5, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"what is the total attendance of the Australian Open in 2022?\", \"assistant\": \"To find the answer, locate the value for the Australian Open in 2022 from the chart and sum it directly since it's only asking for one year. The attendance in 2022 is 700000.\\nAnswer: 700000\" }, { \"user\": \"In which year did Wimbledon have zero attendance?\", \"assistant\": \"Find the point where Wimbledon's attendance is marked as 0. According to the chart, this occurred in 2020.\\nAnswer: 2020\" }, { \"user\": \"What is the difference in attendance between the US Open and the French Open in 2019?\", \"assistant\": \"Look at the attendance values for both the US Open and the French Open in 2019 and subtract the latter from the former. The values are 810,000 for the US Open and 520000 for the French Open.",
    "id": "b2c315ec-73a7-4e3d-adf1-74b194e02a0d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "According to the chart, this occurred in 2020.\\nAnswer: 2020\" }, { \"user\": \"What is the difference in attendance between the US Open and the French Open in 2019?\", \"assistant\": \"Look at the attendance values for both the US Open and the French Open in 2019 and subtract the latter from the former. The values are 810,000 for the US Open and 520000 for the French Open. So, 810000 - 520000 = 290000\\nAnswer: 290000\" }, { \"user\": \"How did the attendance for major tournaments change in 2020 compared to 2019?\", \"assistant\": \"Compare the attendance values for each tournament in 2019 and 2020. Attendance significantly decreased for all tournaments in 2020 compared to 2019, with Wimbledon having 0 attendance due to cancellation.\\nAnswer: Decreased drastically\" }, { \"user\": \"Which tournament had the highest attendance in 2015?\", \"assistant\": \"Compare the attendance values for all four tournaments in 2015. The values are 660,000 for the Australian Open, 490,000 for the French Open, 590,000 for Wimbledon, and 775,000 for the US Open. The US Open has the highest attendance.\\nAnswer: US Open\" }, { \"user\": \"What was the approximate average attendance for the French Open between 2007 and 2011?\", \"assistant\": \"Sum the attendance values for the French Open from 2007 to 2011, then divide by the number of years (5). The values are 410000, 420000, 430000, 445000, 455000. So, the average attendance = (410000 + 420000 + 430000 + 445000 + 455000) / 5 = 432000\\nAnswer: 432000\" }, { \"user\": \"In which years did the Australian Open see the highest incremental increase in attendance?",
    "id": "3ae1ebc2-49df-4c33-9b43-b6ef12f11ef9",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The values are 410000, 420000, 430000, 445000, 455000. So, the average attendance = (410000 + 420000 + 430000 + 445000 + 455000) / 5 = 432000\\nAnswer: 432000\" }, { \"user\": \"In which years did the Australian Open see the highest incremental increase in attendance?\", \"assistant\": \"Identify the point where the difference between consecutive years is the largest for the Australian Open. The largest incremental increases are found by observing the jumps in values for each year, which is between 2019 and 2020 (710000 to 100000), making it a special case, but the next largest would be between 2015 and 2016 (660000 to 675000)\\nAnswer: 2019-2020 and 2015-2016\" }, { \"user\": \"How does the attendance trend for the US Open between 2003 and 2019 appear?\", \"assistant\": \"Examine the attendance values for the US Open from 2003 through 2019. The trend shows a steady increase in attendance each year until 2019.\\nAnswer: Steady increase\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 1, 4, 2, 2, 4, 2, 2, 4 ] 1 [ 4, 4, 5, 5, 5, 4, 4, 5 ] 4 [ 5, 4, 5, 4, 5, 5, 4, 4 ] 4 [ { \"user\": \"what percentage of land is irrigated?\", \"assistant\": \"The pie chart lists the percentage of irrigated vs. non-irrigated land.",
    "id": "650b0051-658e-4b48-a918-9ef40f14edd5",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The pie chart lists the percentage of irrigated vs. non-irrigated land. The segment labeled \\\"Irrigated Land\\\" is marked with 65%, indicating the irrigated portion of the land.\\nAnswer: 65%\" }, { \"user\": \"What percentage of land is non-irrigated?\", \"assistant\": \"The pie chart displays labels for both irrigated and non-irrigated land, with the \\\"Non-Irrigated Land\\\" segment showing 35%.\\nAnswer: 35%\" }, { \"user\": \"How does the percentage of irrigated land compare to non-irrigated land?\", \"assistant\": \"By looking at the pie chart, the \\\"Irrigated Land\\\" portion is 65% and the \\\"Non-Irrigated Land\\\" portion is 35%. 65% is almost double 35%, and therefore, irrigated land is significantly greater in percentage than non-irrigated land.\\nAnswer: Irrigated land is nearly double the non-irrigated land\" }, { \"user\": \"What is the difference in percentage between irrigated and non-irrigated land?\", \"assistant\": \"To find the difference, subtract the non-irrigated land percentage (35%) from the irrigated land percentage (65%): 65% - 35% = 30%.\\nAnswer: 30%\" }, { \"user\": \"If the total land area is 100 hectares, how many hectares are non-irrigated?\", \"assistant\": \"Given that 35% of the total land is non-irrigated, calculate 35% of 100 hectares by multiplying: 100  0.35 = 35 hectares.\\nAnswer: 35 hectares\" }, { \"user\": \"What color is used to represent the irrigated land?\", \"assistant\": \"The pie chart uses different colors to differentiate between categories.",
    "id": "a985b5fe-2592-416d-9edd-6dfe9b21c4de",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Given that 35% of the total land is non-irrigated, calculate 35% of 100 hectares by multiplying: 100  0.35 = 35 hectares.\\nAnswer: 35 hectares\" }, { \"user\": \"What color is used to represent the irrigated land?\", \"assistant\": \"The pie chart uses different colors to differentiate between categories. The segment for \\\"Irrigated Land\\\" is displayed in green.\\nAnswer: Green\" }, { \"user\": \"What color is used to represent the non-irrigated land?\", \"assistant\": \"The pie chart uses different colors to differentiate between categories. The segment for \\\"Non-Irrigated Land\\\" is displayed in an orange shade.\\nAnswer: Orange\" }, { \"user\": \"If we convert the percentages into ratios, what is the ratio of irrigated land to non-irrigated land?\", \"assistant\": \"The ratio of irrigated to non-irrigated land is determined by their percentage values: 65% (irrigated) to 35% (non-irrigated). Simplify this ratio: 65/35 = 13/7. So, the ratio is 13:7.\\nAnswer: 13:7\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 2, 2, 2, 3, 4, 4, 3 ] 2 [ 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ 4, 4, 5, 5, 4, 4, 4, 5 ] 4 [ { \"user\": \"Which player scored the highest in any single game?\", \"assistant\": \"To find the player with the highest score in any single game, observe the peaks of the curves.",
    "id": "46bb5520-0d76-4a34-8003-aa122de2c8bf",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"To find the player with the highest score in any single game, observe the peaks of the curves. Player 4 achieves a score of 34 in Game 19.\\nAnswer: Player 4\" }, { \"user\": \"Who had the most consistent performance across the games, based on the smoothness of their curve?\", \"assistant\": \"Consistent performance implies less fluctuation in scores. By observing the smoothness of the curves, Player 5's curve appears smoother with less fluctuation compared to others.\\nAnswer: Player 5\" }, { \"user\": \"During which game did Player 3 score the lowest, and what was the score?\", \"assistant\": \"Look at the dips in Player 3's curve to identify the lowest point. The lowest score is 19, which occurs during Game 26.\\nAnswer: Game 26; 19\" }, { \"user\": \"What is the average score of Player 1 across all games?\", \"assistant\": \"Calculate the average by summing all scores of Player 1 and dividing by the total number of games: (15+18+22+17+20+25+19+23+28+22+24+27+30+29+25+21+26+28+24+27+30+28+25+23+29+27+26+31+30+28) / 30 = 690 / 30\\nAnswer: 23\" }, { \"user\": \"Between Game 10 and Game 20, which player had the highest average score?",
    "id": "b6f8cd11-28fd-49f7-9f0f-41ed7e2831bc",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Calculate the average score for each player between Game 10 and Game 20:\\n- Player 1: (22+24+27+30+29+25+21+26+28+24+27) / 11 = 283 / 11 ‚âà 25.73\\n- Player 2: (26+29+25+22+23+24+27+22+26+29+24) / 11 = 277 / 11 ‚âà 25.18\\n- Player 3: (24+28+22+24+26+30+28+31+27+29+32) / 11 = 301 / 11 ‚âà 27.36\\n- Player 4: (28+30+30+26+31+28+25+30+34+33+31) / 11 = 326 / 11 ‚âà 29.64\\n- Player 5: (30+19+28+21+22+26+23+24+22+29+26) / 11 = 270 / 11 ‚âà 24.55\\nPlayer 4 has the highest average score.\\nAnswer: Player 4\" }, { \"user\": \"Which player improved their performance the most from Game 1 to Game 30?\", \"assistant\": \"Improvement can be evaluated by comparing the score at Game 1 with the score at Game 30 for each player:\\n- Player 1: 28 - 15 = 13\\n- Player 2: 26 - 20 = 6\\n- Player 3: 27 - 18 = 9\\n- Player 4: 27 - 22 = 5\\n- Player 5: 21 - 17 = 4\\nPlayer 1 shows the most improvement with an increase of 13 points.\\nAnswer: Player 1\" }, { \"user\": \"How many games did Player 2 outperform Player 3?",
    "id": "1ce1467f-c291-4e31-90c3-0e94cfcb1875",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Compare the scores of Player 2 and Player 3 for each game and count how many times Player 2's score is higher:\\n- Player 2 outperforms Player 3 in Game 1, 2, 4, 5, 7, 8, 10, 11, 12, 13, 14, 16, 17, 19, 23, 24, 25, 26, 29, and 30 (20 times in total).\\nAnswer: 20\" }, { \"user\": \"Which game had the highest total combined score of all players?\", \"assistant\": \"To find the highest combined score, sum up the scores for each game and identify the maximum:\\n- Game 19: 24+29+27+34+22 = 136 (has the highest combined score).\\nAnswer: Game 19\" }, { \"user\": \"At which game did Player 5 reach their peak score, and what was the peak?\", \"assistant\": \"Identify the peak of Player 5's curve by locating the highest point. The highest score for Player 5 is 30 in Game 10.\\nAnswer: Game 10; 30\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 4, 4, 3, 3, 1, 4, 4, 4, 3 ] 1 [ 5, 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ 5, 4, 5, 5, 5, 5, 5, 5, 5 ] 4 [ { \"user\": \"Which genre had the higher average box office revenue in the 1920s?",
    "id": "c52c819e-94d9-43b6-af2f-04eb14b121b2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"By looking at the data points for the 1920s, Comedy has an average box office revenue of $3,500,000 while Drama has $4,200,000. Therefore, Drama had the higher average in the 1920s.\\nAnswer: Drama\" }, { \"user\": \"How did the average box office revenue for Comedy films change from the 1920s to the 1980s?\", \"assistant\": \"In the 1920s, Comedy films had an average revenue of $3,500,000, and by the 1980s, it decreased to $600,000. To find the change, subtract $600,000 from $3,500,000, which is $2,900,000.\\nAnswer: Decreased by $2,900,000\" }, { \"user\": \"Which decade shows the biggest difference in average box office revenue between Comedy and Drama genres?\", \"assistant\": \"Look at the differences in each decade: \\n1920s: $700,000 \\n1930s: $700,000 \\n1940s: $700,000 \\n1950s: $500,000 \\n1960s: $500,000 \\n1970s: $400,000 \\n1980s: $300,000 \\n1990s: $400,000 \\n2000s: $300,000 \\n2010s: $250,000. The biggest difference is in the 1920s.\\nAnswer: 1920s\" }, { \"user\": \"Which genre had a consistent decline in box office revenues from the 1920s to the 2010s?\", \"assistant\": \"By examining each genre's trend across the decades, Comedy consistently declines from $3,500,000 in the 1920s to $150,000 in the 2010s.\\nAnswer: Comedy\" }, { \"user\": \"In what decade did Drama films first fall below $1,000,000 in average box office revenue?",
    "id": "25eb7d11-5217-4719-ad84-c0d198cf7f1a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"By examining each genre's trend across the decades, Comedy consistently declines from $3,500,000 in the 1920s to $150,000 in the 2010s.\\nAnswer: Comedy\" }, { \"user\": \"In what decade did Drama films first fall below $1,000,000 in average box office revenue?\", \"assistant\": \"Observing the data points for Drama, it first falls below $1,000,000 in the 1980s with an average revenue of $900,000.\\nAnswer: 1980s\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5 ] 5 [ 2, 2, 4, 2, 5 ] 2 [ 5, 4, 5, 5, 4 ] 4 [ 5, 5, 4, 4, 5 ] 4 [ { \"user\": \"what is the academic performance score in Spring 2023?\", \"assistant\": \"We can look at the blue line representing Academic Performance and find the value for Spring 2023. The score is at 82.6.\\nAnswer: 82.6\" }, { \"user\": \"How many counseling sessions were attended by students in Fall 2022?\", \"assistant\": \"We refer to the red line representing Counseling Sessions and find the value for Fall 2022. The number of sessions attended is 7.\\nAnswer: 7\" }, { \"user\": \"By how much did the event participation rate increase from Fall 2021 to Spring 2023?\", \"assistant\": \"Looking at the green line representing Event Participation Rate, we see that it increased from 45% in Fall 2021 to 68% in Spring 2023. The increase is 68% - 45% = 23%.\\nAnswer: 23%\" }, { \"user\": \"Which semester had the lowest bakery involvement score?",
    "id": "01082caf-e457-497f-b944-33e31072ee3c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Looking at the green line representing Event Participation Rate, we see that it increased from 45% in Fall 2021 to 68% in Spring 2023. The increase is 68% - 45% = 23%.\\nAnswer: 23%\" }, { \"user\": \"Which semester had the lowest bakery involvement score?\", \"assistant\": \"By observing the orange diamond markers, we find the lowest bakery involvement score in Fall 2021, with a score of 2.1.\\nAnswer: Fall 2021\" }, { \"user\": \"In which semester was the difference between the number of counseling sessions and the bakery involvement score the largest?\", \"assistant\": \"We need to calculate the difference in each semester:\\nFall 2021: 3 - 2.1 = 0.9\\nSpring 2022: 5 - 2.7 = 2.3\\nFall 2022: 7 - 3.4 = 3.6\\nSpring 2023: 8 - 4.2 = 3.8\\nThe difference is largest in Spring 2023.\\nAnswer: Spring 2023\" }, { \"user\": \"How did the academic performance score change from Fall 2021 to Spring 2023?\", \"assistant\": \"Analyzing the blue line, we observe that the score increased from 72.5 in Fall 2021 to 82.6 in Spring 2023. The change is 82.6 - 72.5 = 10.1 points.\\nAnswer: 10.1 points\" }, { \"user\": \"What is the color representing event participation rate in the figure?\", \"assistant\": \"Looking at the legend of the plot, the line representing Event Participation is colored green.\\nAnswer: Green\" }, { \"user\": \"What was the average number of counseling sessions per semester?",
    "id": "951185e1-ec8e-4ca6-a66b-ec1a1f3ef9e4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The change is 82.6 - 72.5 = 10.1 points.\\nAnswer: 10.1 points\" }, { \"user\": \"What is the color representing event participation rate in the figure?\", \"assistant\": \"Looking at the legend of the plot, the line representing Event Participation is colored green.\\nAnswer: Green\" }, { \"user\": \"What was the average number of counseling sessions per semester?\", \"assistant\": \"Adding the number of sessions for each semester and dividing by 4: (3 + 5 + 7 + 8) / 4 = 23 / 4 = 5.75.\\nAnswer: 5.75\" }, { \"user\": \"Is there a correlation between the number of counseling sessions and academic performance scores across the semesters?\", \"assistant\": \"Observing the red and blue lines, we see that as the number of counseling sessions increased, the academic performance scores also increased. This suggests a positive correlation.\\nAnswer: Yes\" }, { \"user\": \"Between which two consecutive semesters was the increase in bakery involvement score the greatest?",
    "id": "3253969a-d5cf-4d44-9ad8-58a3c5b5f27b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Observing the red and blue lines, we see that as the number of counseling sessions increased, the academic performance scores also increased. This suggests a positive correlation.\\nAnswer: Yes\" }, { \"user\": \"Between which two consecutive semesters was the increase in bakery involvement score the greatest?\", \"assistant\": \"Calculating the differences between consecutive semesters:\\nFall 2021 to Spring 2022: 2.7 - 2.1 = 0.6\\nSpring 2022 to Fall 2022: 3.4 - 2.7 = 0.7\\nFall 2022 to Spring 2023: 4.2 - 3.4 = 0.8\\nThe greatest increase was between Fall 2022 and Spring 2023.\\nAnswer: Fall 2022 and Spring 2023\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 1, 5, 4, 5, 1, 4, 2, 2, 4 ] 1 [ 5, 1, 2, 2, 2, 5, 3, 2, 4, 1 ] 1 [ 5, 4, 5, 5, 5, 5, 5, 5, 4, 5 ] 4 [ { \"user\": \"Which disease category has the highest percentage of public health expenditure in developed countries?\", \"assistant\": \"The bar representing Non-Communicable Diseases is the tallest among those for developed countries, indicating the highest percentage.\\nAnswer: Non-Communicable Diseases\" }, { \"user\": \"Which disease category has a higher percentage of expenditure in developing countries compared to developed countries?\", \"assistant\": \"Look for bars that are taller in the developing countries series.",
    "id": "866f74d8-b62e-48d0-9d2b-657cbb2ca1f4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The bar representing Non-Communicable Diseases is the tallest among those for developed countries, indicating the highest percentage.\\nAnswer: Non-Communicable Diseases\" }, { \"user\": \"Which disease category has a higher percentage of expenditure in developing countries compared to developed countries?\", \"assistant\": \"Look for bars that are taller in the developing countries series. The bars for Infectious Diseases, Maternal and Child Health, Injuries and Violence, and Nutrition-Related Conditions are taller.\\nAnswer: Infectious Diseases, Maternal and Child Health, Injuries and Violence, Nutrition-Related Conditions\" }, { \"user\": \"How much more or less is spent on Mental Health in developed countries compared to developing countries?\", \"assistant\": \"Compare the height of the bars for Mental Health in both categories. The bar for developed countries is at 10%, and for developing countries, it is at 3%. The difference is 10% - 3% = 7%.\\nAnswer: 7% more\" }, { \"user\": \"What is the combined percentage of expenditure on Infectious Diseases and Non-Communicable Diseases in developing countries?\", \"assistant\": \"Add the percentages of Infectious Diseases (22%) to Non-Communicable Diseases (48%) for developing countries. The sum is 22% + 48% = 70%.\\nAnswer: 70%\" }, { \"user\": \"On which category do developed countries spend more than 60% of their public health expenditure?\", \"assistant\": \"Identify the category for developed countries with a bar height greater than 60%. Non-Communicable Diseases at 70% fulfills this condition.\\nAnswer: Non-Communicable Diseases\" }, { \"user\": \"What is the total percentage of public health expenditure on Mental Health and Injuries and Violence in developed countries?\", \"assistant\": \"Add the percentages for Mental Health (10%) and Injuries and Violence (4%) in developed countries.",
    "id": "73c4cbed-1f43-434f-a7be-076ef4f07a01",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Identify the category for developed countries with a bar height greater than 60%. Non-Communicable Diseases at 70% fulfills this condition.\\nAnswer: Non-Communicable Diseases\" }, { \"user\": \"What is the total percentage of public health expenditure on Mental Health and Injuries and Violence in developed countries?\", \"assistant\": \"Add the percentages for Mental Health (10%) and Injuries and Violence (4%) in developed countries. The sum is 10% + 4% = 14%.\\nAnswer: 14%\" } ] CoSyn-400k-chart [ 5, 5, 3, 5, 5, 5 ] 3 [ 2, 2, 2, 4, 4, 2 ] 2 [ 5, 5, 5, 5, 4, 4 ] 4 [ 4, 4, 5, 4, 4, 4 ] 4 [ { \"user\": \"What's the trend in the cost of saddles from 2000 to 2022?\", \"assistant\": \"From the figure, the cost of saddles steadily increases over time from 850 in 2000 to 1250 in 2022. The slope of the line indicates a consistent rise.\\nAnswer: The cost of saddles increases over time\" }, { \"user\": \"In which year did the cost of helmets reach 150 dollars?\", \"assistant\": \"From the figure, the line representing helmets reaches 150 dollars in the year 2010. Identify where the helmet line intersects with the 150-dollar mark on the Y-axis.\\nAnswer: 2010\" }, { \"user\": \"By how much did the cost of riding boots increase from 2000 to 2022?\", \"assistant\": \"In 2000, the cost of riding boots was 180, and in 2022, it was 290.",
    "id": "ec64d4fa-3acb-4046-8670-7e56d6052d74",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Identify where the helmet line intersects with the 150-dollar mark on the Y-axis.\\nAnswer: 2010\" }, { \"user\": \"By how much did the cost of riding boots increase from 2000 to 2022?\", \"assistant\": \"In 2000, the cost of riding boots was 180, and in 2022, it was 290. Calculate the difference: 290 - 180 = 110.\\nAnswer: 110\" }, { \"user\": \"How does the increase in cost for saddles compare to that for helmets between 2000 and 2022?\", \"assistant\": \"Saddles increased from 850 to 1250 (an increase of 400), while helmets increased from 120 to 210 (an increase of 90). The increase for saddles is 400, which is greater than the increase for helmets (90).\\nAnswer: Saddles increased by 400, helmets by 90\" }, { \"user\": \"Which category consistently had the highest costs over the years?\", \"assistant\": \"From the figure, the line representing saddles is always higher on the Y-axis compared to helmets and riding boots for every year.\\nAnswer: Saddles\" }, { \"user\": \"Did the cost of any equipment decrease at any point in time?\", \"assistant\": \"Examine the lines for any dips or downward trends. All lines for saddles, helmets, and riding boots show a consistent upward trend with no decreases.\\nAnswer: No\" }, { \"user\": \"What is the average cost of riding boots from 2000 to 2022?\", \"assistant\": \"Adding all values: 180 + 190 + 200 + 210 + 220 + 230 + 240 + 250 + 260 + 270 + 280 + 290 = 2820. Divide by 12 (number of years): 2820 / 12 = 235.\\nAnswer: 235\" }, { \"user\": \"Which year saw the highest rate of increase in the cost of helmets?",
    "id": "8c7620ca-c2d2-4cbf-9f14-7290d85bcf7a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Adding all values: 180 + 190 + 200 + 210 + 220 + 230 + 240 + 250 + 260 + 270 + 280 + 290 = 2820. Divide by 12 (number of years): 2820 / 12 = 235.\\nAnswer: 235\" }, { \"user\": \"Which year saw the highest rate of increase in the cost of helmets?\", \"assistant\": \"Identify the years with the steepest slopes in the helmet cost line. The slope from 2010-2012 (150 to 160) and 2012-2014 (160 to 170) shows a steeper rise compared to other years.\\nAnswer: 2010-2012 and 2012-2014\" }, { \"user\": \"By how much did the cost of saddles increase during the period from 2004 to 2006?\", \"assistant\": \"In 2004, the cost of saddles was 900, and in 2006 it was 925. Calculate the difference: 925 - 900 = 25.\\nAnswer: 25\" }, { \"user\": \"Which category saw the most significant change in cost between 2000 and 2006?\", \"assistant\": \"Compare the differences: Saddles increased from 850 to 925 (75), helmets from 120 to 135 (15), and riding boots from 180 to 210 (30).",
    "id": "f10e9050-2432-48b3-bf8f-e5b127ebf237",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Calculate the difference: 925 - 900 = 25.\\nAnswer: 25\" }, { \"user\": \"Which category saw the most significant change in cost between 2000 and 2006?\", \"assistant\": \"Compare the differences: Saddles increased from 850 to 925 (75), helmets from 120 to 135 (15), and riding boots from 180 to 210 (30). The most significant change is in saddles with an increase of 75.\\nAnswer: Saddles\" } ] CoSyn-400k-chart [ 3, 5, 5, 5, 5, 5, 1, 3, 5, 5 ] 1 [ 3, 2, 3, 3, 4, 2, 2, 2, 2, 4 ] 2 [ 5, 4, 5, 5, 5, 4, 4, 4, 5, 5 ] 4 [ 4, 4, 4, 5, 4, 4, 4, 4, 4, 5 ] 4 [ { \"user\": \"What's the most abundant material used in vintage typewriters?\", \"assistant\": \"The pie chart shows that Cast Iron is represented by the largest segment.\\nAnswer: Cast Iron\" }, { \"user\": \"Which two materials have the smallest percentages?\", \"assistant\": \"By examining the smallest segments of the pie chart, Bakelite and Leather have the lowest values.\\nAnswer: Bakelite and Leather\" }, { \"user\": \"What's the combined percentage of Nickel, Wood, and Glass?\", \"assistant\": \"Sum the percentages of Nickel (5%), Wood (4%), and Glass (3%): 5 + 4 + 3.\\nAnswer: 12%\" }, { \"user\": \"Is Steel used more or less than Brass in vintage typewriters?\", \"assistant\": \"Compare the segments for Steel (25%) and Brass (15%).",
    "id": "40d63667-1803-4fc2-b889-89fbc0215fca",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Sum the percentages of Nickel (5%), Wood (4%), and Glass (3%): 5 + 4 + 3.\\nAnswer: 12%\" }, { \"user\": \"Is Steel used more or less than Brass in vintage typewriters?\", \"assistant\": \"Compare the segments for Steel (25%) and Brass (15%). Steel is greater than Brass.\\nAnswer: More\" }, { \"user\": \"How does the percentage of Rubber compare to the sum of Glass and Bakelite?\", \"assistant\": \"Rubber is 10%, while the sum of Glass (3%) and Bakelite (2%) is 5%. Rubber is greater.\\nAnswer: Greater\" }, { \"user\": \"What's the difference in percentage between the most and least used materials?\", \"assistant\": \"Subtract the percentage of Leather (1%) from the percentage of Cast Iron (35%): 35 - 1.\\nAnswer: 34%\" }, { \"user\": \"If the 'pull' effect was applied to emphasize the largest segment, which material would that be?\", \"assistant\": \"The 'pull' effect is applied to the largest segment, which is Cast Iron.\\nAnswer: Cast Iron\" }, { \"user\": \"Which material has the second smallest percentage, and what is it?\", \"assistant\": \"The second smallest segment is Bakelite, which has a percentage of 2%.\\nAnswer: Bakelite, 2%\" }, { \"user\": \"What's the percentage difference between the most used metal (Cast Iron) and the least used non-metal (Leather)?",
    "id": "8f51386e-f5f2-4c44-9f65-79ee13b102d8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The second smallest segment is Bakelite, which has a percentage of 2%.\\nAnswer: Bakelite, 2%\" }, { \"user\": \"What's the percentage difference between the most used metal (Cast Iron) and the least used non-metal (Leather)?\", \"assistant\": \"Subtract Leather's percentage (1%) from Cast Iron's percentage (35%): 35 - 1.\\nAnswer: 34%\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 3, 4, 4, 2, 4, 5, 4, 4, 4 ] 2 [ 5, 5, 4, 5, 5, 5, 4, 5, 4 ] 4 [ 4, 4, 4, 4, 4, 5, 5, 4, 4 ] 4 [ { \"user\": \"How many Oscar nominations did the \\\"Best Picture\\\" category have in the 2010s?\", \"assistant\": \"Look at the subplot titled \\\"Best Picture\\\" and refer to the bar that represents the 2010s. The label above this bar shows the number of nominations.\\nAnswer: 20\" }, { \"user\": \"Which category saw an increase from the 1990s to the 2000s and again from the 2000s to the 2010s?",
    "id": "d3c9b487-19f6-4516-bebd-fcec9127c509",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Look at the subplot titled \\\"Best Picture\\\" and refer to the bar that represents the 2010s. The label above this bar shows the number of nominations.\\nAnswer: 20\" }, { \"user\": \"Which category saw an increase from the 1990s to the 2000s and again from the 2000s to the 2010s?\", \"assistant\": \"Identify the subplots where the bars for each decade (1990s, 2000s, 2010s) show an increasing trend.\\nAnswer: Best Picture, Best Director, Best Actor, Best Actress, Best Supporting Actor, Best Supporting Actress, Best Original Screenplay, Best Adapted Screenplay, Best Cinematography, Best Film Editing, Best Production Design, Best Costume Design, Best Original Score, Best Visual Effects\" }, { \"user\": \"What is the total number of Oscar nominations for the \\\"Best Actor\\\" category across all decades?\", \"assistant\": \"Add the number of nominations for \\\"Best Actor\\\" from the 1990s, 2000s, and 2010s using the data displayed in the respective subplot. 10 + 12 + 15 = 37\\nAnswer: 37\" }, { \"user\": \"Which category had more nominations in the 2010s: \\\"Best Original Screenplay\\\" or \\\"Best Adapted Screenplay\\\"?\", \"assistant\": \"Compare the number of nominations in the 2010s for both \\\"Best Original Screenplay\\\" and \\\"Best Adapted Screenplay\\\". As per the subplots, \\\"Best Original Screenplay\\\" has 17 and \\\"Best Adapted Screenplay\\\" has 16.\\nAnswer: Best Original Screenplay\" }, { \"user\": \"What was the difference in nominations between \\\"Best Cinematography\\\" and \\\"Best Supporting Actress\\\" in the 2000s?\", \"assistant\": \"Subtract the number of nominations for the \\\"Best Supporting Actress\\\" category in the 2000s from those of \\\"Best Cinematography\\\" in the same decade.",
    "id": "43eaf310-2c64-44f3-a1e1-111c1dad1519",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Subtract the number of nominations for the \\\"Best Supporting Actress\\\" category in the 2000s from those of \\\"Best Cinematography\\\" in the same decade. 16 - 10 = 6\\nAnswer: 6\" }, { \"user\": \"Which category had the least nominations in the 1990s?\", \"assistant\": \"Identify the subplot with the lowest bar for the 1990s. \\\"Best Visual Effects\\\" has the least with 8 nominations.\\nAnswer: Best Visual Effects\" }, { \"user\": \"How many more nominations did \\\"Best Director\\\" receive in the 2010s compared to the 1990s?\", \"assistant\": \"Subtract the number of nominations in the 1990s from the number in the 2010s for \\\"Best Director\\\". 16 - 12 = 4\\nAnswer: 4\" }, { \"user\": \"Are there any categories where the number of nominations stayed constant across two consecutive decades?\", \"assistant\": \"Check each subplot to see if any category has the same number of nominations in two consecutive decades.\\nAnswer: No\" } ] CoSyn-400k-chart [ 5, 3, 5, 5, 5, 5, 5, 3 ] 3 [ 2, 5, 2, 2, 5, 2, 2, 4 ] 2 [ 5, 5, 4, 5, 5, 5, 5, 4 ] 4 [ 4, 4, 4, 4, 5, 4, 4, 4 ] 4 [ { \"user\": \"Which weight category won the most gold medals?\", \"assistant\": \"By examining the heights of the bars, the weight category with the tallest bar represents the most gold medals won.",
    "id": "1f193938-5f08-487a-928c-ea0d2ba7c92d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"By examining the heights of the bars, the weight category with the tallest bar represents the most gold medals won. The \\\"67kg\\\" category has the tallest bar with 15 gold medals.\\nAnswer: 67kg\" }, { \"user\": \"Which weight category has the fewest gold medals among the women?\", \"assistant\": \"By looking at the heights of the bars for the female categories (those with \\\"Women\\\"), we see that \\\"+75kg (Women)\\\" has the shortest bar with 3 gold medals.\\nAnswer: +75kg (Women)\" }, { \"user\": \"How many more gold medals does the 61kg category have compared to the 109kg category?\", \"assistant\": \"The 61kg category has 14 gold medals and the 109kg category has 6 gold medals. The difference is 14 - 6.\\nAnswer: 8\" }, { \"user\": \"Sum up the total number of gold medals won by all male categories.\", \"assistant\": \"Add the counts of gold medals for all male categories: 55kg (12) + 61kg (14) + 67kg (15) + 73kg (13) + 81kg (11) + 89kg (9) + 96kg (8) + 102kg (7) + 109kg (6) + +109kg (10) = 105.\\nAnswer: 105\" }, { \"user\": \"Which gender category (men's or women's) has won more gold medals in total?\", \"assistant\": \"Sum the gold medals for all male and female categories. Males: 105. Females: 48kg (Women) 8 + 53kg (Women) 9 + 58kg (Women) 7 + 63kg (Women) 6 + 69kg (Women) 5 + 75kg (Women) 4 + +75kg (Women) 3 = 42.",
    "id": "90c8410b-eb2e-4a49-9181-e364b585b6be",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Sum the gold medals for all male and female categories. Males: 105. Females: 48kg (Women) 8 + 53kg (Women) 9 + 58kg (Women) 7 + 63kg (Women) 6 + 69kg (Women) 5 + 75kg (Women) 4 + +75kg (Women) 3 = 42. Males have won more gold medals.\\nAnswer: Men\" }, { \"user\": \"What is the average number of gold medals won by weight categories weighing above 75kg (including +75kg) for women?\", \"assistant\": \"The categories above 75kg for women with their gold medal counts are 75kg (Women) 4 and +75kg (Women) 3. Average = (4 + 3) / 2 = 3.5.\\nAnswer: 3.5\" }, { \"user\": \"Which category has more gold medals, 55kg or 81kg?\", \"assistant\": \"By comparing the heights of the bars, we see that the 55kg category has 12 gold medals while the 81kg category has 11 gold medals. Thus, the 55kg category has more gold medals.\\nAnswer: 55kg\" }, { \"user\": \"How many categories have won fewer than 10 gold medals?\", \"assistant\": \"Count the bars representing fewer than 10 medals: 89kg, 96kg, 102kg, 109kg, 48kg (Women), 58kg (Women), 63kg (Women), 69kg (Women), 75kg (Women), and +75kg (Women). There are 10 such categories.\\nAnswer: 10\" }, { \"user\": \"Describe the prominence of your weight category (55kg) visually.\", \"assistant\": \"The bar representing the 55kg category is colored crimson, which stands out compared to other bars colored goldenrod.",
    "id": "3f31e442-83fc-4cac-a52c-901b156d10d2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "There are 10 such categories.\\nAnswer: 10\" }, { \"user\": \"Describe the prominence of your weight category (55kg) visually.\", \"assistant\": \"The bar representing the 55kg category is colored crimson, which stands out compared to other bars colored goldenrod. This coloring emphasizes its significance.\\nAnswer: Crimson-colored, quite tall\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 3 ] 3 [ 4, 4, 5, 4, 2, 4, 4, 5, 4 ] 2 [ 5, 5, 5, 4, 5, 4, 5, 5, 4 ] 4 [ 4, 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What is the title of the figure?\", \"assistant\": \"The title is found at the top of the figure in a bold font. It typically provides a summary or indication of what the figure represents. In this case, the title is \\\"Age Demographics and Income Distribution by District.\\\"\\nAnswer: Age Demographics and Income Distribution by District\" }, { \"user\": \"Which district has the highest number of people in the 30-44 age group earning 40k-60k?\", \"assistant\": \"To answer this, locate the 30-44 age group on the y-axis, then find the 40k-60k income group on the x-axis. Compare the contour levels (colors) across different district subplots to determine which one has the highest value. The Central district shows the highest density in this group.\\nAnswer: Central\" }, { \"user\": \"What age group has the lowest number of people in the South district?\", \"assistant\": \"For the South district, compare the contour levels across different age groups.",
    "id": "bc70fca4-6eab-42dc-a2af-0624f26cfaa2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Compare the contour levels (colors) across different district subplots to determine which one has the highest value. The Central district shows the highest density in this group.\\nAnswer: Central\" }, { \"user\": \"What age group has the lowest number of people in the South district?\", \"assistant\": \"For the South district, compare the contour levels across different age groups. The contours for the age group 60+ show the least dense regions.\\nAnswer: 60+\" }, { \"user\": \"Which income group in the East district has more people in the 15-29 age range: <20k or 20k-40k?\", \"assistant\": \"Look at the sub-plot for the East district, locate the 15-29 age group on the y-axis and compare the contour levels for <20k and 20k-40k income groups on the x-axis. The <20k income group has a denser contour level than the 20k-40k income group.\\nAnswer: <20k\" }, { \"user\": \"How does the number of people in the 0-14 age group earning <20k compare between the North and West districts?\", \"assistant\": \"Compare the contour levels for the 0-14 age group and <20k income group in the North and West district subplots. The North district has a denser contour level than the West district in this category.\\nAnswer: North\" }, { \"user\": \"In which district is the distribution of people across all age groups and income levels the most uniform?\", \"assistant\": \"Examine the contour plots for each district to see which one displays the most even spread of contour levels across different age groups and income levels. The West district shows a relatively uniform distribution compared to others.\\nAnswer: West\" }, { \"user\": \"What is the income group with the highest number of people aged 60+ in the Central district?",
    "id": "0886e6ef-75ab-4497-a112-b24e1c37fa87",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Examine the contour plots for each district to see which one displays the most even spread of contour levels across different age groups and income levels. The West district shows a relatively uniform distribution compared to others.\\nAnswer: West\" }, { \"user\": \"What is the income group with the highest number of people aged 60+ in the Central district?\", \"assistant\": \"In the Central district subplot, locate the 60+ age group on the y-axis and identify which income group on the x-axis has the highest contour level. The 40k-60k group has the highest contour density for the 60+ age group.\\nAnswer: 40k-60k\" }, { \"user\": \"Which district has the fewest number of people in the 15-29 age group earning 40k-60k?\", \"assistant\": \"Compare the 15-29 age groups earning 40k-60k across all districts by analyzing the contour levels. The East district shows the lowest density in this group.\\nAnswer: East\" }, { \"user\": \"What age group is most populous in the North district?\", \"assistant\": \"Locate the North district subplot and compare the contour levels across all age groups. The 15-29 age group shows the highest density contours.\\nAnswer: 15-29\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ 5, 5, 4, 4, 4, 5, 5, 5, 4 ] 4 [ 4, 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"Which composer has the longer average First Movement length?",
    "id": "ada0cf89-57fd-47ad-b43e-1c83e6166918",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The average length of the First Movement for Mozart is 13.5 minutes, while for Beethoven, it is 18.3 minutes. By comparing these two values, Beethoven's average First Movement length is longer.\\nAnswer: Beethoven\" }, { \"user\": \"How long is Mozart's Piano Concerto No. 27 in comparison to Beethoven's Piano Concerto No. 5?\", \"assistant\": \"Mozart's Piano Concerto No. 27 has a length of 11.9 minutes, while Beethoven's Piano Concerto No. 5 has a length of 20.1 minutes. Comparing the two, Beethoven's Piano Concerto No. 5 is longer.\\nAnswer: Beethoven's Piano Concerto No. 5\" }, { \"user\": \"What is the average length difference between the Third Movements of Mozart and Beethoven?\", \"assistant\": \"The Third Movement for Mozart is 10.2 minutes, and for Beethoven, it is 14.8 minutes. The difference is calculated as 14.8 - 10.2 = 4.6 minutes.\\nAnswer: 4.6 minutes\" }, { \"user\": \"Which movement, on average, is the shortest for both composers?\", \"assistant\": \"Based on the average lengths provided, Mozart's Second Movement is 8.7 minutes, and Beethoven's Second Movement is 11.5 minutes. Therefore, Mozart's Second Movement is the shortest for both composers.\\nAnswer: Second Movement\" }, { \"user\": \"Which of Beethoven's selected Piano Concertos is the longest?\", \"assistant\": \"Among Beethoven's selected Piano Concertos, No. 3 is 16.5 minutes, No. 4 is 17.8 minutes, and No. 5 is 20.1 minutes. The longest is Piano Concerto No. 5 at 20.1 minutes.\\nAnswer: Piano Concerto No.",
    "id": "5e1db29b-63da-4fef-9504-00bf18b5f062",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Among Beethoven's selected Piano Concertos, No. 3 is 16.5 minutes, No. 4 is 17.8 minutes, and No. 5 is 20.1 minutes. The longest is Piano Concerto No. 5 at 20.1 minutes.\\nAnswer: Piano Concerto No. 5\" }, { \"user\": \"What is the combined length of Mozart's First and Third Movements?\", \"assistant\": \"The First Movement length for Mozart is 13.5 minutes, and the Third Movement length is 10.2 minutes. Summing these gives 13.5 + 10.2 = 23.7 minutes.\\nAnswer: 23.7 minutes\" }, { \"user\": \"How much longer is the average length of Beethoven's First Movement compared to Mozart's?\", \"assistant\": \"Beethoven's First Movement average length is 18.3 minutes, and Mozart's is 13.5 minutes. The difference is 18.3 - 13.5 = 4.8 minutes.\\nAnswer: 4.8 minutes\" }, { \"user\": \"Which selected Mozart Piano Concerto has the shortest length?\", \"assistant\": \"Among Mozart's selected Piano Concertos, No. 20 is 14.2 minutes, No. 23 is 12.8 minutes, and No. 27 is 11.9 minutes. The shortest is Piano Concerto No. 27 at 11.9 minutes.\\nAnswer: Piano Concerto No. 27\" }, { \"user\": \"Between the average Second Movements, which composer has the shorter length and by how much?\", \"assistant\": \"The Second Movement average length for Mozart is 8.7 minutes, while Beethoven's is 11.5 minutes.",
    "id": "bc27469c-a1dc-4d37-8c9a-1b558168abd3",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "27 is 11.9 minutes. The shortest is Piano Concerto No. 27 at 11.9 minutes.\\nAnswer: Piano Concerto No. 27\" }, { \"user\": \"Between the average Second Movements, which composer has the shorter length and by how much?\", \"assistant\": \"The Second Movement average length for Mozart is 8.7 minutes, while Beethoven's is 11.5 minutes. Mozart's is shorter, and the difference is 11.5 - 8.7 = 2.8 minutes.\\nAnswer: Mozart, by 2.8 minutes\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 4, 1, 2, 4, 2, 2, 2, 2, 4 ] 1 [ 5, 4, 5, 5, 5, 4, 5, 4, 4 ] 4 [ 5, 3, 5, 4, 5, 5, 5, 5, 5 ] 3 [ { \"user\": \"What is the title of the figure?\", \"assistant\": \"The title of the figure is located at the top center of the plot. It is in a larger and bold font compared to other text on the figure.\\nAnswer: Digital Device Ownership Across Age Groups\" }, { \"user\": \"How many different age groups are represented in the plot?\", \"assistant\": \"Count the distinct age groups listed along the x-axis of each subplot.\\nAnswer: 6\" }, { \"user\": \"Which age group has the highest percentage of smartphone ownership?\", \"assistant\": \"Identify the age group with the highest data point in the smartphone subplot.",
    "id": "4966be91-4f70-4553-91f4-a7ba830ed627",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Count the distinct age groups listed along the x-axis of each subplot.\\nAnswer: 6\" }, { \"user\": \"Which age group has the highest percentage of smartphone ownership?\", \"assistant\": \"Identify the age group with the highest data point in the smartphone subplot. This can be seen from the trend of the plot line.\\nAnswer: 25-34\" }, { \"user\": \"Which device has the smallest percentage of ownership in the 65+ age group?\", \"assistant\": \"Examine the data points in the subplot corresponding to the 65+ age group in each of the subplots and find the smallest percentage value.\\nAnswer: Smartphones\" }, { \"user\": \"Are there any age groups where the percentage of e-reader ownership is greater than laptop ownership?\", \"assistant\": \"Compare each age group's e-reader and laptop ownership percentages to check if the e-reader percentage exceeds the laptop percentage.\\nAnswer: Yes, 65+ age group\" }, { \"user\": \"Which device shows the least variation in ownership percentages across different age groups?\", \"assistant\": \"Examine the range of percentages for each device across all age groups and identify the one with the smallest range.\\nAnswer: E-readers\" } ] CoSyn-400k-chart [ 5, 5, 5, 3, 5, 5 ] 3 [ 5, 4, 2, 4, 4, 4 ] 2 [ 5, 5, 5, 5, 4, 5 ] 4 [ 5, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What's the title of the figure?\", \"assistant\": \"The title is displayed at the top of the figure. It reads \\\"Distribution of Relationship Duration Before Marriage Across Generations\\\".\\nAnswer: Distribution of Relationship Duration Before Marriage Across Generations\" }, { \"user\": \"Which generation has the highest density peak for relationship duration before marriage?",
    "id": "ef0cd315-b87c-4e51-82f3-ba9c88fc84f7",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title is displayed at the top of the figure. It reads \\\"Distribution of Relationship Duration Before Marriage Across Generations\\\".\\nAnswer: Distribution of Relationship Duration Before Marriage Across Generations\" }, { \"user\": \"Which generation has the highest density peak for relationship duration before marriage?\", \"assistant\": \"By examining the plot, we can see the highest peak in density for each generation. The Millennials have the highest peak at around 0.21 density at approximately 20 years duration.\\nAnswer: Millennials\" }, { \"user\": \"How does the peak density of Generation Z compare to the peak densitiy of Baby Boomers?\", \"assistant\": \"Generation Z has a peak density of around 0.20, while the Baby Boomers' peak density is around 0.22. To compare, subtract the two densities: 0.22 - 0.20 = 0.02. Therefore, Baby Boomers have a peak density 0.02 higher than Generation Z.\\nAnswer: Baby Boomers have a higher peak density by 0.02\" }, { \"user\": \"What is the approximate relationship duration before marriage for the Silent Generation's density peak?\", \"assistant\": \"Locate the highest point on the Silent Generation curve. The peak of the Silent Generation's density is around 4 years duration.\\nAnswer: 4 years\" }, { \"user\": \"Which generation shows the broadest distribution of relationship duration before marriage?\", \"assistant\": \"The broad distribution can be identified by observing the spread of the densities across the duration axis. Millennials display a broader distribution as their densities spread up to around 36 years.\\nAnswer: Millennials\" }, { \"user\": \"Which generation has a density peak around 10 years of relationship duration before marriage?\", \"assistant\": \"By observing the figure, the Baby Boomers have a noticeable density peak around the 10-year mark.\\nAnswer: Baby Boomers\" }, { \"user\": \"What is the approximate density for Generation X at 15 years of relationship duration before marriage?",
    "id": "6d72a81b-cf65-40cb-8d97-8bfcc62b88e6",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"By observing the figure, the Baby Boomers have a noticeable density peak around the 10-year mark.\\nAnswer: Baby Boomers\" }, { \"user\": \"What is the approximate density for Generation X at 15 years of relationship duration before marriage?\", \"assistant\": \"Look for Generation X on the plot and find approximately where the 15 years mark is. The density at that point is about 0.18.\\nAnswer: 0.18\" }, { \"user\": \"Which generation has the least density at zero years of relationship duration before marriage?\", \"assistant\": \"Compare the densities at the zero-year mark for all generations. Generation X has the lowest density, around 0.01.\\nAnswer: Generation X\" }, { \"user\": \"How does the relationship duration distribution before marriage change over generations?\", \"assistant\": \"Starting with the Silent Generation, the peak durations gradually shift to longer durations in subsequent generations. The density peaks are highest for Millennials, indicating longer relationships before marriage becoming more common over time.\\nAnswer: Trends towards longer relationships over generations\" }, { \"user\": \"What relationship duration range accounts for the highest density among Millennials?",
    "id": "825228e2-cefa-48d9-86e8-fc0ef3157f94",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Starting with the Silent Generation, the peak durations gradually shift to longer durations in subsequent generations. The density peaks are highest for Millennials, indicating longer relationships before marriage becoming more common over time.\\nAnswer: Trends towards longer relationships over generations\" }, { \"user\": \"What relationship duration range accounts for the highest density among Millennials?\", \"assistant\": \"For Millennials, the highest density is around the 20-year mark, and the density stays relatively high from around 12 to 28 years.\\nAnswer: 12-28 years\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 3, 5 ] 3 [ 5, 2, 2, 2, 4, 4, 4, 2, 2, 2 ] 2 [ 5, 5, 4, 5, 5, 5, 4, 5, 5, 5 ] 4 [ 5, 5, 5, 4, 4, 5, 4, 4, 4, 4 ] 4 [ { \"user\": \"Which ad genre had the highest percentage in 2022?\", \"assistant\": \"The figure shows the percentage distribution of different ad genres over several years. In 2022, \\\"Drama\\\" had the highest percentage.\\nAnswer: Drama\" }, { \"user\": \"What is the overall trend for the Humor genre from 2010 to 2022?\", \"assistant\": \"The percentage of Humor ads steadily declined from 35% in 2010 to 22% in 2022. This suggests a decreasing trend over the years.\\nAnswer: Decreasing\" }, { \"user\": \"By how many percentage points did the Documentary genre percentage change from 2010 to 2022?\", \"assistant\": \"In 2010, the Documentary genre was at 15%, while in 2022, it was at 12%.",
    "id": "4e6b7536-292e-4297-8eb9-fd58f4e072ca",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "This suggests a decreasing trend over the years.\\nAnswer: Decreasing\" }, { \"user\": \"By how many percentage points did the Documentary genre percentage change from 2010 to 2022?\", \"assistant\": \"In 2010, the Documentary genre was at 15%, while in 2022, it was at 12%. The change is 15% - 12% = 3%.\\nAnswer: 3%\" }, { \"user\": \"Which genre saw the largest increase in percentage from 2010 to 2022?\", \"assistant\": \"By examining the differences between percentages in 2010 and 2022 for each genre, Drama increased from 20% to 33%, which is the largest increase of 13%.\\nAnswer: Drama\" }, { \"user\": \"Which genre consistently had the smallest percentage from 2010 to 2022?\", \"assistant\": \"The Romance genre consistently had the smallest percentage, spanning from 4% to 5% throughout the years from 2010 to 2022.\\nAnswer: Romance\" }, { \"user\": \"Between which years did the Action genre see its highest increase in percentage?\", \"assistant\": \"From examining the trend line, Action saw its highest increase from 2010 (15%) to 2016 (18%), making a 3% increase.\\nAnswer: 2010 to 2016\" }, { \"user\": \"What was the percentage difference between Humor and Drama genres in 2020?\", \"assistant\": \"In 2020, Humor was at 24% and Drama was at 32%. Their percentage difference is 32% - 24% = 8%.\\nAnswer: 8%\" }, { \"user\": \"Which genre showed the most stability over the years?\", \"assistant\": \"The Romance genre showed the most stability, maintaining a percentage between 4% and 5% consistently from 2010 to 2022.\\nAnswer: Romance\" }, { \"user\": \"At which year did Drama surpass Humor in percentage?",
    "id": "50b24dcb-c655-495a-8703-75ee636f510b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Their percentage difference is 32% - 24% = 8%.\\nAnswer: 8%\" }, { \"user\": \"Which genre showed the most stability over the years?\", \"assistant\": \"The Romance genre showed the most stability, maintaining a percentage between 4% and 5% consistently from 2010 to 2022.\\nAnswer: Romance\" }, { \"user\": \"At which year did Drama surpass Humor in percentage?\", \"assistant\": \"Drama surpassed Humor in 2018, with Drama at 30% and Humor at 26%.\\nAnswer: 2018\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 2, 5, 5, 5, 2, 4, 2, 4 ] 2 [ 5, 5, 5, 5, 5, 4, 5, 4, 5 ] 4 [ 4, 4, 5, 5, 4, 4, 4, 5, 4 ] 4 [ { \"user\": \"What are the three main industries shown in the Venn Diagram?\", \"assistant\": \"The three main industries in the diagram are indicated by the largest text, which appear inside the three largest and differently colored circles. These industries are Agriculture, Oil, and Manufacturing.\\nAnswer: Agriculture, Oil, Manufacturing\" }, { \"user\": \"Which sectors are common between Agriculture and Oil?\", \"assistant\": \"To find sectors that are common between Agriculture and Oil, look for elements that fall in the overlapping region of the circles labeled Agriculture (red) and Oil (green). The sectors present in this overlap are Farm Equipment, Grain Storage, and Biofuels.\\nAnswer: Farm Equipment, Grain Storage, Biofuels\" }, { \"user\": \"Are there any sectors shared between all three industries?",
    "id": "c174199f-2fb8-40d8-920f-d36b5c23acf8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"To find sectors that are common between Agriculture and Oil, look for elements that fall in the overlapping region of the circles labeled Agriculture (red) and Oil (green). The sectors present in this overlap are Farm Equipment, Grain Storage, and Biofuels.\\nAnswer: Farm Equipment, Grain Storage, Biofuels\" }, { \"user\": \"Are there any sectors shared between all three industries?\", \"assistant\": \"Sectors shared by all three industries will appear in the central overlap where all three circles intersect. Look here for the relevant labels. The sectors found in this intersection are Transportation and Labor Force.\\nAnswer: Transportation, Labor Force\" }, { \"user\": \"What sector is shared between Oil and Manufacturing but not Agriculture?\", \"assistant\": \"Look for the overlapping region between the Oil (green) and Manufacturing (blue) circles, excluding the region that also overlaps with Agriculture (red). The sectors in this region are Machinery and Metal Fabrication.\\nAnswer: Machinery, Metal Fabrication\" }, { \"user\": \"List the unique sectors under Manufacturing.\", \"assistant\": \"To identify sectors unique to Manufacturing, find those that are within the blue circle but not in any overlapping region with either Agriculture (red) or Oil (green). These sectors include Furniture and Textiles.\\nAnswer: Furniture, Textiles\" }, { \"user\": \"Do any sectors intersect between Agriculture and Manufacturing but not with Oil?\", \"assistant\": \"Check the overlapping region between the Agriculture (red) and Manufacturing (blue) circles, while excluding any sectors that also overlap with Oil (green). The only sector in this region is Food Processing and Canning.\\nAnswer: Food Processing, Canning\" }, { \"user\": \"Which circles overlap directly in the Venn Diagram, creating secondary intersections?\", \"assistant\": \"Secondary intersections are regions where only two circles overlap.",
    "id": "56b3b3a5-a7b2-4300-b08f-d0a969334d84",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Check the overlapping region between the Agriculture (red) and Manufacturing (blue) circles, while excluding any sectors that also overlap with Oil (green). The only sector in this region is Food Processing and Canning.\\nAnswer: Food Processing, Canning\" }, { \"user\": \"Which circles overlap directly in the Venn Diagram, creating secondary intersections?\", \"assistant\": \"Secondary intersections are regions where only two circles overlap. Agriculture (red) overlaps with Oil (green), Agriculture (red) with Manufacturing (blue), and Oil (green) with Manufacturing (blue).\\nAnswer: Agriculture & Oil, Agriculture & Manufacturing, Oil & Manufacturing\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 4, 5, 5 ] 4 [ 5, 2, 4, 4, 2, 4, 4 ] 2 [ 5, 4, 4, 4, 4, 4, 5 ] 4 [ 4, 5, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What is the overall trend in retention rates as tenure increases?\", \"assistant\": \"The outer pie chart shows retention rates for different tenure groups. By observing the chart, retention rates generally increase as tenure moves from 0-1 years to 10+ years. The percentages are 65%, 75%, 82%, 88%, and 92% respectively.\\nAnswer: Retention rates increase with tenure\" }, { \"user\": \"Which tenure group has the highest retention rate?\", \"assistant\": \"Observing the outer pie chart, the 10+ years tenure group has the highest retention rate of 92%.\\nAnswer: 10+ years\" }, { \"user\": \"In the 0-1 years tenure group, what is the most common reason for retention?",
    "id": "73adee5a-8cda-427d-b4ff-24a162fd2350",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Observing the outer pie chart, the 10+ years tenure group has the highest retention rate of 92%.\\nAnswer: 10+ years\" }, { \"user\": \"In the 0-1 years tenure group, what is the most common reason for retention?\", \"assistant\": \"In the inner pie chart corresponding to the 0-1 years tenure group, the largest segment is labeled \\\"Compensation,\\\" representing 30%.\\nAnswer: Compensation\" }, { \"user\": \"How does the importance of \\\"Job Satisfaction\\\" change as tenure increases from 0-1 years to 10+ years?\", \"assistant\": \"By examining the inner segments for each tenure group, \\\"Job Satisfaction\\\" starts at 15% for 0-1 years, 12% for 1-3 years, 15% for 3-5 years, 35% for 5-10 years, and 40% for 10+ years, showing a clear increase with rising tenure.\\nAnswer: It increases\" }, { \"user\": \"Compare the importance of \\\"Career Growth\\\" between the 1-3 years and the 5-10 years tenure groups.\", \"assistant\": \"Looking at the inner pie charts for these tenure ranges, \\\"Career Growth\\\" is 25% for 1-3 years and 25% for 5-10 years, indicating equal importance in these two groups.\\nAnswer: Equal\" }, { \"user\": \"Calculate the combined percentage for \\\"Compensation\\\" and \\\"Career Growth\\\" for the 3-5 years tenure group.\", \"assistant\": \"From the inner pie chart segments for 3-5 years, \\\"Compensation\\\" is 28% and \\\"Career Growth\\\" is 30%. Summing these values, 28% + 30% = 58%.\\nAnswer: 58%\" }, { \"user\": \"Which reason has the smallest percentage in the 10+ years tenure group?",
    "id": "e4030c5f-2c04-44be-aac6-a72f09831724",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"From the inner pie chart segments for 3-5 years, \\\"Compensation\\\" is 28% and \\\"Career Growth\\\" is 30%. Summing these values, 28% + 30% = 58%.\\nAnswer: 58%\" }, { \"user\": \"Which reason has the smallest percentage in the 10+ years tenure group?\", \"assistant\": \"In the inner pie segment for 10+ years, the smallest segment is \\\"Career Growth\\\" with 8%.\\nAnswer: Career Growth\" }, { \"user\": \"What is the difference in the percentage for \\\"Work-Life Balance\\\" between the 0-1 years and 1-3 years tenure groups?\", \"assistant\": \"\\\"Work-Life Balance\\\" is 25% for 0-1 years and 20% for 1-3 years. The difference is 25% - 20% = 5%.\\nAnswer: 5%\" }, { \"user\": \"Which factor is consistently one of the top two reasons for retention across all tenure groups?\", \"assistant\": \"By inspecting all inner pie chart segments, \\\"Compensation\\\" frequently appears as one of the top two reasons for retention across all tenure groups.\\nAnswer: Compensation\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 2, 3, 2, 2, 4, 4, 4, 4 ] 2 [ 4, 5, 5, 5, 4, 4, 5, 5, 5 ] 4 [ 4, 5, 4, 4, 4, 4, 5, 5, 4 ] 4 [ { \"user\": \"What's the title of the figure?\", \"assistant\": \"The title is usually displayed prominently at the top of the figure.",
    "id": "587fd44b-295d-4618-abdb-36a1735725c1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title is usually displayed prominently at the top of the figure. In this case, we can see that the title is centered and reads \\\"Growth of Home Renovation Investments by Type of Exterior Upgrade\\\".\\nAnswer: Growth of Home Renovation Investments by Type of Exterior Upgrade\" }, { \"user\": \"What are the different types of exterior upgrades shown in the figure?\", \"assistant\": \"The types of exterior upgrades are listed in the legend and color-coded. They include Landscaping, Paint, Siding, Roofing, Lighting, Front Door, and Windows.\\nAnswer: Landscaping, Paint, Siding, Roofing, Lighting, Front Door, Windows\" }, { \"user\": \"Which upgrade type had the highest investment in 2023?\", \"assistant\": \"To determine this, we look at the area chart and identify the category with the highest value on the Y-axis for the year 2023.\\nAnswer: Roofing\" }, { \"user\": \"How much did the investment in Landscaping increase from 2018 to 2023?\", \"assistant\": \"We locate the values for Landscaping in 2018 and 2023. The difference between these two values (3200 - 2000) gives the increase.\\nAnswer: 1200\" }, { \"user\": \"Which two types of exterior upgrades saw the least investment in 2020?\", \"assistant\": \"We compare the values for each category in 2020. The two lowest values are for Lighting and Front Door.\\nAnswer: Lighting and Front Door\" }, { \"user\": \"What is the total investment in all exterior upgrades for the year 2022?\", \"assistant\": \"We sum up the investments for all categories in 2022. (3000 + 2100 + 2500 + 3400 + 1500 + 2000 + 2000)\\nAnswer: 16500\" }, { \"user\": \"Between which two consecutive years did the investment in Windows see the greatest increase?",
    "id": "6bc32173-5ebd-4bfe-9cc1-1e8db4fbcb93",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"We sum up the investments for all categories in 2022. (3000 + 2100 + 2500 + 3400 + 1500 + 2000 + 2000)\\nAnswer: 16500\" }, { \"user\": \"Between which two consecutive years did the investment in Windows see the greatest increase?\", \"assistant\": \"We calculate the yearly increases for Windows and identify the largest one: 2018-2019 (1600 - 1400), 2019-2020 (1750 - 1600), 2020-2021 (1850 - 1750), 2021-2022 (2000 - 1850), 2022-2023 (2200 - 2000). The largest increase is 200.\\nAnswer: 2021-2022\" }, { \"user\": \"Which upgrade type shows the most consistent yearly growth over the displayed period?\", \"assistant\": \"To determine consistency, we visually inspect each category's line and note that a linear, steady increase is most consistent. The category that fits this description best is Landscaping.\\nAnswer: Landscaping\" }, { \"user\": \"Did investment in any upgrade type decrease in any year?\", \"assistant\": \"We can look at each category's trend line. If a line dips between two years, it indicates a decrease. It's clear from the chart that no category experienced a decrease in investment from year to year.\\nAnswer: No\" }, { \"user\": \"How did the investment trend for Paint change over time?\", \"assistant\": \"We examine the Paint category's trend line from 2018 to 2023.",
    "id": "33920725-f4a0-481c-8050-f39fe01127e3",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"We can look at each category's trend line. If a line dips between two years, it indicates a decrease. It's clear from the chart that no category experienced a decrease in investment from year to year.\\nAnswer: No\" }, { \"user\": \"How did the investment trend for Paint change over time?\", \"assistant\": \"We examine the Paint category's trend line from 2018 to 2023. The investment in Paint steadily increased over time without any drops.\\nAnswer: Steady increase\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 5, 4, 5, 4, 3, 4, 4, 2, 3 ] 2 [ 5, 5, 5, 5, 5, 5, 5, 5, 3, 4 ] 3 [ 4, 4, 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What year did the number of cruisers reach its peak?\", \"assistant\": \"The number of cruisers reached its highest value in 1890 and 1910, at 5 and 4, respectively.\\nAnswer: 1890\" }, { \"user\": \"Which vessel type had the most consistent increase from 1822 to 2023?\", \"assistant\": \"To determine the most consistent increase, observe which vessel type shows a steady growth over time without significant drops. Patrol Boats show a steady increase from 4 in 1822 to 44 in 2023, without major fluctuations.\\nAnswer: Patrol Boats\" }, { \"user\": \"What is the sum of Frigates and Corvettes in 1990?",
    "id": "4666271a-768c-483d-8f51-9f34961e791f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"To determine the most consistent increase, observe which vessel type shows a steady growth over time without significant drops. Patrol Boats show a steady increase from 4 in 1822 to 44 in 2023, without major fluctuations.\\nAnswer: Patrol Boats\" }, { \"user\": \"What is the sum of Frigates and Corvettes in 1990?\", \"assistant\": \"In 1990, there were 10 Frigates and 8 Corvettes. Adding these together: 10 + 8 = 18.\\nAnswer: 18\" }, { \"user\": \"Between which two decades was the greatest decrease in the number of Battleships observed?\", \"assistant\": \"Battleships fell drastically from 1910 (2) to 1950 (0), showing the largest decline between any two decades in fleet history.\\nAnswer: Between 1910 and 1950\" }, { \"user\": \"Which vessel type started to emerge around the 1930s and grew significantly later?\", \"assistant\": \"Submarines emerged in 1910 with 3 and grew notably to 5 by 2023. They show increasing presence across the dataset from their introduction.\\nAnswer: Submarines\" }, { \"user\": \"How does the fleet composition in 1930 compare to that in 1950?\", \"assistant\": \"In 1930, the fleet had 2 Battleships, 2 Cruisers, 4 Destroyers, 6 Corvettes, 3 Submarines, and 30 Patrol Boats. By 1950, the Battleships were gone, and the fleet included 2 Cruisers, 8 Destroyers, 4 Frigates, 8 Corvettes, 2 Submarines, and 36 Patrol Boats.",
    "id": "96ec844a-cd35-4331-93e0-e4b9f2a9484e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"In 1930, the fleet had 2 Battleships, 2 Cruisers, 4 Destroyers, 6 Corvettes, 3 Submarines, and 30 Patrol Boats. By 1950, the Battleships were gone, and the fleet included 2 Cruisers, 8 Destroyers, 4 Frigates, 8 Corvettes, 2 Submarines, and 36 Patrol Boats. Notably, Patrol Boats increased from 30 to 36, and Frigates were introduced.\\nAnswer: 1930 fleet had fewer Frigates and Patrol Boats than in 1950\" }, { \"user\": \"Which vessel type declined the most by absolute number from its peak to 2023?\", \"assistant\": \"Battleships reached their peak at 8 in 1890, but decreased to 0 by 2023, showing the largest decline by absolute number (-8).\\nAnswer: Battleships\" }, { \"user\": \"When did Destroyers reach their maximum count?\", \"assistant\": \"The graph shows Destroyers peaked at 12 in 1970, which is their highest count throughout the period.\\nAnswer: 1970\" }, { \"user\": \"What is the average number of Submarines from the year they first appeared until 2023?\", \"assistant\": \"Submarines first appeared in 1910. The counts are: 3, 3, 2, 4, 5, 5, averaging (3+3+2+4+5+5)/6 = 22/6.\\nAnswer: 3.67\" }, { \"user\": \"What is the trend for Patrol Boats from 1822 to 2023?",
    "id": "8f95e532-a5c6-49be-8573-b7b509616f40",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Submarines first appeared in 1910. The counts are: 3, 3, 2, 4, 5, 5, averaging (3+3+2+4+5+5)/6 = 22/6.\\nAnswer: 3.67\" }, { \"user\": \"What is the trend for Patrol Boats from 1822 to 2023?\", \"assistant\": \"Patrol Boats have shown a consistent increase from 4 in 1822 to 44 in 2023, implying a growing trend with no major reductions over the years.\\nAnswer: Increasing trend\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 2, 3, 5, 5, 1, 5 ] 1 [ 2, 4, 4, 4, 2, 3, 4, 4, 2, 2 ] 2 [ 4, 5, 4, 4, 4, 4, 5, 4, 4, 5 ] 4 [ 4, 5, 5, 5, 4, 4, 5, 4, 4, 5 ] 4 [ { \"user\": \"What age group has the highest percentage of people with Medicaid coverage?\", \"assistant\": \"Check the length of the bars for Medicaid coverage. The 25-34 age group has the longest bar in this category.\\nAnswer: 25-34\" }, { \"user\": \"Which age group has the lowest percentage of people with Employer-Sponsored Insurance?\", \"assistant\": \"Compare the lengths of the Employer-Sponsored Insurance bars for all age groups. The 65+ age group's bar is the shortest.\\nAnswer: 65+\" }, { \"user\": \"What is the total percentage of uninsured individuals across all age groups?",
    "id": "57b4e6b8-d40c-472f-aded-2085ee87f5b7",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Compare the lengths of the Employer-Sponsored Insurance bars for all age groups. The 65+ age group's bar is the shortest.\\nAnswer: 65+\" }, { \"user\": \"What is the total percentage of uninsured individuals across all age groups?\", \"assistant\": \"Sum the percentages of uninsured individuals from all age groups (10 + 9 + 8 + 7 + 10 + 14).\\nAnswer: 58%\" }, { \"user\": \"Which age group has the highest percentage of uninsured individuals?\", \"assistant\": \"Look for the longest bar in the uninsured category. The 65+ age group has the longest bar.\\nAnswer: 65+\" }, { \"user\": \"Between the 18-24 and 25-34 age groups, which has a higher percentage of individuals with Marketplace Subsidized Insurance?\", \"assistant\": \"Compare the lengths of the bars for Marketplace Subsidized Insurance. The 25-34 age group has a longer bar than the 18-24 age group.\\nAnswer: 25-34\" }, { \"user\": \"How does the percentage of individuals with Medicaid coverage change from the 35-44 age group to the 55-64 age group?\", \"assistant\": \"Subtract the Medicaid percentage of the 55-64 age group (22) from that of the 35-44 age group (26). The change is 26 - 22.\\nAnswer: Decrease by 4\" }, { \"user\": \"Which age group has the highest total percentage of people with either Medicaid or Employer-Sponsored Insurance?\", \"assistant\": \"Sum the percentages for Medicaid and Employer-Sponsored Insurance in each age group and find the highest. For the 25-34 age group: Medicaid (28) + Employer-Sponsored (15) = 43%. Compare all sums.\\nAnswer: 25-34\" }, { \"user\": \"What is the average percentage of people with Marketplace Subsidized Insurance across all age groups?",
    "id": "ac5781cb-1c52-4ce3-85a0-aab74c16a1be",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Sum the percentages for Medicaid and Employer-Sponsored Insurance in each age group and find the highest. For the 25-34 age group: Medicaid (28) + Employer-Sponsored (15) = 43%. Compare all sums.\\nAnswer: 25-34\" }, { \"user\": \"What is the average percentage of people with Marketplace Subsidized Insurance across all age groups?\", \"assistant\": \"Sum the Marketplace Subsidized Insurance percentages (20 + 22 + 23 + 24 + 21 + 18) then divide by the number of age groups (6). Average = (128) / 6 = 21.33\\nAnswer: 21.33%\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 2, 2, 2, 4, 4, 4, 2 ] 2 [ 5, 5, 3, 5, 5, 4, 5, 4 ] 3 [ 4, 5, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What year did the Hispanic income first reach $40,000?\", \"assistant\": \"Look at the data for Hispanic income over the years and identify the first year where the income is at or above $40,000. From the figure, it‚Äôs clear that the Hispanic income reaches $40,000 in 2016.\\nAnswer: 2016\" }, { \"user\": \"Which group had the highest increase in income from 2011 to 2022?\", \"assistant\": \"Look at the income values for each group in 2011 and 2022, then calculate the increase (2022 income - 2011 income) for each group.",
    "id": "93e6bbf6-3446-49e8-aed5-1a33eff51ee5",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "From the figure, it‚Äôs clear that the Hispanic income reaches $40,000 in 2016.\\nAnswer: 2016\" }, { \"user\": \"Which group had the highest increase in income from 2011 to 2022?\", \"assistant\": \"Look at the income values for each group in 2011 and 2022, then calculate the increase (2022 income - 2011 income) for each group. The Asian group had the highest increase: $61,000 - $50,000 = $11,000.\\nAnswer: Asian\" }, { \"user\": \"By how much did the income of the Native American group increase from 2014 to 2020?\", \"assistant\": \"Look at the income for the Native American group in both 2014 and 2020, then subtract the 2014 value from the 2020 value. The income increased from $30,000 to $33,000, an increase of $3,000.\\nAnswer: $3,000\" }, { \"user\": \"Which group had the smallest income in 2015, and what was it?\", \"assistant\": \"Look at the data for 2015 across all groups and identify the smallest income value. The Native American group had the smallest income of $30,000 in 2015.\\nAnswer: Native American, $30,000\" }, { \"user\": \"What is the average income for the Black group between 2018 and 2022?\", \"assistant\": \"Calculate the average by summing the incomes for the Black group from 2018 to 2022 and then divide by the number of years (5). Sum: $34,000 + $34,500 + $35,000 + $35,500 + $36,000 = $174,000. Average is $174,000 / 5 = $34,800.\\nAnswer: $34,800\" }, { \"user\": \"Compare the income increase of the Pacific Islander group and the White group between 2011 and 2021.",
    "id": "4eb44883-e510-4092-a551-f998cfe54b0e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Sum: $34,000 + $34,500 + $35,000 + $35,500 + $36,000 = $174,000. Average is $174,000 / 5 = $34,800.\\nAnswer: $34,800\" }, { \"user\": \"Compare the income increase of the Pacific Islander group and the White group between 2011 and 2021. Which group had a higher rise and by how much?\", \"assistant\": \"Calculate the income increase from 2011 to 2021 for both groups and compare. For Pacific Islanders: $42,000 - $32,000 = $10,000. For Whites: $65,000 - $55,000 = $10,000. Both groups had the same increase.\\nAnswer: Same, $10,000\" }, { \"user\": \"What is the difference in income between the White and Black groups in 2011, and how does it compare to the difference in 2022?\", \"assistant\": \"Look at the income for both groups in 2011 and 2022 and calculate the differences. In 2011, the difference was $55,000 - $30,000 = $25,000. In 2022, the difference was $66,000 - $36,000 = $30,000. The difference increased by $5,000 from 2011 to 2022.\\nAnswer: $25,000 (2011), $30,000 (2022), Increased by $5,000\" }, { \"user\": \"What trend can be observed for the income of the Hispanic group over the decade?\", \"assistant\": \"Observe the pattern/title and note the general trend.",
    "id": "ee68cd67-35db-4728-95d3-5796ea2982c4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The difference increased by $5,000 from 2011 to 2022.\\nAnswer: $25,000 (2011), $30,000 (2022), Increased by $5,000\" }, { \"user\": \"What trend can be observed for the income of the Hispanic group over the decade?\", \"assistant\": \"Observe the pattern/title and note the general trend. Starting from $35,000 in 2011 and increasing steadily to $46,000 in 2022, the trend shows consistent growth every year.\\nAnswer: Steady increase\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 4, 3, 2, 2, 4, 2, 4 ] 2 [ 4, 5, 4, 5, 4, 5, 5, 5 ] 4 [ 4, 4, 5, 5, 5, 4, 4, 4 ] 4 [ { \"user\": \"What condition had the highest prevalence in 2020?\", \"assistant\": \"By looking at the lines in the figure, we can observe which one reaches the highest value on the y-axis in the year 2020. The orange line representing Osteoarthritis is the highest in 2020.\\nAnswer: Osteoarthritis\" }, { \"user\": \"Which condition had a lower prevalence in 1980, Alzheimer's Disease or Parkinson's Disease?\", \"assistant\": \"By checking the values at the year 1980 in the figure, we see that Alzheimer's Disease is at 2.8% and Parkinson's Disease is at 1.2%. Therefore, Parkinson's Disease had a lower prevalence.\\nAnswer: Parkinson's Disease\" }, { \"user\": \"Between which years did Macular Degeneration show the greatest increase in prevalence?",
    "id": "0138dffb-186b-4157-86fe-a14c660d333f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"By checking the values at the year 1980 in the figure, we see that Alzheimer's Disease is at 2.8% and Parkinson's Disease is at 1.2%. Therefore, Parkinson's Disease had a lower prevalence.\\nAnswer: Parkinson's Disease\" }, { \"user\": \"Between which years did Macular Degeneration show the greatest increase in prevalence?\", \"assistant\": \"To answer this, we need to compare the differences in prevalence of Macular Degeneration between consecutive years and identify the largest increment. The largest increase is between 2015 and 2020 (2.9% to 3.1%).\\nAnswer: 2015-2020\" }, { \"user\": \"How much did the prevalence of Alzheimer's Disease increase from 1980 to 2020?\", \"assistant\": \"Subtract the value of Alzheimer's Disease in 1980 from its value in 2020. (5.9% - 2.8%)\\nAnswer: 3.1%\" }, { \"user\": \"What was the combined prevalence of Osteoarthritis and Huntington's Disease in 2000?\", \"assistant\": \"Add the prevalence values of Osteoarthritis and Huntington's Disease for the year 2000. (9.9% + 0.09%)\\nAnswer: 9.99%\" }, { \"user\": \"Which disorder has consistently shown the least prevalence over the years?\", \"assistant\": \"By observing the figure, Huntington's Disease has the lowest prevalence values across all years.\\nAnswer: Huntington's Disease\" }, { \"user\": \"Between 1980 and 2020, which condition's prevalence increased the least?\", \"assistant\": \"Calculate the increase in prevalence for each condition and compare them.",
    "id": "3d656253-1458-47d3-9e51-f745de066500",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"By observing the figure, Huntington's Disease has the lowest prevalence values across all years.\\nAnswer: Huntington's Disease\" }, { \"user\": \"Between 1980 and 2020, which condition's prevalence increased the least?\", \"assistant\": \"Calculate the increase in prevalence for each condition and compare them. Huntington's Disease increased from 0.07% to 0.11%, which is the smallest increase of 0.04%.\\nAnswer: Huntington's Disease\" }, { \"user\": \"Order the conditions by their prevalence in the year 1995 from highest to lowest.\", \"assistant\": \"Checking the values for each condition in 1995: Osteoarthritis (9.4%), Alzheimer's Disease (3.9%), Macular Degeneration (2.1%), Parkinson's Disease (1.5%), Huntington's Disease (0.08%).\\nAnswer: Osteoarthritis, Alzheimer's Disease, Macular Degeneration, Parkinson's Disease, Huntington's Disease\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 2, 4, 2, 2, 2, 2, 4 ] 2 [ 5, 5, 4, 5, 5, 5, 5, 5 ] 4 [ 5, 4, 5, 4, 4, 4, 4, 5 ] 4 [ { \"user\": \"What is the title of the figure?\", \"assistant\": \"The title is presented at the top of the figure in a larger and bolder font size. It provides a summary of what the figure is about.\\nAnswer: Digital Marketing Channel Performance by Business Sector\" }, { \"user\": \"Which marketing channel has the highest CTR in the Healthcare sector?",
    "id": "4b279686-c910-4dc9-8350-f2648db14744",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title is presented at the top of the figure in a larger and bolder font size. It provides a summary of what the figure is about.\\nAnswer: Digital Marketing Channel Performance by Business Sector\" }, { \"user\": \"Which marketing channel has the highest CTR in the Healthcare sector?\", \"assistant\": \"Look at the plot for CTR by Channel and find the highest point for the Healthcare sector.\\nAnswer: Email Marketing\" }, { \"user\": \"How does the CPC of Display Ads compare between the Finance and E-commerce sectors?\", \"assistant\": \"Observe the CPC by Channel plot and compare the values of Display Ads for both sectors. The marker positions indicate the CPC values.\\nAnswer: Higher in Finance\" }, { \"user\": \"Which sector has the highest ROI for Search Ads?\", \"assistant\": \"Check the ROI by Channel plot and identify the sector with the highest point for Search Ads.\\nAnswer: Technology\" }, { \"user\": \"What is the average Conversion Rate for Social Media across all sectors?\", \"assistant\": \"Locate the Conversion Rate by Channel plot, record the Conversion Rate values for Social Media in all sectors, sum them up and divide by the number of sectors.\\nAnswer: (2.1+1.7+2.0+2.7)/4 = 2.125\" }, { \"user\": \"Which sector has the most consistent performance across different metrics for Email Marketing?\", \"assistant\": \"Investigate all subplots (CTR, CPC, Conversion Rate, and ROI) and observe which sector shows the least variation in its values for Email Marketing across these plots.\\nAnswer: E-commerce\" }, { \"user\": \"In which sector is the difference between the highest and lowest ROI values the greatest?\", \"assistant\": \"For each sector, calculate the difference between the highest and lowest ROI values across all channels. Compare these differences across sectors.\\nAnswer: Technology\" }, { \"user\": \"How does the Conversion Rate of Email Marketing in the Technology sector compare to that in the Finance sector?",
    "id": "a67a0469-3c11-4d9d-9981-7f9ed96157c9",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"For each sector, calculate the difference between the highest and lowest ROI values across all channels. Compare these differences across sectors.\\nAnswer: Technology\" }, { \"user\": \"How does the Conversion Rate of Email Marketing in the Technology sector compare to that in the Finance sector?\", \"assistant\": \"Refer to the Conversion Rate by Channel plot and compare the values of Email Marketing for Technology and Finance sectors.\\nAnswer: Higher in Technology\" }, { \"user\": \"Which marketing channel has the lowest CPC in all sectors?\", \"assistant\": \"Spot the lowest points in the CPC by Channel plot for each sector and identify the channel.\\nAnswer: Email Marketing\" }, { \"user\": \"How many sectors are included in the figure?\", \"assistant\": \"Count the number of unique sectors shown in the legends of each subplot.\\nAnswer: Four\" } ] CoSyn-400k-chart [ 5, 5, 4, 5, 5, 5, 4, 5, 5, 5 ] 4 [ 5, 2, 2, 2, 2, 2, 4, 2, 2, 5 ] 2 [ 5, 5, 4, 5, 4, 5, 5, 4, 5, 5 ] 4 [ 5, 4, 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What is the title of the figure?\", \"assistant\": \"The title is usually prominently displayed at the top of the figure. In this case, it states the main topic of the plot.\\nAnswer: Pet Food Sales by Category, Subcategory, and Brand\" }, { \"user\": \"Which category has the highest total sales?",
    "id": "3ccb8a08-3cfc-4639-aa36-e6fe0bbcde38",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title is usually prominently displayed at the top of the figure. In this case, it states the main topic of the plot.\\nAnswer: Pet Food Sales by Category, Subcategory, and Brand\" }, { \"user\": \"Which category has the highest total sales?\", \"assistant\": \"By looking at the various segments of the sunburst plot, we can identify which central section is the largest, indicating the highest total sales.\\nAnswer: Dog Food\" }, { \"user\": \"How many subcategories are there within the \\\"Specialty\\\" category?\", \"assistant\": \"Counting the slices that fall directly under the \\\"Specialty\\\" category will give the number of subcategories.\\nAnswer: 3\" }, { \"user\": \"Which brand has the highest sales in the \\\"Cat Food\\\" category?\", \"assistant\": \"Within the \\\"Cat Food\\\" category's segment, we check the size of the individual brand segments to determine the largest one.\\nAnswer: Iams\" }, { \"user\": \"What is the combined sales figure for \\\"Dry Food\\\" in the \\\"Dog Food\\\" category?\", \"assistant\": \"Add the sales figures for Purina and Hill's Science Diet under \\\"Dry Food\\\" in the \\\"Dog Food\\\" category. 1200 + 950 = 2150\\nAnswer: 2150\" }, { \"user\": \"Which subcategory in \\\"Treats\\\" has the lowest sales?\", \"assistant\": \"Within the \\\"Treats\\\" category, compare the sales in each subcategory and identify the smallest one.\\nAnswer: Training Treats\" }, { \"user\": \"Are there more sales in \\\"Grain-Free\\\" or \\\"Weight Control\\\" under \\\"Specialty\\\"?\", \"assistant\": \"Compare the sales figures for \\\"Grain-Free\\\" and \\\"Weight Control\\\" in the \\\"Specialty\\\" category. Grain-Free: 550, Weight Control: 450.\\nAnswer: Grain-Free\" }, { \"user\": \"How many brands are listed under \\\"Fish Food\\\"?",
    "id": "436d33c9-f2ca-411d-96e2-486fc53ee329",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Compare the sales figures for \\\"Grain-Free\\\" and \\\"Weight Control\\\" in the \\\"Specialty\\\" category. Grain-Free: 550, Weight Control: 450.\\nAnswer: Grain-Free\" }, { \"user\": \"How many brands are listed under \\\"Fish Food\\\"?\", \"assistant\": \"Count the number of segments that fall under \\\"Fish Food\\\".\\nAnswer: 2\" }, { \"user\": \"Which category or subcategory does the \\\"Blue Buffalo\\\" brand belong to?\", \"assistant\": \"By following the path segments in the sunburst, we can trace Blue Buffalo to see its category and subcategory.\\nAnswer: Dog Food, Wet Food\" }, { \"user\": \"What is the total sales value for all \\\"Cat Food\\\" subcategories combined?\", \"assistant\": \"Sum up the sales for all subcategories under \\\"Cat Food\\\". 950 (Dry Food) + 850 (Wet Food) + 500 (Kitten Food) = 2300\\nAnswer: 2300\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 4, 5, 5, 5, 5 ] 4 [ 5, 4, 4, 2, 4, 4, 4, 4, 2, 4 ] 2 [ 5, 5, 3, 5, 4, 4, 4, 4, 4, 4 ] 3 [ 4, 4, 4, 4, 5, 4, 4, 4, 4, 5 ] 4 [ { \"user\": \"How many activities are represented in the plot?\", \"assistant\": \"Count the number of unique activities depicted as markers on the ternary plot.\\nAnswer: 15\" }, { \"user\": \"What is the title of the plot?",
    "id": "22d07826-579a-4f23-bb37-af251b596b3c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Count the number of unique activities depicted as markers on the ternary plot.\\nAnswer: 15\" }, { \"user\": \"What is the title of the plot?\", \"assistant\": \"The title is given at the top of the plot as specified in the layout.\\nAnswer: Distribution of Extracurricular Activities at Kintore Primary School\" }, { \"user\": \"Which activity is most heavily skewed towards \\\"Sports\\\"?\", \"assistant\": \"Look for the marker that is closest to the \\\"Sports\\\" vertex of the ternary plot.\\nAnswer: Athletics\" }, { \"user\": \"How many activities have their highest composition in \\\"Academics\\\"?\", \"assistant\": \"Identify the markers where the ‚ÄúAcademics‚Äù value is the highest among the three categories (Sports, Arts, Academics).\\nAnswer: 6\" }, { \"user\": \"Which activity has balanced contributions among Sports, Arts, and Academics?\", \"assistant\": \"Identify the marker that lies closest to the center of the ternary plot, indicating relatively equal values for all three categories.\\nAnswer: Nature Explorers\" }, { \"user\": \"Are there more activities skewed heavily towards \\\"Arts\\\" or \\\"Academics\\\"?\", \"assistant\": \"Compare the number of markers closest to the \\\"Arts\\\" vertex with those closest to the \\\"Academics\\\" vertex.\\nAnswer: Arts\" }, { \"user\": \"Which activity has the highest \\\"Arts\\\" value?\", \"assistant\": \"Look for the marker closest to the \\\"Arts\\\" vertex and check its label.\\nAnswer: Music Ensemble\" }, { \"user\": \"Which two activities have the same ratio of Sports, Arts, and Academics?\", \"assistant\": \"Check if any two markers align perfectly on the same radial line from the center, having the same ratio of the three categories.\\nAnswer: Debate Team and Science Fair\" }, { \"user\": \"What's the combined percentage of \\\"Sports\\\" in Football Club and Swimming Team?",
    "id": "9a2aad66-3ceb-4990-9d5b-c1c05517d1e1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Check if any two markers align perfectly on the same radial line from the center, having the same ratio of the three categories.\\nAnswer: Debate Team and Science Fair\" }, { \"user\": \"What's the combined percentage of \\\"Sports\\\" in Football Club and Swimming Team?\", \"assistant\": \"Add the \\\"Sports\\\" percentage values of Football Club and Swimming Team.\\nAnswer: 130\" }, { \"user\": \"Which has a higher proportion of Sports, Math Club or Chess Club?\", \"assistant\": \"Compare the \\\"Sports\\\" percentages for Math Club and Chess Club.\\nAnswer: Math Club\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 4, 4 ] 4 [ 4, 5, 4, 4, 2, 2, 1, 4, 4, 3 ] 1 [ 5, 5, 5, 4, 4, 4, 5, 3, 4, 4 ] 3 [ 4, 4, 4, 4, 4, 4, 5, 4, 4, 4 ] 4 [ { \"user\": \"What is the trend in the number of Joint Meetings over the months?\", \"assistant\": \"The number of Joint Meetings starts at 4 in January, increases to 6 in March, sees minor fluctuation until it hits 9 in August, and peaks at 11 in December.\\nAnswer: Increasing trend\" }, { \"user\": \"Which month had the highest Communication Score?\", \"assistant\": \"The highest Communication Score is in December, where it reaches 9.7.\\nAnswer: December\" }, { \"user\": \"By how much did the number of Shared Documents increase from January to December?\", \"assistant\": \"The number of Shared Documents in January is 12 and in December it reaches 40.",
    "id": "753006e7-def9-4c62-9a48-1d6288b0175a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The highest Communication Score is in December, where it reaches 9.7.\\nAnswer: December\" }, { \"user\": \"By how much did the number of Shared Documents increase from January to December?\", \"assistant\": \"The number of Shared Documents in January is 12 and in December it reaches 40. The increase is 40 - 12 = 28.\\nAnswer: 28\" }, { \"user\": \"What is the difference in Revision Cycles between the months with the maximum and minimum scores?\", \"assistant\": \"The highest number of Revision Cycles is in January at 3.5 and the lowest is in December at 1.6. The difference is 3.5 - 1.6 = 1.9.\\nAnswer: 1.9\" }, { \"user\": \"Which metric had the most significant increase over the year?\", \"assistant\": \"Comparing the initial and final values for all metrics: Joint Meetings (4 to 11), Shared Documents (12 to 40), Revision Cycles (3.5 to 1.6), Completed Projects (2 to 8), Communication Score (7.2 to 9.7), Shared Documents increased the most significantly from 12 to 40, an increase of 28.\\nAnswer: Shared Documents\" }, { \"user\": \"When were the number of Completed Projects equal to the number of Joint Meetings?\", \"assistant\": \"Both the Completed Projects and Joint Meetings are equal to 5 in June and July.\\nAnswer: June, July\" }, { \"user\": \"What is the average Communication Score for the entire year?\", \"assistant\": \"The Communication Scores are: 7.2, 7.5, 7.8, 8.0, 8.3, 8.6, 8.8, 9.0, 9.2, 9.4, 9.5, 9.7.",
    "id": "2d455477-d4c5-4349-98ba-60681ab1a8d2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The Communication Scores are: 7.2, 7.5, 7.8, 8.0, 8.3, 8.6, 8.8, 9.0, 9.2, 9.4, 9.5, 9.7. The sum is 101 and there are 12 months, so the average is 101 / 12 = 8.42.\\nAnswer: 8.42\" }, { \"user\": \"In which months did Revision Cycles drop below 3?\", \"assistant\": \"Revision Cycles drop below 3 starting from April to December: April (2.8), May (2.5), June (2.3), July (2.2), August (2.0), September (1.9), October (1.8), November (1.7), December (1.6).\\nAnswer: April to December\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 4, 5, 4, 5, 2, 4, 5, 4 ] 2 [ 4, 5, 5, 5, 5, 4, 5, 4 ] 4 [ 4, 5, 4, 5, 4, 4, 5, 4 ] 4 [ { \"user\": \"What's the title of the plot?\", \"assistant\": \"The title of the plot is displayed prominently at the top of the figure. It reads 'Magnetic Field Intensity Around Rectangular Loop Antenna'.\\nAnswer: Magnetic Field Intensity Around Rectangular Loop Antenna\" }, { \"user\": \"What are the units of the colorbar?",
    "id": "aa6f9169-a4e2-40fb-aee7-f0c5609bf0cf",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title of the plot is displayed prominently at the top of the figure. It reads 'Magnetic Field Intensity Around Rectangular Loop Antenna'.\\nAnswer: Magnetic Field Intensity Around Rectangular Loop Antenna\" }, { \"user\": \"What are the units of the colorbar?\", \"assistant\": \"The colorbar, which indicates the magnetic field intensity, has the title showing its units in A/m (Amperes per meter).\\nAnswer: A/m\" }, { \"user\": \"Which frequency range has the highest magnetic field intensity?\", \"assistant\": \"By comparing the contour levels among the three subplots, the highest magnetic field intensity can be seen in the 100 MHz plot, reaching up to 3.0 A/m.\\nAnswer: 100 MHz\" }, { \"user\": \"What's the range of magnetic field intensity values at 50 MHz?\", \"assistant\": \"The contour lines for 50 MHz show that magnetic field intensity ranges from 1.2 to 1.8 A/m as indicated by the minimum and maximum values in the colorbar.\\nAnswer: 1.2 to 1.8 A/m\" }, { \"user\": \"How does the field intensity at (10 cm, 20 cm) compare between 10 MHz and 50 MHz?\", \"assistant\": \"At 10 MHz, the magnetic field intensity at (10 cm, 20 cm) is 0.75 A/m. At 50 MHz, it is 1.7 A/m. 1.7 A/m is greater than 0.75 A/m, meaning it‚Äôs much higher at 50 MHz.\\nAnswer: Higher at 50 MHz\" }, { \"user\": \"What are the labels used for the x-axis and y-axis?",
    "id": "db7c3822-e203-4b8e-b7f9-d9e697684656",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"At 10 MHz, the magnetic field intensity at (10 cm, 20 cm) is 0.75 A/m. At 50 MHz, it is 1.7 A/m. 1.7 A/m is greater than 0.75 A/m, meaning it‚Äôs much higher at 50 MHz.\\nAnswer: Higher at 50 MHz\" }, { \"user\": \"What are the labels used for the x-axis and y-axis?\", \"assistant\": \"The x-axis label is 'X Position (cm)', and the y-axis label is 'Y Position (cm)' as shown at the bottom and side of the figure.\\nAnswer: X Position (cm) and Y Position (cm)\" }, { \"user\": \"What is the magnetic field intensity at the origin (0 cm, 0 cm) in the 100 MHz plot?\", \"assistant\": \"By looking at the contour plot for 100 MHz and checking the contour label at the (0 cm, 0 cm) point, the magnetic field intensity is 2.1 A/m.\\nAnswer: 2.1 A/m\" }, { \"user\": \"Which subplot shows the most variation in magnetic field intensity?\", \"assistant\": \"By comparing the range and difference in contour lines, the 100 MHz subplot shows the most variation, ranging from 2.1 to 3.0 A/m.\\nAnswer: 100 MHz\" }, { \"user\": \"Is there a point where the field intensity is the same across all frequency ranges?",
    "id": "864e902f-b872-44cc-a203-51f62596c117",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"By comparing the range and difference in contour lines, the 100 MHz subplot shows the most variation, ranging from 2.1 to 3.0 A/m.\\nAnswer: 100 MHz\" }, { \"user\": \"Is there a point where the field intensity is the same across all frequency ranges?\", \"assistant\": \"Observing the contour labels, there is no single coordinate point where the magnetic field intensity is the same for 10 MHz, 50 MHz, and 100 MHz plots simultaneously.\\nAnswer: No\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 4, 2, 2, 4, 4, 4, 4, 2 ] 2 [ 5, 4, 5, 4, 5, 5, 4, 5, 4 ] 4 [ 4, 4, 4, 5, 5, 4, 5, 4, 4 ] 4 [ { \"user\": \"Which month had the highest number of HVAC maintenance requests?\", \"assistant\": \"The bar representing July is the tallest in the chart, indicating it had the highest number of requests.\\nAnswer: July\" }, { \"user\": \"Which month had fewer HVAC maintenance requests, February or March?\", \"assistant\": \"By comparing the heights of the bars for February and March, it is clear that the February bar is shorter than the March bar.\\nAnswer: February\" }, { \"user\": \"What is the total number of HVAC maintenance requests in the summer months (June, July, and August)?\", \"assistant\": \"Sum the values for June, July, and August: 60 + 73 + 68 = 201.\\nAnswer: 201\" }, { \"user\": \"How many more maintenance requests were there in July compared to January?",
    "id": "f9c604c5-9773-40d1-918d-cd765f72ebd1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Sum the values for June, July, and August: 60 + 73 + 68 = 201.\\nAnswer: 201\" }, { \"user\": \"How many more maintenance requests were there in July compared to January?\", \"assistant\": \"Subtract the number of requests in January from the number in July: 73 - 38 = 35.\\nAnswer: 35\" }, { \"user\": \"Did April have more or fewer requests compared to the average monthly requests over the year?\", \"assistant\": \"First, calculate the total number of requests for the year: 38 + 30 + 35 + 40 + 50 + 60 + 73 + 68 + 55 + 45 + 42 + 39 = 575. The average is 575 / 12 ‚âà 47.92. April had 40 requests, which is fewer than the average.\\nAnswer: Fewer\" }, { \"user\": \"Which month had the second-highest number of HVAC maintenance requests?\", \"assistant\": \"The bar for August is the second-tallest after July, so August had the second-highest number of requests.\\nAnswer: August\" }, { \"user\": \"Compare the HVAC maintenance requests in Q1 (January to March) and Q4 (October to December). Which quarter had more requests?\", \"assistant\": \"Sum the requests in Q1: 38 + 30 + 35 = 103. Sum the requests in Q4: 45 + 42 + 39 = 126. Q4 had more requests.\\nAnswer: Q4\" }, { \"user\": \"Is there a noticeable trend in the number of HVAC maintenance requests as the year progresses?\", \"assistant\": \"Observing the chart, the number of requests increases from January to July, peaks in July, and then decreases towards December.\\nAnswer: Yes, increasing then decreasing\" }, { \"user\": \"How much higher was the number of requests in May compared to February?",
    "id": "987543b4-ddb6-4963-93a0-39bc1ee55503",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Q4 had more requests.\\nAnswer: Q4\" }, { \"user\": \"Is there a noticeable trend in the number of HVAC maintenance requests as the year progresses?\", \"assistant\": \"Observing the chart, the number of requests increases from January to July, peaks in July, and then decreases towards December.\\nAnswer: Yes, increasing then decreasing\" }, { \"user\": \"How much higher was the number of requests in May compared to February?\", \"assistant\": \"Subtract the number of requests in February from May: 50 - 30 = 20.\\nAnswer: 20\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 4, 4, 4, 4, 2, 4, 4, 4, 4 ] 2 [ 5, 4, 4, 5, 4, 5, 4, 5, 4 ] 4 [ 4, 4, 4, 4, 5, 4, 5, 4, 4 ] 4 [ { \"user\": \"How many version control systems are in this comparison?\", \"assistant\": \"The diagram shows three labeled circles, each representing one version control system: Git, SVN, and Mercurial.\\nAnswer: 3\" }, { \"user\": \"Which version control systems are described as \\\"Easy to learn\\\"?\", \"assistant\": \"The circles representing \\\"Easy to learn\\\" intersect with both Git and SVN.\\nAnswer: Git, SVN\" }, { \"user\": \"Which two version control systems are both \\\"Distributed\\\"?\", \"assistant\": \"The \\\"Distributed\\\" property intersects with the Git and Mercurial circles.\\nAnswer: Git, Mercurial\" }, { \"user\": \"Which version control system is noted as being \\\"Used in many enterprises\\\"?",
    "id": "3213eb5c-1b43-4457-8969-b228721fa504",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The circles representing \\\"Easy to learn\\\" intersect with both Git and SVN.\\nAnswer: Git, SVN\" }, { \"user\": \"Which two version control systems are both \\\"Distributed\\\"?\", \"assistant\": \"The \\\"Distributed\\\" property intersects with the Git and Mercurial circles.\\nAnswer: Git, Mercurial\" }, { \"user\": \"Which version control system is noted as being \\\"Used in many enterprises\\\"?\", \"assistant\": \"\\\"Used in many enterprises\\\" intersects with both Git and SVN.\\nAnswer: Git, SVN\" }, { \"user\": \"What features are unique to Git and Mercurial jointly?\", \"assistant\": \"The intersection of the Git and Mercurial circles without overlapping with the SVN circle lists: \\\"Fast,\\\" \\\"Popular in open source,\\\" \\\"Branching and merging,\\\" and \\\"Distributed.\\\"\\nAnswer: Fast, Popular in open source, Branching and merging, Distributed\" }, { \"user\": \"Is SVN marked as \\\"Popular in open source\\\"?\", \"assistant\": \"The \\\"Popular in open source\\\" feature does not intersect with the SVN circle.\\nAnswer: No\" }, { \"user\": \"List all features shared by Git, SVN, and Mercurial.\", \"assistant\": \"The center intersection where Git, SVN, and Mercurial overlap does not contain any feature labels.\\nAnswer: None\" }, { \"user\": \"Which system is easy to learn but not marked as being \\\"Used in many enterprises\\\"?",
    "id": "2598ec50-39d7-4980-9413-2340831f87f0",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The \\\"Popular in open source\\\" feature does not intersect with the SVN circle.\\nAnswer: No\" }, { \"user\": \"List all features shared by Git, SVN, and Mercurial.\", \"assistant\": \"The center intersection where Git, SVN, and Mercurial overlap does not contain any feature labels.\\nAnswer: None\" }, { \"user\": \"Which system is easy to learn but not marked as being \\\"Used in many enterprises\\\"?\", \"assistant\": \"The feature \\\"Easy to learn\\\" intersects with both Git and SVN individually in separate intersections; however, when considering SVN and \\\"Used in many enterprises,\\\" \\\"Easy to learn\\\" stands alone for just Git.\\nAnswer: None\" } ] CoSyn-400k-chart [ 5, 5, 5, 4, 5, 5, 5, 1 ] 1 [ 5, 2, 4, 4, 3, 4, 2, 4 ] 2 [ 5, 5, 3, 4, 4, 3, 5, 4 ] 3 [ 5, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What is the title of the figure?\", \"assistant\": \"The title is typically located at the top of the figure. In this case, it reads \\\"Organ Donation and Transplant Statistics by Country\\\".\\nAnswer: Organ Donation and Transplant Statistics by Country\" }, { \"user\": \"How many countries are represented in this scatterplot matrix?\", \"assistant\": \"The figure uses different colors to differentiate between the countries. Each unique color represents a country. By counting these colors, we find there are 10 countries represented.\\nAnswer: 10\" }, { \"user\": \"Which country has the highest donation rate?\", \"assistant\": \"To determine the country with the highest donation rate, identify the point that is furthest to the right on the 'Donation Rate' axis.",
    "id": "1450dbcb-481d-4fd9-99bc-9627ff438ee5",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The figure uses different colors to differentiate between the countries. Each unique color represents a country. By counting these colors, we find there are 10 countries represented.\\nAnswer: 10\" }, { \"user\": \"Which country has the highest donation rate?\", \"assistant\": \"To determine the country with the highest donation rate, identify the point that is furthest to the right on the 'Donation Rate' axis. This point corresponds to Spain with a donation rate of 49.6.\\nAnswer: Spain\" }, { \"user\": \"Is there a positive correlation between donation rate and transplant success rate?\", \"assistant\": \"Looking at the scatterplot that intersects the 'Donation Rate' and 'Transplant Success Rate' dimensions, the points do not show a clear positive or negative trend. Thus, there does not appear to be a strong correlation.\\nAnswer: No\" }, { \"user\": \"Which country has the lowest public awareness percentage?\", \"assistant\": \"Find the point that is lowest on the 'Public Awareness (%)' axis. This point corresponds to Japan with a public awareness percentage of 45%.\\nAnswer: Japan\" }, { \"user\": \"What is the median waiting time across the countries?\", \"assistant\": \"To find the median waiting time, list all waiting times: 4.5, 6.8, 7.3, 8.1, 9.6, 10.5, 11.2, 12.8, 15.3, 36.7 and find the middle value. The middle value in this ordered list (since the number of values is 10, the median is the average of the 5th and 6th values): (9.6 + 10.5)/2 = 10.05.\\nAnswer: 10.05 months\" }, { \"user\": \"Which two countries have the smallest difference in their transplant success rates?\", \"assistant\": \"Compare the transplant success rates to find the smallest difference.",
    "id": "f081ce80-839a-427f-94b3-b736df2e0453",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The middle value in this ordered list (since the number of values is 10, the median is the average of the 5th and 6th values): (9.6 + 10.5)/2 = 10.05.\\nAnswer: 10.05 months\" }, { \"user\": \"Which two countries have the smallest difference in their transplant success rates?\", \"assistant\": \"Compare the transplant success rates to find the smallest difference. The closest rates are 85.2 (Spain) and 85.7 (Germany). The difference is 85.7 - 85.2 = 0.5%.\\nAnswer: Spain and Germany\" }, { \"user\": \"Which country has the highest waiting time, and what is that value?\", \"assistant\": \"The highest point on the 'Waiting Time (Months)' axis corresponds to Japan, with a value of 36.7 months.\\nAnswer: Japan, 36.7 months\" }, { \"user\": \"Which pairs of countries have almost identical donation rates?\", \"assistant\": \"Identify points that are close to each other on the 'Donation Rate' axis. Croatia (34.8) and Portugal (33.6), France (33.3) and Belgium (32.9) are close, but Belgium and France are the most similar.\\nAnswer: Croatia and Portugal; France and Belgium\" }, { \"user\": \"Is there a trend between donation rate and public awareness percentage across countries?",
    "id": "0113df46-dd01-427c-b7cc-49c2fc2dacca",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Identify points that are close to each other on the 'Donation Rate' axis. Croatia (34.8) and Portugal (33.6), France (33.3) and Belgium (32.9) are close, but Belgium and France are the most similar.\\nAnswer: Croatia and Portugal; France and Belgium\" }, { \"user\": \"Is there a trend between donation rate and public awareness percentage across countries?\", \"assistant\": \"By observing the scatterplot comparing 'Donation Rate' and 'Public Awareness (%)', there appears to be a trend where higher donation rates correspond to higher public awareness percentages, suggesting a positive correlation.\\nAnswer: Yes\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 5, 2, 2, 2, 4, 2, 2, 4, 2 ] 2 [ 5, 1, 1, 1, 1, 1, 1, 1, 1, 1 ] 1 [ 4, 4, 4, 5, 5, 4, 5, 5, 4, 4 ] 4 [ { \"user\": \"What is the most common source of inspiration for classical art-based films?\", \"assistant\": \"By observing the pie chart, Mythology has the largest sector, which means it is the most common source. The other sources, Historical Events and Religious Texts, share a slightly smaller portion, and Literature has the smallest.\\nAnswer: Mythology\" }, { \"user\": \"Which two sources share the same percentage of inspiration?\", \"assistant\": \"The pie chart shows that both Historical Events and Religious Texts each make up 25% of the inspiration sources.\\nAnswer: Historical Events and Religious Texts\" }, { \"user\": \"How much larger is the percentage of Mythology than Literature?",
    "id": "6ffa1ba5-efdc-420d-aeaf-c6eff7695959",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The pie chart shows that both Historical Events and Religious Texts each make up 25% of the inspiration sources.\\nAnswer: Historical Events and Religious Texts\" }, { \"user\": \"How much larger is the percentage of Mythology than Literature?\", \"assistant\": \"The pie chart indicates that Mythology makes up 30% of the sources, while Literature makes up 20%. The difference between these percentages is calculated as 30% - 20% = 10%.\\nAnswer: 10%\" }, { \"user\": \"What is the combined percentage of Historical Events and Religious Texts?\", \"assistant\": \"By adding the percentages shown for Historical Events (25%) and Religious Texts (25%), the combined total is 25% + 25% = 50%.\\nAnswer: 50%\" }, { \"user\": \"Which source has a golden color?\", \"assistant\": \"Based on the color descriptions provided, Literature is represented in a golden color.\\nAnswer: Literature\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5 ] 5 [ 2, 4, 4, 1, 4 ] 1 [ 5, 4, 4, 4, 3 ] 3 [ 4, 4, 5, 5, 4 ] 4 [ { \"user\": \"What is the trend observed for Vogue from 2010 to 2022?\", \"assistant\": \"The influence of Vogue shows a continuous decline from 35 in 2010 to 10 in 2022. This can be observed by noting the sequential decrease at each time point.\\nAnswer: Decline\" }, { \"user\": \"Compare the influence scores of Instagram and Pinterest in 2022 and identify which one is higher.\", \"assistant\": \"In 2022, Instagram has an influence score of 35, while Pinterest has 23.",
    "id": "ef82a2a2-ee9d-44b5-8949-2826a54d881b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The influence of Vogue shows a continuous decline from 35 in 2010 to 10 in 2022. This can be observed by noting the sequential decrease at each time point.\\nAnswer: Decline\" }, { \"user\": \"Compare the influence scores of Instagram and Pinterest in 2022 and identify which one is higher.\", \"assistant\": \"In 2022, Instagram has an influence score of 35, while Pinterest has 23. By comparing these values, we can see that Instagram has a higher score.\\nAnswer: Instagram\" }, { \"user\": \"What is the combined influence score of The Sartorialist and Man Repeller in 2014?\", \"assistant\": \"The influence score of The Sartorialist in 2014 is 15, and for Man Repeller, it's 18. Adding these together gives 15 + 18 = 33.\\nAnswer: 33\" }, { \"user\": \"Which data source showed the most significant increase in influence from 2010 to 2022?\", \"assistant\": \"Comparing the change in influence scores from 2010 to 2022 for each source, Instagram shows the highest increase (from 3 to 35, a 32-point increase).\\nAnswer: Instagram\" }, { \"user\": \"Compare Vogue's influence in 2012 and 2020. By how many points did it decline?\", \"assistant\": \"Vogue's influence in 2012 is 33, and it drops to 15 in 2020. The difference is 33 - 15 = 18.\\nAnswer: 18\" }, { \"user\": \"Identify the data source with the smallest influence score in 2012.\", \"assistant\": \"In 2012, the influence scores are: Vogue (33), Elle (23), FashionTV (18), E! Network (14), The Sartorialist (10), Man Repeller (12), Instagram (7), Pinterest (5).",
    "id": "afb0092f-f2e5-4ac9-89e4-a8cabfbb0f6c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The difference is 33 - 15 = 18.\\nAnswer: 18\" }, { \"user\": \"Identify the data source with the smallest influence score in 2012.\", \"assistant\": \"In 2012, the influence scores are: Vogue (33), Elle (23), FashionTV (18), E! Network (14), The Sartorialist (10), Man Repeller (12), Instagram (7), Pinterest (5). The smallest score is for Pinterest (5).\\nAnswer: Pinterest\" }, { \"user\": \"What is the average influence score of FashionTV over the years 2010 to 2022?\", \"assistant\": \"Summing the scores for FashionTV across the years gives 20 + 18 + 15 + 12 + 10 + 8 + 5 = 88. Dividing this sum by the number of years (7) gives an average of 88 / 7 ‚âà 12.57.\\nAnswer: 12.57\" }, { \"user\": \"In 2016, which had a higher influence score: Elle or The Sartorialist?\", \"assistant\": \"In 2016, Elle had an influence score of 18, and The Sartorialist had a score of 20. The Sartorialist's score is higher.\\nAnswer: The Sartorialist\" }, { \"user\": \"Between 2018 and 2020, did Pinterest's influence increase or decrease, and by how much?\", \"assistant\": \"Pinterest's influence was 20 in 2018 and rose to 22 in 2020. The increase is 22 - 20 = 2.\\nAnswer: Increased by 2\" }, { \"user\": \"Which data sources had declining trends throughout all measured years?\", \"assistant\": \"Vogue, Elle, FashionTV, and E! Network show continuous declines in their influence scores throughout all measured years (2010-2022).\\nAnswer: Vogue, Elle, FashionTV, E!",
    "id": "45287ce0-d29a-466d-b789-ec1778ef9209",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The increase is 22 - 20 = 2.\\nAnswer: Increased by 2\" }, { \"user\": \"Which data sources had declining trends throughout all measured years?\", \"assistant\": \"Vogue, Elle, FashionTV, and E! Network show continuous declines in their influence scores throughout all measured years (2010-2022).\\nAnswer: Vogue, Elle, FashionTV, E! Network\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 4, 2, 2, 4, 2, 2, 2, 2, 2, 4 ] 2 [ 5, 5, 4, 5, 5, 4, 4, 5, 5, 5 ] 4 [ 4, 5, 5, 5, 5, 5, 5, 4, 5, 4 ] 4 [ { \"user\": \"What city had the highest urban green space coverage in 1950?\", \"assistant\": \"From the line chart, we see that Sydney had the highest green space coverage in 1950 with over 33.1%.\\nAnswer: Sydney\" }, { \"user\": \"Which two cities showed an increase in urban green space coverage from 2000 to 2020?\", \"assistant\": \"By observing the lines for each city, we can see that New York City and Paris experienced an increase in their green space coverage from 2000 to 2020.\\nAnswer: New York City and Paris\" }, { \"user\": \"What was the change in urban green space coverage for Tokyo between 1950 and 1990?\", \"assistant\": \"Tokyo's green space coverage decreased from 7.8% in 1950 to 5.8% in 1990.",
    "id": "393007e4-c8f1-4142-9b6a-2b2bf3fba657",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Tokyo's green space coverage decreased from 7.8% in 1950 to 5.8% in 1990. The change is 7.8 - 5.8 = 2%.\\nAnswer: 2%\" }, { \"user\": \"Which city experienced the most consistent decrease in urban green space coverage from 1950 to 1990?\", \"assistant\": \"By examining the slopes of the lines, London shows a consistent decrease from 31.5% in 1950 to 29.3% in 1990.\\nAnswer: London\" }, { \"user\": \"What was the average urban green space coverage for Paris from 1950 to 2020?\", \"assistant\": \"By summing Paris's data points (9.4 + 9.1 + 8.9 + 8.7 + 8.6 + 8.9 + 9.3 + 9.7) and dividing by the number of points (8), we get (72.6 / 8) = 9.1%.\\nAnswer: 9.1%\" }, { \"user\": \"In which decade did New York City see the lowest urban green space coverage?\", \"assistant\": \"By following New York City's line, the lowest green space coverage occurred in 1990 with a value of 17.2%.\\nAnswer: 1990\" }, { \"user\": \"How does the green space coverage of Sydney in 1980 compare to that of Tokyo in 2020?\", \"assistant\": \"Sydney had around 32.3% green space coverage in 1980, while Tokyo had 7.1% in 2020. Sydney's coverage was significantly higher.\\nAnswer: Sydney had significantly higher green space coverage\" }, { \"user\": \"Which city's green space coverage returned to near its original 1950 level by 2020?",
    "id": "96143343-3ef2-4add-aa6e-7e8bfb463ecd",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Sydney had around 32.3% green space coverage in 1980, while Tokyo had 7.1% in 2020. Sydney's coverage was significantly higher.\\nAnswer: Sydney had significantly higher green space coverage\" }, { \"user\": \"Which city's green space coverage returned to near its original 1950 level by 2020?\", \"assistant\": \"Observing all lines, New York City's green space coverage in 2020 (19.1%) is very close to its 1950 figure (19.2%).\\nAnswer: New York City\" }, { \"user\": \"What was the average increase in urban green space coverage for Sydney from 2000 to 2020?\", \"assistant\": \"Sydney's green space coverage increased from 32.4% in 2000 to 33.0% in 2020. The increase is 33.0 - 32.4 = 0.6%.\\nAnswer: 0.6%\" }, { \"user\": \"Which cities had green space coverage values greater than 30% in 2020?",
    "id": "63110b45-f66c-4796-82cc-5f195a449cba",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Sydney's green space coverage increased from 32.4% in 2000 to 33.0% in 2020. The increase is 33.0 - 32.4 = 0.6%.\\nAnswer: 0.6%\" }, { \"user\": \"Which cities had green space coverage values greater than 30% in 2020?\", \"assistant\": \"Observing the endpoints of the lines in 2020, London and Sydney had green space coverages larger than 30%.\\nAnswer: London and Sydney\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 4, 3, 4, 3, 4, 3, 3, 2, 4 ] 2 [ 5, 5, 4, 5, 4, 4, 5, 5, 4, 5 ] 4 [ 5, 4, 5, 4, 5, 4, 4, 4, 4, 5 ] 4 [ { \"user\": \"What is the total number of listeners in Canada for Beverly Glenn-Copeland's music?\", \"assistant\": \"Sum of Canadian listeners: 5000 (18-24) + 7500 (25-34) + 6000 (35-44) + 4500 (45-54) + 3500 (55+) = 26500\\nAnswer: 26500\" }, { \"user\": \"Which age group in the USA has the highest number of listeners?\", \"assistant\": \"The USA age group with the highest number of listeners is 25-34 with 12000 listeners.\\nAnswer: 25-34\" }, { \"user\": \"Which country has the fewest listeners for the 55+ age group?",
    "id": "542fbb76-c9f1-4e70-8195-fa1804a961c8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The USA age group with the highest number of listeners is 25-34 with 12000 listeners.\\nAnswer: 25-34\" }, { \"user\": \"Which country has the fewest listeners for the 55+ age group?\", \"assistant\": \"Japan has the fewest listeners in the 55+ age group with 1000 listeners.\\nAnswer: Japan\" }, { \"user\": \"How many more listeners are there in the UK's largest age group compared to its smallest age group?\", \"assistant\": \"The UK's largest age group is 25-34 with 4500 listeners, and the smallest is 55+ with 2000 listeners. Difference: 4500 - 2000 = 2500\\nAnswer: 2500\" }, { \"user\": \"What is the proportion of listeners in the 18-24 age group in Germany compared to the total number of German listeners?\", \"assistant\": \"Total listeners in Germany: 2000 (18-24) + 3000 (25-34) + 2500 (35-44) + 1800 (45-54) + 1500 (55+) = 10800. Proportion: 2000 / 10800  100% = 18.52%\\nAnswer: 18.52%\" }, { \"user\": \"In which age group does Japan have the second lowest number of listeners?\", \"assistant\": \"The second lowest number of listeners in Japan is in the 45-54 age group with 1200 listeners.\\nAnswer: 45-54\" }, { \"user\": \"Compare the total number of listeners in Canada and the USA. Which country has more listeners?\", \"assistant\": \"Total listeners in Canada: 26500. Total listeners in the USA: 41500. The USA has more listeners.\\nAnswer: USA\" }, { \"user\": \"Which country has the highest total number of listeners?",
    "id": "03b7219b-6ce0-491a-9ce1-bc654b90d67a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Which country has more listeners?\", \"assistant\": \"Total listeners in Canada: 26500. Total listeners in the USA: 41500. The USA has more listeners.\\nAnswer: USA\" }, { \"user\": \"Which country has the highest total number of listeners?\", \"assistant\": \"The country with the highest total number of listeners is the USA with 41500 listeners.\\nAnswer: USA\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 2, 2, 3, 4, 2, 4, 2 ] 2 [ 4, 5, 5, 4, 4, 4, 5, 5 ] 4 [ 4, 4, 5, 4, 5, 4, 5, 4 ] 4 [ { \"user\": \"What is the title of the chart?\", \"assistant\": \"The title is located at the very top of the polar area chart, clearly labeled to describe what the figure represents.\\nAnswer: Goals Scored in Different Playing Conditions by Top Algerian Players\" }, { \"user\": \"Which player scored the most goals at home?\", \"assistant\": \"To determine this, look at the \\\"Home\\\" section of the polar chart and identify the player with the highest point in that section. Baghdad Bounedjah has the largest value (15 goals).\\nAnswer: Baghdad Bounedjah\" }, { \"user\": \"How many goals did Riyad Mahrez score in neutral conditions?\", \"assistant\": \"Locate Riyad Mahrez's segment in the \\\"Neutral\\\" area on the polar chart. The value at that point represents the number of goals he scored in neutral conditions. It is 5 goals.\\nAnswer: 5 goals\" }, { \"user\": \"Which player has the smallest difference between home and away goals?",
    "id": "af9dc4ef-06c1-4e8a-91a7-4dc6f50221a9",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Locate Riyad Mahrez's segment in the \\\"Neutral\\\" area on the polar chart. The value at that point represents the number of goals he scored in neutral conditions. It is 5 goals.\\nAnswer: 5 goals\" }, { \"user\": \"Which player has the smallest difference between home and away goals?\", \"assistant\": \"Calculate the difference between home and away goals for each player by visually inspecting the chart. For Mahrez (4), Bounedjah (5), Brahimi (3), Slimani (2), Feghouli (2), both Slimani and Feghouli have the smallest difference.\\nAnswer: Islam Slimani and Sofiane Feghouli\" }, { \"user\": \"How many goals did Yacine Brahimi score in total?\", \"assistant\": \"Add Yacine Brahimi's goals scored in each playing condition: Home (10), Away (7), and Neutral (4). Summing these gives 21 goals.\\nAnswer: 21 goals\" }, { \"user\": \"Who scored more goals in away conditions, Riyad Mahrez or Islam Slimani?\", \"assistant\": \"Look at the \\\"Away\\\" segment for both Riyad Mahrez and Islam Slimani. Riyad Mahrez scored 8 goals and Islam Slimani scored 11 goals.\\nAnswer: Islam Slimani\" }, { \"user\": \"Which player has the widest spread in goals scored across different conditions?\", \"assistant\": \"To find the widest spread, compute the difference between the highest and lowest values for each player. For Mahrez (12-5=7), Bounedjah (15-7=8), Brahimi (10-4=6), Slimani (11-3=8), Feghouli (8-2=6). Bounedjah and Slimani both have the widest spread (8 goals).\\nAnswer: Baghdad Bounedjah and Islam Slimani\" }, { \"user\": \"Arrange the players in ascending order of the total goals scored.",
    "id": "bbb7d402-0fed-4240-a74e-03a9c4546c05",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "For Mahrez (12-5=7), Bounedjah (15-7=8), Brahimi (10-4=6), Slimani (11-3=8), Feghouli (8-2=6). Bounedjah and Slimani both have the widest spread (8 goals).\\nAnswer: Baghdad Bounedjah and Islam Slimani\" }, { \"user\": \"Arrange the players in ascending order of the total goals scored.\", \"assistant\": \"Calculate the total goals for each player: Mahrez (25), Bounedjah (32), Brahimi (21), Slimani (23), Feghouli (16). Then arrange them in ascending order: Feghouli, Brahimi, Slimani, Mahrez, Bounedjah.\\nAnswer: Sofiane Feghouli, Yacine Brahimi, Islam Slimani, Riyad Mahrez, Baghdad Bounedjah\" }, { \"user\": \"Which playing condition appears to have the highest cumulative goals scored by all players?\", \"assistant\": \"Sum the goals for all players in each condition: Home (54), Away (42), Neutral (21). The \\\"Home\\\" condition has the highest cumulative goals.\\nAnswer: Home\" }, { \"user\": \"Which player performed more consistently across all playing conditions?\", \"assistant\": \"Consistency can be approximated by looking for players with similar values across all conditions, indicating less variation.",
    "id": "a0b0c08c-6ddc-433c-9e8f-1d9bacea23f0",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Sum the goals for all players in each condition: Home (54), Away (42), Neutral (21). The \\\"Home\\\" condition has the highest cumulative goals.\\nAnswer: Home\" }, { \"user\": \"Which player performed more consistently across all playing conditions?\", \"assistant\": \"Consistency can be approximated by looking for players with similar values across all conditions, indicating less variation. Yacine Brahimi‚Äôs values are relatively close (10, 7, 4), which suggests consistency.\\nAnswer: Yacine Brahimi\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 4, 2, 4, 2, 2, 4, 4, 4, 2 ] 2 [ 5, 5, 4, 4, 4, 4, 5, 5, 5, 4 ] 4 [ 5, 5, 4, 4, 4, 4, 4, 5, 4, 5 ] 4 [ { \"user\": \"How many data points are representing the Puzzle genre?\", \"assistant\": \"To find this, look for the color representing the Puzzle genre and count all data points with that color across all scatter plots in the matrix.\\nAnswer: Five\" }, { \"user\": \"Which platform has more daily active users for FPS games, PC or Console?\", \"assistant\": \"Observe the plot dimension representing Daily Active Users against Genre, locate the points for FPS games, and compare the DAU values for different platforms.\\nAnswer: Console\" }, { \"user\": \"What's the average session time for RPG games?\", \"assistant\": \"Look at the data points for RPG games in the Average Session Time axis.",
    "id": "3db9063f-c6a7-46f0-8cea-b362a90f1a2c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Observe the plot dimension representing Daily Active Users against Genre, locate the points for FPS games, and compare the DAU values for different platforms.\\nAnswer: Console\" }, { \"user\": \"What's the average session time for RPG games?\", \"assistant\": \"Look at the data points for RPG games in the Average Session Time axis. Identify the session times and compute their average: RPG has only one point with a session time of 120 minutes.\\nAnswer: 120 minutes\" }, { \"user\": \"Which genre has the highest daily active users?\", \"assistant\": \"Examine the matrix and look at the uppermost data point in the Daily Active Users dimension to identify the corresponding genre.\\nAnswer: Puzzle\" }, { \"user\": \"Do mobile platforms generally have higher player retention rates than PC platforms?\", \"assistant\": \"Compare the data points labeled as Mobile with those labeled as PC in the Player Retention Rate dimension to determine which platform has higher retention rates.\\nAnswer: Yes\" }, { \"user\": \"Which genre/platform combination has the most in-game purchases?\", \"assistant\": \"Identify the highest value in the In-Game Purchases dimension and check which genre and platform that data point represents.\\nAnswer: Puzzle/Mobile\" }, { \"user\": \"Are there any genres with less than 50% player retention rate?\", \"assistant\": \"Examine all data points in the Player Retention Rate dimension to see if any of them fall below the 50% mark, paying attention to their genre labels.\\nAnswer: No\" }, { \"user\": \"What is the relationship between average session time and player retention rate for MMO games?\", \"assistant\": \"Look at points representing MMO games, then compare the points in the respective dimensions of Average Session Time and Player Retention Rate to identify any trend.\\nAnswer: Positive correlation\" }, { \"user\": \"Which genre has a higher average session time on PC: RPG, Strategy, or MMO?",
    "id": "0e4a48d9-a8e6-4b65-b791-df19341b1d27",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Look at points representing MMO games, then compare the points in the respective dimensions of Average Session Time and Player Retention Rate to identify any trend.\\nAnswer: Positive correlation\" }, { \"user\": \"Which genre has a higher average session time on PC: RPG, Strategy, or MMO?\", \"assistant\": \"Compare the average session times for RPG, Strategy, and MMO genres on PC by checking the respective points in the Average Session Time dimension.\\nAnswer: MMO\" }, { \"user\": \"Which data point represents the highest in-game purchases for mobile platforms, and what are its Daily Active Users?\", \"assistant\": \"Locate the highest point in the In-Game Purchases dimension for Mobile platforms and check the Daily Active Users dimension for that point.\\nAnswer: Puzzle, 2,000,000\" } ] CoSyn-400k-chart [ 5, 3, 5, 4, 5, 5, 3, 5, 3, 5 ] 3 [ 4, 2, 2, 2, 2, 2, 2, 2, 2, 4 ] 2 [ 4, 4, 4, 5, 4, 5, 3, 4, 4, 5 ] 3 [ 4, 4, 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"Which day has the highest number of gaming sessions?\", \"assistant\": \"The rose chart on the left shows the frequency of gaming sessions by day of the week. By examining the bars, Sunday sticks out as the day with the highest number of sessions.\\nAnswer: Sunday\" }, { \"user\": \"How many gaming sessions occur on weekdays versus weekends?\", \"assistant\": \"Sum the counts of gaming sessions from Monday to Friday (weekdays) and compare that to the sum for Saturday and Sunday (weekends).",
    "id": "125d7d80-bbb3-45a2-a3a2-78a488955ad6",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The rose chart on the left shows the frequency of gaming sessions by day of the week. By examining the bars, Sunday sticks out as the day with the highest number of sessions.\\nAnswer: Sunday\" }, { \"user\": \"How many gaming sessions occur on weekdays versus weekends?\", \"assistant\": \"Sum the counts of gaming sessions from Monday to Friday (weekdays) and compare that to the sum for Saturday and Sunday (weekends). From the left chart: there are 4 on Monday, 3 on Tuesday, 5 on Wednesday, 6 on Thursday, and 7 on Friday, totaling 25 for weekdays. For weekends, there are 18 on Saturday and 17 on Sunday, totaling 35.\\nAnswer: 25 weekdays, 35 weekends\" }, { \"user\": \"Which time of day has the highest gaming session frequency?\", \"assistant\": \"The right side of the rose chart indicates the frequency of sessions by time ranges. The range \\\"19:00-22:00\\\" has the longest bar, signifying the highest frequency.\\nAnswer: 19:00-22:00\" }, { \"user\": \"How does the number of sessions on Saturday compare to those on Thursday?\", \"assistant\": \"Using the left chart, we see that Saturday has 2 time ranges (15:00-18:00 and 19:00-22:00) totaling (8 + 10 = 18) sessions. Thursday has 1 session range (18:00-21:00) totaling 6 sessions. Therefore, Saturday has 12 more sessions than Thursday.\\nAnswer: 12 more on Saturday\" }, { \"user\": \"What is the most popular time slot for gaming sessions on weekends?\", \"assistant\": \"The right chart shows the time distribution. \\\"19:00-22:00\\\" is the most frequent time slot for both weekend days (Saturday and Sunday combined).\\nAnswer: 19:00-22:00\" }, { \"user\": \"On average, how many sessions occur per day?",
    "id": "144f514f-45d8-404a-925b-a86d3ae6ccbf",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The right chart shows the time distribution. \\\"19:00-22:00\\\" is the most frequent time slot for both weekend days (Saturday and Sunday combined).\\nAnswer: 19:00-22:00\" }, { \"user\": \"On average, how many sessions occur per day?\", \"assistant\": \"Divide the total number of sessions across all days by 7 (the number of days). In total, there are 60 sessions. Calculation: 60/7 ‚âà 8.57.\\nAnswer: About 8.57\" }, { \"user\": \"Which day has the least number of gaming sessions?\", \"assistant\": \"From the left chart, Tuesday has the shortest bar, indicating the least number of gaming sessions.\\nAnswer: Tuesday\" }, { \"user\": \"Which time interval has the least number of gaming sessions and how many?\", \"assistant\": \"Observing the right chart, \\\"20:00-23:00\\\" and \\\"14:00-17:00\\\" both show the shortest bars, each having 1 session.\\nAnswer: 2 intervals, 1 session each\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 2, 2, 4, 2, 2, 2, 4 ] 2 [ 4, 4, 4, 4, 3, 3, 4, 4 ] 3 [ 4, 4, 5, 5, 4, 5, 4, 4 ] 4 [ { \"user\": \"What is the title of the plot?\", \"assistant\": \"The title is placed at the top and centered. It reads \\\"Volcano Plot: Gene Expression Changes in EHR-Enabled vs. Non-EHR Areas\\\".\\nAnswer: Volcano Plot: Gene Expression Changes in EHR-Enabled vs.",
    "id": "1bc5866a-4f1d-4eff-ab7f-2afbecf093e8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title is placed at the top and centered. It reads \\\"Volcano Plot: Gene Expression Changes in EHR-Enabled vs. Non-EHR Areas\\\".\\nAnswer: Volcano Plot: Gene Expression Changes in EHR-Enabled vs. Non-EHR Areas\" }, { \"user\": \"Which gene has the most significant negative log2 fold change?\", \"assistant\": \"Look for the point farthest to the left and check the annotated gene label. The gene farthest to the left is IL6.\\nAnswer: IL6\" }, { \"user\": \"How many genes have a p-value less than 0.05?\", \"assistant\": \"Count the data points that are above the horizontal line representing -log10(p-value) of 0.05. There are 10 such points.\\nAnswer: 10\" }, { \"user\": \"Which gene has the highest -log10(p-value)?\", \"assistant\": \"Identify the point with the highest y-axis value and read the annotated gene label. The gene with the highest -log10(p-value) is IL6.\\nAnswer: IL6\" }, { \"user\": \"Is there any gene with a log2 fold change greater than 1 that is also statistically significant (p-value < 0.05)?\", \"assistant\": \"Identify points with x-coordinate greater than 1 and y-coordinate above the horizontal line at -log10(0.05). The gene IFNG meets this criterion.\\nAnswer: IFNG\" }, { \"user\": \"Which gene has a log2 fold change closest to zero but is still statistically significant?\", \"assistant\": \"Look for the point closest to the y-axis (x=0) that also lies above the horizontal significance threshold line. The gene STAT3 fits this description.\\nAnswer: STAT3\" }, { \"user\": \"What color represents genes with positive log2 fold change values?",
    "id": "8d7e2ee7-1a52-4947-89a0-bf7403edaef4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The gene IFNG meets this criterion.\\nAnswer: IFNG\" }, { \"user\": \"Which gene has a log2 fold change closest to zero but is still statistically significant?\", \"assistant\": \"Look for the point closest to the y-axis (x=0) that also lies above the horizontal significance threshold line. The gene STAT3 fits this description.\\nAnswer: STAT3\" }, { \"user\": \"What color represents genes with positive log2 fold change values?\", \"assistant\": \"The colors represented along the color bar for positive log2 fold changes range from light blue to red.\\nAnswer: Light blue to red\" }, { \"user\": \"Of the genes with a log2 fold change less than -1, which has the smallest p-value?\", \"assistant\": \"Among the points to the left of -1, find the one highest on the y-axis (smallest p-value). The gene IL6 fits this criterion.\\nAnswer: IL6\" }, { \"user\": \"What is the log2 fold change of VEGFA?\", \"assistant\": \"Locate the point labeled VEGFA and check its x-coordinate value. VEGFA has a log2 fold change of 1.0.\\nAnswer: 1.0\" }, { \"user\": \"Are there more genes with a positive or negative log2 fold change that are statistically significant?\", \"assistant\": \"Count the number of genes above the horizontal line at -log10(0.05) with positive log2 fold change and compare with those with negative log2 fold change.",
    "id": "ff62a9a6-068b-476a-a511-317e548d8c40",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "VEGFA has a log2 fold change of 1.0.\\nAnswer: 1.0\" }, { \"user\": \"Are there more genes with a positive or negative log2 fold change that are statistically significant?\", \"assistant\": \"Count the number of genes above the horizontal line at -log10(0.05) with positive log2 fold change and compare with those with negative log2 fold change. There are 4 statistically significant positive FC genes (IFNG, IL10, NFKB1, CCL2) and 6 statistically significant negative FC genes (IL6, TNF, CXCL8, TLR4, HLA-DRA, PTGS2).\\nAnswer: Negative\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 4, 4, 4, 3, 2, 4, 4, 2, 4 ] 2 [ 5, 5, 4, 5, 5, 5, 5, 5, 4, 5 ] 4 [ 4, 5, 4, 5, 4, 4, 4, 5, 5, 4 ] 4 [ { \"user\": \"What's the total number of free trial users?\", \"assistant\": \"The total number of free trial users is indicated at the beginning of the chart under \\\"Total Free Trial Users\\\".\\nAnswer: 10,000\" }, { \"user\": \"How many users completed onboarding?\", \"assistant\": \"The decrease in users after completing onboarding is shown as -1,500.\\nAnswer: 1,500\" }, { \"user\": \"What is the number of users who converted to paid subscribers?",
    "id": "1dd81818-268c-4ff7-8efe-7f1b64b71ef3",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The total number of free trial users is indicated at the beginning of the chart under \\\"Total Free Trial Users\\\".\\nAnswer: 10,000\" }, { \"user\": \"How many users completed onboarding?\", \"assistant\": \"The decrease in users after completing onboarding is shown as -1,500.\\nAnswer: 1,500\" }, { \"user\": \"What is the number of users who converted to paid subscribers?\", \"assistant\": \"The final value on the chart, next to \\\"Converted to Paid\\\", shows this number as +2,000.\\nAnswer: 2,000\" }, { \"user\": \"How many users did not engage with the AI Therapist?\", \"assistant\": \"The change in users due to not engaging with the AI Therapist is shown as -2,000.\\nAnswer: 2,000\" }, { \"user\": \"What is the net change in users from the starting point to the final conversion?\", \"assistant\": \"The chart starts with 10,000 users and ends with 2,000 users converting to paid subscribers. The net change is \\\\(10,000 - 2,000 = 8,000\\\\) users lost.\\nAnswer: 8,000\" }, { \"user\": \"Which step has the highest number of users lost apart from the initial and final steps?\", \"assistant\": \"The step \\\"Engaged with AI Therapist\\\" shows the largest decrease of -2,000 users.\\nAnswer: Engaged with AI Therapist\" }, { \"user\": \"What's the cumulative number of users who left before reaching the trial end?\", \"assistant\": \"Sum the losses: -1,500 (Onboarding) + -2,000 (AI Therapist) + -1,200 (Mood Tracker) + -800 (Group Session) + -1,000 (Personalized Plan) = -6,500 users\\nAnswer: 6,500\" }, { \"user\": \"What percentage of the total free trial users converted to paid subscribers?",
    "id": "9003486e-e375-4ba0-a5cf-4e2251173396",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Sum the losses: -1,500 (Onboarding) + -2,000 (AI Therapist) + -1,200 (Mood Tracker) + -800 (Group Session) + -1,000 (Personalized Plan) = -6,500 users\\nAnswer: 6,500\" }, { \"user\": \"What percentage of the total free trial users converted to paid subscribers?\", \"assistant\": \"Divide the number of users who converted by the total free trial users and multiply by 100: \\\\((2,000 / 10,000)  100 = 20\\\\%\\\\).\\nAnswer: 20%\" }, { \"user\": \"How does the number of users completing onboarding compare to those attending group sessions?\", \"assistant\": \"After onboarding, 1,500 users were lost while 800 were lost after attending group sessions. 1,500 is greater than 800.\\nAnswer: More users were lost after onboarding\" }, { \"user\": \"How many steps resulted in a decrease in user count?\", \"assistant\": \"Five steps (Onboarding, AI Therapist, Mood Tracker, Group Session, Personalized Plan) resulted in a loss of users.\\nAnswer: Five steps\" } ] CoSyn-400k-chart [ 5, 1, 5, 5, 5, 5, 5, 5, 3, 5 ] 1 [ 1, 2, 2, 4, 1, 4, 2, 2, 2, 4 ] 1 [ 5, 4, 5, 4, 5, 5, 5, 5, 4, 5 ] 4 [ 4, 4, 5, 4, 5, 4, 5, 5, 4, 4 ] 4 [ { \"user\": \"How many stadiums are displayed in the figure?",
    "id": "6a87b33f-51d8-4672-9fd8-b2ddb45aaedc",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"There are 7 data entries, each representing a different stadium.\\nAnswer: 7\" }, { \"user\": \"Which stadium has the highest actual attendance?\", \"assistant\": \"The actual attendance bars show that Camp Nou has the highest actual attendance at 84,500.\\nAnswer: Camp Nou\" }, { \"user\": \"What is the title of the figure?\", \"assistant\": \"The title is displayed at the top of the figure. It reads \\\"Fan Attendance at Major Soccer Stadiums\\\".\\nAnswer: Fan Attendance at Major Soccer Stadiums\" }, { \"user\": \"Which stadium has the closest actual attendance to its capacity?\", \"assistant\": \"By comparing the heights of the actual attendance and stadium capacity bars, Old Trafford has the closest actual attendance to its capacity of 74,140 versus 72,000 actual attendance.\\nAnswer: Old Trafford\" }, { \"user\": \"Which stadium has the lowest average attendance?\", \"assistant\": \"The orange tick marks represent average attendance, showing San Siro with the lowest average attendance at 60,000.\\nAnswer: San Siro\" }, { \"user\": \"What is the difference between the actual attendance and average attendance for Signal Iduna Park?\", \"assistant\": \"Subtract the average attendance (75,000) from the actual attendance (79,000): 79,000 - 75,000 = 4,000.\\nAnswer: 4,000\" }, { \"user\": \"How does Wembley Stadium‚Äôs actual attendance compare to its capacity?\", \"assistant\": \"The actual attendance bar for Wembley Stadium is 82,000, while the stadium capacity bar is 90,000. 82,000 is less than 90,000.\\nAnswer: It is less\" }, { \"user\": \"Which stadiums have an actual attendance greater than 80,000?",
    "id": "076fa011-6e58-4de4-bf13-4a8e8c291cdd",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The actual attendance bar for Wembley Stadium is 82,000, while the stadium capacity bar is 90,000. 82,000 is less than 90,000.\\nAnswer: It is less\" }, { \"user\": \"Which stadiums have an actual attendance greater than 80,000?\", \"assistant\": \"Identify the bars over 80,000: Camp Nou (84,500) and Wembley Stadium (82,000).\\nAnswer: Camp Nou, Wembley Stadium\" }, { \"user\": \"What's the average of actual attendance across all stadiums?\", \"assistant\": \"Summing all actual attendances: 84,500+72,000+79,000+69,000+71,000+82,000+65,000 = 522,500. Dividing by the number of stadiums (7): 522,500/7 = 74,643\\nAnswer: 74,643\" }, { \"user\": \"What is the median stadium capacity for the listed stadiums?\", \"assistant\": \"Arrange capacities: 74,140, 75,024, 80,018, 81,044, 81,365, 90,000, 99,354.",
    "id": "f9ef8ce9-7df1-4d1a-9768-0ef392f5a837",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Dividing by the number of stadiums (7): 522,500/7 = 74,643\\nAnswer: 74,643\" }, { \"user\": \"What is the median stadium capacity for the listed stadiums?\", \"assistant\": \"Arrange capacities: 74,140, 75,024, 80,018, 81,044, 81,365, 90,000, 99,354. The median is the fourth value in an odd-numbered list, which is 81,044.\\nAnswer: 81,044\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 4, 5, 4, 2, 4, 4, 3, 4, 4 ] 2 [ 5, 5, 5, 5, 5, 4, 4, 5, 4, 4 ] 4 [ 4, 5, 4, 4, 4, 5, 5, 4, 5, 4 ] 4 [ { \"user\": \"What was the revenue for the Technology category in 2020?\", \"assistant\": \"Look at the line corresponding to the Technology category and find its value at the year 2020.\\nAnswer: 950\" }, { \"user\": \"Which course category had the highest revenue growth from 2019 to 2020?\", \"assistant\": \"Compare the revenue values for each category between 2019 and 2020 and identify the one with the largest increase.\\nAnswer: Technology\" }, { \"user\": \"Did any category have a decline in revenue over the analyzed period?\", \"assistant\": \"Check the overall trend lines for each category to see if any have a downward slope at any point.\\nAnswer: No\" }, { \"user\": \"In which year did the Business category first exceed 700 million USD in revenue?",
    "id": "1d110f26-905c-42d0-9b17-b0a8d421d008",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Check the overall trend lines for each category to see if any have a downward slope at any point.\\nAnswer: No\" }, { \"user\": \"In which year did the Business category first exceed 700 million USD in revenue?\", \"assistant\": \"Follow the Business category line and identify the year when it first crosses the 700 million USD mark.\\nAnswer: 2021\" }, { \"user\": \"How much higher was the revenue for the Technology category than for Personal Development in 2023?\", \"assistant\": \"Look at the revenue values for both Technology and Personal Development categories in 2023 and subtract the latter from the former.\\nAnswer: 1230\" }, { \"user\": \"What is the average revenue growth per year for the Language category between 2015 and 2020?\", \"assistant\": \"Calculate the difference in revenue for Language between 2015 and 2020 and divide by the number of years (2020-2015).\\nAnswer: 51\" }, { \"user\": \"Which year saw the largest single-year revenue jump across all categories?\", \"assistant\": \"Compare the year-over-year changes for all categories and identify the year with the highest single-year increase.\\nAnswer: 2020\" }, { \"user\": \"For how many years did the revenue for the Arts category remain below 100 million USD?\", \"assistant\": \"Check the revenue figures for the Arts category and count the number of years it stayed below 100 million USD.\\nAnswer: 4\" }, { \"user\": \"Compare the revenue of Business and Personal Development categories in 2018. Which was higher, and by how much?\", \"assistant\": \"Find the revenue values for both categories in 2018, identify the higher one, and calculate the difference.\\nAnswer: Business, 185\" }, { \"user\": \"What is the cumulative revenue for the Technology category from 2013 to 2023?",
    "id": "3cfc3dfa-a472-45f8-8365-502d8d38e54d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Which was higher, and by how much?\", \"assistant\": \"Find the revenue values for both categories in 2018, identify the higher one, and calculate the difference.\\nAnswer: Business, 185\" }, { \"user\": \"What is the cumulative revenue for the Technology category from 2013 to 2023?\", \"assistant\": \"Sum up the revenue figures for the Technology category across all the years from 2013 to 2023.\\nAnswer: 8330\" } ] CoSyn-400k-chart [ 5, 5, 3, 5, 5, 5, 5, 5, 3, 5 ] 3 [ 3, 4, 2, 4, 4, 4, 2, 5, 4, 5 ] 2 [ 5, 5, 4, 4, 5, 4, 5, 4, 5, 4 ] 4 [ 4, 4, 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"Which vertebrate class has the highest pepsin activity level?\", \"assistant\": \"Look at the \\\"Pepsin\\\" subplot and identify the bar with the greatest height. The tallest bar corresponds to mammals.\\nAnswer: Mammals\" }, { \"user\": \"Compare the activity levels of trypsin in reptiles and fish; which one is higher?\", \"assistant\": \"Locate the \\\"Trypsin\\\" subplot and compare the heights of the bars representing reptiles and fish. The bars are of equal height.\\nAnswer: They are equal\" }, { \"user\": \"What is the difference in protease activity levels between mammals and amphibians?\", \"assistant\": \"Find the \\\"Protease\\\" subplot and locate the bars for mammals and amphibians.",
    "id": "1e22e46b-6c1d-4a69-8201-30a4a95ec36a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Locate the \\\"Trypsin\\\" subplot and compare the heights of the bars representing reptiles and fish. The bars are of equal height.\\nAnswer: They are equal\" }, { \"user\": \"What is the difference in protease activity levels between mammals and amphibians?\", \"assistant\": \"Find the \\\"Protease\\\" subplot and locate the bars for mammals and amphibians. The difference is calculated as 95 - 70.\\nAnswer: 25\" }, { \"user\": \"Which enzyme shows the smallest activity level across all vertebrate classes?\", \"assistant\": \"Examine the subplots for the shortest bars across all enzymes. The enzyme with the smallest average height is Chitinase.\\nAnswer: Chitinase\" }, { \"user\": \"Which enzyme has the most balanced activity levels across all vertebrate classes?\", \"assistant\": \"Look for a subplot where the bars have nearly the same height across all classes. The subplot for Carboxypeptidase shows visually similar bar heights.\\nAnswer: Carboxypeptidase\" }, { \"user\": \"What is the highest activity level of lactase and in which vertebrate class does it occur?\", \"assistant\": \"Check the \\\"Lactase\\\" subplot to find the tallest bar, which corresponds to mammals. The height is 75.\\nAnswer: 75 in Mammals\" }, { \"user\": \"Calculate the average activity level of phospholipase across all vertebrate classes.\", \"assistant\": \"Find the \\\"Phospholipase\\\" subplot, sum the values (75 + 70 + 65 + 60 + 65 = 335), and divide by the number of classes (5). The average is 335 / 5.\\nAnswer: 67\" }, { \"user\": \"Which vertebrate class has the lowest activity level for protease?",
    "id": "cfd53ed0-5c57-4a15-8af8-595adb4048e6",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Find the \\\"Phospholipase\\\" subplot, sum the values (75 + 70 + 65 + 60 + 65 = 335), and divide by the number of classes (5). The average is 335 / 5.\\nAnswer: 67\" }, { \"user\": \"Which vertebrate class has the lowest activity level for protease?\", \"assistant\": \"Look at the \\\"Protease\\\" subplot and identify the shortest bar, which belongs to amphibians with an activity level of 70.\\nAnswer: Amphibians\" }, { \"user\": \"Compare the activity levels of maltase in birds and amphibians and determine the percentage difference.\", \"assistant\": \"Find the \\\"Maltase\\\" subplot, locate the bars for birds (80) and amphibians (60), and calculate the percentage difference ((80 - 60) / 80  100).\\nAnswer: 25%\" }, { \"user\": \"Which three enzymes have the highest activity levels in reptiles?\", \"assistant\": \"Look at the enzyme subplots and identify the three tallest bars for reptiles. They are Protease (80), Pepsin (85), and Trypsin (85).\\nAnswer: Protease, Pepsin, Trypsin\" } ] CoSyn-400k-chart [ 5, 3, 1, 5, 5, 5, 5, 5, 5, 5 ] 1 [ 2, 2, 2, 2, 2, 2, 2, 2, 1, 2 ] 1 [ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 4, 4, 4, 4, 4, 4, 4, 4, 5 ] 4 [ { \"user\": \"What is the title of the plot?",
    "id": "470d620e-b598-4e7a-9a4d-bdf8b01216b7",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title is located at the top of the figure, centered and in larger font, which typically describes the subject of the plot.\\nAnswer: Voting Rights Violations by State and Demographic\" }, { \"user\": \"What is the color representing the highest number of voting rights violations?\", \"assistant\": \"The contour plot uses a color gradient to represent different values, with the legend or color bar indicating which colors correspond to higher or lower values.\\nAnswer: Darker shades (usually towards the end of the color gradient)\" }, { \"user\": \"Which state reports the highest number of voting rights violations for African Americans?\", \"assistant\": \"Look at the labeled values on the Y-axis for states, then cross-reference the African American demographic on the X-axis to find the highest labeled number.\\nAnswer: Georgia (200)\" }, { \"user\": \"Which demographic group in Texas has the least number of voting rights violations reported?\", \"assistant\": \"Locate Texas on the Y-axis, and then find the smallest labeled number in that row corresponding to each demographic on the X-axis.\\nAnswer: Asian (30)\" }, { \"user\": \"How many states have reported more than 150 voting rights violations for African Americans?\", \"assistant\": \"Go through the African American column and count the number of states with values over 150.\\nAnswer: 5\" }, { \"user\": \"Compare the number of voting rights violations reported for Hispanics in Alabama and Texas. Which state has more violations?\", \"assistant\": \"Locate the Hispanic demographic under both Alabama and Texas rows and compare their values.\\nAnswer: Texas (100 vs. 45)\" }, { \"user\": \"Which demographic group shows the most equal distribution of voting rights violations among all states?\", \"assistant\": \"Visually compare the spread and deviation of each demographic in the contour plot to see which has the most uniform color or similar values.\\nAnswer: Caucasian\" }, { \"user\": \"What is the total number of voting rights violations reported in Florida across all demographics?",
    "id": "5cb39f05-f0e6-4856-99e7-96be809883b1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "45)\" }, { \"user\": \"Which demographic group shows the most equal distribution of voting rights violations among all states?\", \"assistant\": \"Visually compare the spread and deviation of each demographic in the contour plot to see which has the most uniform color or similar values.\\nAnswer: Caucasian\" }, { \"user\": \"What is the total number of voting rights violations reported in Florida across all demographics?\", \"assistant\": \"Sum up the violations for each demographic in Florida (African American, Hispanic, Caucasian, Asian).\\nAnswer: 190 + 90 + 50 + 25 = 355\" }, { \"user\": \"Find the average number of voting rights violations reported for Asians across all states.\", \"assistant\": \"Sum up the violations reported for Asians in all states and divide by the number of states.\\nAnswer: (20+25+15+10+30+25+20) / 7 = 145 / 7 ‚âà 20.7\" }, { \"user\": \"Between African Americans in Mississippi and Hispanics in Florida, who experienced more reported voting rights violations and by how much?\", \"assistant\": \"Compare the values for African Americans in Mississippi and Hispanics in Florida and calculate the difference.\\nAnswer: African Americans in Mississippi; 170 - 90 = 80 more\" } ] CoSyn-400k-chart [ 5, 3, 4, 4, 5, 5, 3, 5, 5, 5 ] 3 [ 5, 4, 2, 2, 2, 4, 2, 2, 2, 2 ] 2 [ 5, 5, 5, 5, 4, 5, 5, 4, 4, 5 ] 4 [ 5, 4, 5, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"Which sensor type do professional photographers use most frequently when capturing volcanic activity?",
    "id": "f55ee32e-e4ab-4da4-95d9-300406786cbe",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Professional photographers use full-frame sensors most frequently. We can see this as the bar for full-frame sensor usage by professionals is the tallest compared to other sensors in its category, reaching 45 units of frequency.\\nAnswer: Full-frame\" }, { \"user\": \"Which sensor type is more popular among amateur photographers, APS-C or Micro Four Thirds?\", \"assistant\": \"To determine popularity, compare the height of the bars for amateur photographers under APS-C and Micro Four Thirds. The bar for Micro Four Thirds is taller than that of APS-C for amateur photographers, with a usage frequency of 50 versus 40. Therefore, Micro Four Thirds is more popular among amateurs.\\nAnswer: Micro Four Thirds\" }, { \"user\": \"What is the total usage frequency for APS-C sensors by both professional and amateur photographers?\", \"assistant\": \"To find the total, add the usage frequencies of APS-C sensors by professionals and amateurs. The usage frequency is 35 for professionals and 40 for amateurs, resulting in a sum of 35 + 40 = 75.\\nAnswer: 75\" }, { \"user\": \"Is the usage frequency of full-frame sensors by professional photographers greater than the sum of the usage frequencies of all other sensors by professional photographers?\", \"assistant\": \"First, check the usage frequency of full-frame sensors by professionals, which is 45. Then, sum the usage frequencies of APS-C (35) and Micro Four Thirds (20) by professionals, which totals 35 + 20 = 55. Since 45 is not greater than 55, the full-frame usage is not greater than the sum of the others.\\nAnswer: No\" }, { \"user\": \"Which user type, professional or amateur, uses Micro Four Thirds sensors more frequently for volcanic photography?\", \"assistant\": \"Compare the heights of the bars for professional and amateur photographers using Micro Four Thirds sensors. The bar for amateurs reaches a usage frequency of 50, which is higher than 20 for professionals.",
    "id": "833b8241-7099-4543-b707-0a207b48dbc6",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Compare the heights of the bars for professional and amateur photographers using Micro Four Thirds sensors. The bar for amateurs reaches a usage frequency of 50, which is higher than 20 for professionals. Therefore, amateurs use Micro Four Thirds sensors more frequently.\\nAnswer: Amateur\" }, { \"user\": \"What is the difference in usage frequency of Micro Four Thirds sensors between amateur and professional photographers?\", \"assistant\": \"Subtract the usage frequency of professional photographers from that of amateur photographers for Micro Four Thirds sensors. The values are 50 for amateurs and 20 for professionals, resulting in a difference of 50 - 20 = 30.\\nAnswer: 30\" }, { \"user\": \"What is the average usage frequency of full-frame sensors when considering both professional and amateur photographers?\", \"assistant\": \"To calculate the average, add the usage frequencies of full-frame sensors by professionals (45) and amateurs (30), then divide by 2. The sum is 45 + 30 = 75, and the average is 75 √∑ 2 = 37.5.\\nAnswer: 37.5\" } ] CoSyn-400k-chart [ 3, 5, 5, 5, 5, 3, 2 ] 2 [ 2, 2, 4, 2, 4, 2, 2 ] 2 [ 5, 5, 5, 4, 5, 5, 4 ] 4 [ 4, 4, 4, 4, 4, 5, 5 ] 4 [ { \"user\": \"which city has the lowest average female income?\", \"assistant\": \"Comparing the average female incomes, Cairo has the lowest average income among females. The average incomes are represented by the text values in the plot.\\nAnswer: Cairo\" }, { \"user\": \"What is the average male income in Riyadh?",
    "id": "53023e38-7ab3-46b5-ae6f-b018e6edfdcb",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Comparing the average female incomes, Cairo has the lowest average income among females. The average incomes are represented by the text values in the plot.\\nAnswer: Cairo\" }, { \"user\": \"What is the average male income in Riyadh?\", \"assistant\": \"The average male income in Riyadh can be found by looking at the labeled mean value for males in Riyadh. The mean income is shown as text in the plot.\\nAnswer: 3100\" }, { \"user\": \"Which city has the highest male income?\", \"assistant\": \"By comparing the average male incomes across all cities, Dubai stands out with the highest average male income. The average incomes are displayed as text values in the plot.\\nAnswer: Dubai\" }, { \"user\": \"How does the average female income in Amman compare to the average female income in Beirut?\", \"assistant\": \"The average incomes are displayed on the plot as text values. Beirut's average female income is 2067, while Amman's is 1800. Beirut's average female income is higher.\\nAnswer: Beirut has a higher average female income\" }, { \"user\": \"In which city is the income distribution for males more spread out compared to females?\", \"assistant\": \"Observing the widths of the violin plots for each gender in each city, we can see in Dubai, the male income distribution is visibly more spread out compared to the female income distribution.\\nAnswer: Dubai\" }, { \"user\": \"What is the purpose of the colors in the chart?\", \"assistant\": \"The colors in the chart differentiate between male and female income distributions. One color represents males, and the other represents females, ensuring it's easy to compare between genders.\\nAnswer: To differentiate between genders\" }, { \"user\": \"Can you find outliers in any gender in any city?\", \"assistant\": \"Outliers in a violin plot are typically shown as distinct points outside the main distribution range.",
    "id": "9bed9bee-1212-4ea9-a8b6-cfe8555a7171",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The colors in the chart differentiate between male and female income distributions. One color represents males, and the other represents females, ensuring it's easy to compare between genders.\\nAnswer: To differentiate between genders\" }, { \"user\": \"Can you find outliers in any gender in any city?\", \"assistant\": \"Outliers in a violin plot are typically shown as distinct points outside the main distribution range. In this chart, no distinct outliers appear since the data seems evenly distributed and lacks extreme values.\\nAnswer: No visible outliers\" }, { \"user\": \"Which city has the closest average incomes for males and females?\", \"assistant\": \"By comparing the average incomes for males and females displayed in the plot, Riyadh shows the smallest difference between male (3100) and female (2900) incomes. The difference is 200, which is less compared to other cities.\\nAnswer: Riyadh\" } ] CoSyn-400k-chart [ 3, 3, 5, 5, 5, 5, 3, 5 ] 3 [ 2, 2, 2, 2, 4, 4, 2, 2 ] 2 [ 5, 4, 5, 4, 4, 5, 4, 5 ] 4 [ 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What is the title of the figure?\", \"assistant\": \"The title is located at the top of the plot and provides a summary of what the plot represents.\\nAnswer: Distribution of Charitable Donations: French Citizens vs. Celebrities\" }, { \"user\": \"What does the x-axis represent?\", \"assistant\": \"The x-axis is the horizontal axis of the plot and represents the variable that is being measured. In this case, it shows the donation amounts in euros.\\nAnswer: Donation Amount (‚Ç¨)\" }, { \"user\": \"Which group has a wider range of donation amounts?",
    "id": "90ae482c-c8bb-4101-9503-d9cbff61a625",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Celebrities\" }, { \"user\": \"What does the x-axis represent?\", \"assistant\": \"The x-axis is the horizontal axis of the plot and represents the variable that is being measured. In this case, it shows the donation amounts in euros.\\nAnswer: Donation Amount (‚Ç¨)\" }, { \"user\": \"Which group has a wider range of donation amounts?\", \"assistant\": \"To determine this, examine the x-axis range where each group's density is nonzero. The French Citizens' donations range from around ‚Ç¨100 to ‚Ç¨100,000, while Celebrities' donations range from around ‚Ç¨500,000 to ‚Ç¨10,000,000. Celebrities have a wider range.\\nAnswer: Celebrities\" }, { \"user\": \"What is the color of the area representing French Citizens?\", \"assistant\": \"The color can be identified by looking at the legend in the plot. The area representing French Citizens is filled with a blue color.\\nAnswer: Blue\" }, { \"user\": \"For which group is the density peak higher?\", \"assistant\": \"To find the group with a higher density peak, compare the height of the filled areas on the plot. The peak density for French Citizens is visibly higher than for Celebrities.\\nAnswer: French Citizens\" }, { \"user\": \"How do the donation amounts compare at the ‚Ç¨1,000 mark for both groups?\", \"assistant\": \"Locate the ‚Ç¨1,000 mark on the x-axis and observe the density values for both groups at this point. French Citizens have a significant density at this donation amount, whereas Celebrities have almost none.\\nAnswer: French Citizens have higher density\" }, { \"user\": \"What is the difference in the maximum donation amounts between the two groups?\", \"assistant\": \"The maximum donation for French Citizens is approximately ‚Ç¨100,000. For Celebrities, it is ‚Ç¨10,000,000.",
    "id": "290f32a1-7021-46e4-8fef-9fd097b2ed20",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "French Citizens have a significant density at this donation amount, whereas Celebrities have almost none.\\nAnswer: French Citizens have higher density\" }, { \"user\": \"What is the difference in the maximum donation amounts between the two groups?\", \"assistant\": \"The maximum donation for French Citizens is approximately ‚Ç¨100,000. For Celebrities, it is ‚Ç¨10,000,000. Calculate the difference: ‚Ç¨10,000,000 - ‚Ç¨100,000 = ‚Ç¨9,900,000.\\nAnswer: ‚Ç¨9,900,000\" }, { \"user\": \"What can we infer about the general trend of donation amounts from French Citizens compared to Celebrities?\", \"assistant\": \"The general trend shows that French Citizens tend to make smaller, more frequent donations, while Celebrities make fewer but significantly larger donations.\\nAnswer: French Citizens make smaller, frequent donations; Celebrities make larger, fewer donations\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 5, 4, 4, 4, 4, 4, 2 ] 2 [ 5, 5, 5, 4, 5, 4, 4, 5 ] 4 [ 4, 4, 4, 5, 4, 4, 5, 4 ] 4 [ { \"user\": \"What is the title of the Sankey diagram?\", \"assistant\": \"The title of the Sankey diagram is usually displayed at the top of the figure. In this case, it is provided in the description of the layout under the title_text parameter.\\nAnswer: Changes in Family Structure Over Generations\" }, { \"user\": \"How many different family structures are represented in the diagram?\", \"assistant\": \"The labels section lists all the unique family structures displayed in the diagram.",
    "id": "798ef144-cd34-45ec-9720-7a43de8d5b47",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title of the Sankey diagram is usually displayed at the top of the figure. In this case, it is provided in the description of the layout under the title_text parameter.\\nAnswer: Changes in Family Structure Over Generations\" }, { \"user\": \"How many different family structures are represented in the diagram?\", \"assistant\": \"The labels section lists all the unique family structures displayed in the diagram. Counting these labels gives us the total number of different family structures.\\nAnswer: 6\" }, { \"user\": \"What is the most common transition from a nuclear family in the 1920s?\", \"assistant\": \"By examining the values associated with the transitions from the Nuclear Family (1920s) to the other family structures in the 1950s, we can see that the transition with the highest value indicates the most common transition.\\nAnswer: Extended Family (1950s)\" }, { \"user\": \"How many transitions involve a nuclear family in the 1950s?\", \"assistant\": \"We need to count the number of links that have Nuclear Family (1950s) as either the source or the target node.\\nAnswer: 3\" }, { \"user\": \"What percentage of families were part of an extended family in the 1980s?\", \"assistant\": \"To find this, we need to examine the values of links pointing to Extended Family (1980s) and sum them up, then divide by the total number of families in the 1980s.\\nAnswer: 55%\" }, { \"user\": \"Does the diagram show more transitions to extended families or nuclear families in the 2020s?\", \"assistant\": \"We compare the number of and the values of the transitions that end at Extended Family (2020s) with those that end at Nuclear Family (2020s).\\nAnswer: Nuclear families\" }, { \"user\": \"How many families transitioned from a nuclear family in the 1980s to a multi-generational household in the 2020s?",
    "id": "2a00fe3e-3229-4c06-9fa5-adf53b1574f4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"We compare the number of and the values of the transitions that end at Extended Family (2020s) with those that end at Nuclear Family (2020s).\\nAnswer: Nuclear families\" }, { \"user\": \"How many families transitioned from a nuclear family in the 1980s to a multi-generational household in the 2020s?\", \"assistant\": \"By looking at the value associated with the transition from Nuclear Family (1980s) to Multi-Generational Household (2020s), we can determine the number of families involved.\\nAnswer: 15\" }, { \"user\": \"What is the largest single transition in the entire diagram?\", \"assistant\": \"Finding the link with the highest value will give us the largest single transition in the diagram.\\nAnswer: Nuclear Family (1920s) to Extended Family (1950s)\" }, { \"user\": \"How many families were part of a nuclear family in all three periods: 1920s, 1950s, and 1980s?\", \"assistant\": \"We need to look for a continuous transition from Nuclear Family (1920s) to Nuclear Family (1950s) and then to Nuclear Family (1980s) and sum the values associated with these transitions.\\nAnswer: 25\" } ] CoSyn-400k-chart [ 5, 5, 3, 5, 4, 4, 5, 5, 3 ] 3 [ 5, 5, 2, 4, 2, 5, 2, 4, 3 ] 2 [ 5, 5, 5, 4, 4, 5, 4, 5, 3 ] 3 [ 4, 4, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What's the largest percentage of Maori language fluency among the age groups?",
    "id": "1cba5511-3fd1-4a70-949c-59b9f7a343fa",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Look at the pie chart and identify the age group with the largest segment. The largest segment represents \\\"65 and older\\\" with 23%.\\nAnswer: 23%\" }, { \"user\": \"Which age group has the smallest percentage of Maori language fluency?\", \"assistant\": \"Identify the smallest segment in the pie chart, which corresponds to the \\\"Under 5\\\" age group with 4%.\\nAnswer: Under 5\" }, { \"user\": \"How much larger is the fluency percentage of the \\\"65 and older\\\" group compared to the \\\"Under 5\\\" group?\", \"assistant\": \"The \\\"65 and older\\\" group has 23% fluency, and the \\\"Under 5\\\" group has 4%. The difference is 23% - 4% = 19%.\\nAnswer: 19%\" }, { \"user\": \"What is the combined percentage of Maori language fluency for the \\\"13-17\\\" and \\\"5-12\\\" age groups?\", \"assistant\": \"Add the percentages of the \\\"13-17\\\" group (15%) and the \\\"5-12\\\" group (21%). The combined percentage is 15% + 21% = 36%.\\nAnswer: 36%\" }, { \"user\": \"Is the fluency percentage of the \\\"25-34\\\" age group less than that of the \\\"50-64\\\" age group?\", \"assistant\": \"Compare the two percentages: \\\"25-34\\\" age group has 9% and \\\"50-64\\\" age group has 18%. Since 9% is less than 18%, the answer is yes.\\nAnswer: Yes\" }, { \"user\": \"Among the age groups \\\"35-49\\\" and \\\"25-34\\\", which one has a higher percentage of Maori language fluency?\", \"assistant\": \"Compare the percentages of \\\"35-49\\\" (12%) and \\\"25-34\\\" (9%).",
    "id": "55a9c7b4-c1c6-4e51-8143-83ff9efd2310",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Since 9% is less than 18%, the answer is yes.\\nAnswer: Yes\" }, { \"user\": \"Among the age groups \\\"35-49\\\" and \\\"25-34\\\", which one has a higher percentage of Maori language fluency?\", \"assistant\": \"Compare the percentages of \\\"35-49\\\" (12%) and \\\"25-34\\\" (9%). \\\"35-49\\\" has a higher percentage.\\nAnswer: 35-49\" }, { \"user\": \"What is the average percentage of Maori language fluency for the age groups: \\\"18-24\\\", \\\"5-12\\\", and \\\"Under 5\\\"?\", \"assistant\": \"Calculate the average by adding the percentages (7% + 21% + 4%) and dividing by the number of groups (3): (7 + 21 + 4) / 3 = 32 / 3 = approximately 10.67%.\\nAnswer: 10.67%\" }, { \"user\": \"What percentage of Maori language fluency is accounted for by the age groups under 18 years old?\", \"assistant\": \"Add percentages for \\\"Under 5\\\" (4%), \\\"5-12\\\" (21%), and \\\"13-17\\\" (15%): 4% + 21% + 15% = 40%.\\nAnswer: 40%\" }, { \"user\": \"Is the fluency of the \\\"13-17\\\" age group greater than the combined fluency percentages of the \\\"18-24\\\" and \\\"Under 5\\\" age groups?\", \"assistant\": \"Compare the percentage of \\\"13-17\\\" (15%) with the combined percentages of \\\"18-24\\\" (7%) and \\\"Under 5\\\" (4%): 7% + 4% = 11%.",
    "id": "8179bc8a-923d-4031-9a1a-8b034aeec05e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Compare the percentage of \\\"13-17\\\" (15%) with the combined percentages of \\\"18-24\\\" (7%) and \\\"Under 5\\\" (4%): 7% + 4% = 11%. Since 15% is greater than 11%, the answer is yes.\\nAnswer: Yes\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 4, 4, 4, 4, 4, 2, 4, 2, 4 ] 2 [ 5, 5, 5, 5, 5, 5, 4, 4, 5 ] 4 [ 4, 4, 4, 5, 5, 4, 4, 4, 5 ] 4 [ { \"user\": \"Which activity has the most time spent in Week 4?\", \"assistant\": \"In Week 4, look at the highest point on the line plot to see which activity reached the highest value\\nAnswer: Reading at the Park\" }, { \"user\": \"What is the average time spent on Playing Catch in January?\", \"assistant\": \"Add the time spent on Playing Catch for all weeks in January (20 + 30 + 25 + 40) and divide by the number of weeks (4)\\nAnswer: 28.75 minutes\" }, { \"user\": \"Does the amount of time spent on Picnicking generally increase or decrease over 16 weeks?\", \"assistant\": \"Look at the trend line for Picnicking. It starts at 15 minutes in Week 1 and ends up at 95 minutes in Week 16, showing a continuous increase\\nAnswer: Increase\" }, { \"user\": \"Which activity shows the most variation in February?\", \"assistant\": \"Compare the lengths of the error bars for each activity in February.",
    "id": "fec77b8b-db6f-4708-a5ea-4c2541f9a389",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Look at the trend line for Picnicking. It starts at 15 minutes in Week 1 and ends up at 95 minutes in Week 16, showing a continuous increase\\nAnswer: Increase\" }, { \"user\": \"Which activity shows the most variation in February?\", \"assistant\": \"Compare the lengths of the error bars for each activity in February. Longer error bars indicate higher variation\\nAnswer: Playing Catch\" }, { \"user\": \"What can you infer about the error bars' length for Grandma Story Time?\", \"assistant\": \"Observe that the error bars for Grandma Story Time are consistent and not very long compared to other activities, suggesting little variation\\nAnswer: Low variation\" }, { \"user\": \"Between Hiking and Bird Watching, which had more time spent on average in March?\", \"assistant\": \"Calculate the average for both activities in March, for Hiking (50 + 55 + 60 + 65)/4 = 57.5 and for Bird Watching (45 + 50 + 55 + 60)/4 = 52.5, then compare\\nAnswer: Hiking\" }, { \"user\": \"How does the time spent on Reading at the Park in January compare to April?\", \"assistant\": \"Compare the values: In January, times are (30 + 35 + 40 + 50), and in April, times are (95 + 100 + 105 + 110). Clearly, the values are much higher in April\\nAnswer: Higher in April\" }, { \"user\": \"What do the cheerful annotations about spending more time outside suggest?\", \"assistant\": \"The annotation \\\"Yay! More time outside!\\\" points specifically at Week 8 with increased outdoor activity time, specifically highlighting a general trend\\nAnswer: Increased outdoor activity around Week 8\" }, { \"user\": \"What's the total time spent on all activities in Week 9?",
    "id": "204021d9-3af7-4ae0-958b-9b400efaf3c2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Clearly, the values are much higher in April\\nAnswer: Higher in April\" }, { \"user\": \"What do the cheerful annotations about spending more time outside suggest?\", \"assistant\": \"The annotation \\\"Yay! More time outside!\\\" points specifically at Week 8 with increased outdoor activity time, specifically highlighting a general trend\\nAnswer: Increased outdoor activity around Week 8\" }, { \"user\": \"What's the total time spent on all activities in Week 9?\", \"assistant\": \"Sum the times spent on each activity in Week 9: (80 + 50 + 45 + 60 + 60 + 60)\\nAnswer: 355 minutes\" }, { \"user\": \"By how much did time spent on Picnicking increase from Week 1 to Week 12?\", \"assistant\": \"Subtract the Week 1 value from the Week 12 value for Picnicking: (75 - 15)\\nAnswer: 60 minutes\" } ] CoSyn-400k-chart [ 5, 5, 5, 3, 5, 5, 5, 3, 5, 5 ] 3 [ 4, 2, 2, 4, 4, 4, 4, 3, 5, 5 ] 2 [ 4, 3, 4, 3, 4, 4, 4, 4, 4, 4 ] 3 [ 4, 4, 5, 4, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What's the trend in discipline referrals for African American students over the 5 years?\", \"assistant\": \"Analyze the chart to observe the line representing African American students. The number of referrals increased slightly overall from 200 in 2018 to 240 in 2022.\\nAnswer: Increased\" }, { \"user\": \"Which ethnic group had the highest number of discipline referrals in 2022?",
    "id": "78298efc-c37d-4d81-a511-9482509b4988",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Analyze the chart to observe the line representing African American students. The number of referrals increased slightly overall from 200 in 2018 to 240 in 2022.\\nAnswer: Increased\" }, { \"user\": \"Which ethnic group had the highest number of discipline referrals in 2022?\", \"assistant\": \"Observe the heights of the areas in the chart for the year 2022. The African American group has the highest area thickness, indicating the highest number of referrals (240).\\nAnswer: African American\" }, { \"user\": \"Compare the number of referrals for Hispanic and Asian students in 2020. Which group had more?\", \"assistant\": \"Observe the chart and locate the data points for Hispanic and Asian students in 2020. Hispanic students had 170 referrals, and Asian students had 60 referrals.\\nAnswer: Hispanic students\" }, { \"user\": \"Which ethnic group experienced the biggest increase in discipline referrals from 2019 to 2022?\", \"assistant\": \"Calculate the difference in the number of referrals between 2019 and 2022 for each ethnic group. For African American (240-220 = 20), Hispanic (150-180 = -30), White (130-110 = 20), Asian (65-55 = 10), and Other (100-80 = 20). The African American, White, and Other groups all increased by 20 referrals.\\nAnswer: African American, White, and Other (all increased by 20)\" }, { \"user\": \"What's the total number of discipline referrals recorded in 2018 across all ethnic groups?\", \"assistant\": \"Sum the referrals for all ethnic groups in 2018. (200 + 150 + 100 + 50 + 75 = 575).\\nAnswer: 575\" }, { \"user\": \"Which ethnic group showed the most variability in discipline referrals over the 5 years?\", \"assistant\": \"Observe the fluctuations in the height of the areas across the years.",
    "id": "f8b59f10-5d9e-4e44-94f1-6d4651b9199b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Sum the referrals for all ethnic groups in 2018. (200 + 150 + 100 + 50 + 75 = 575).\\nAnswer: 575\" }, { \"user\": \"Which ethnic group showed the most variability in discipline referrals over the 5 years?\", \"assistant\": \"Observe the fluctuations in the height of the areas across the years. African American and Other groups show noticeable variability, whereas other groups have more subtle changes.\\nAnswer: African American\" }, { \"user\": \"What was the average annual number of referrals for White students from 2018 to 2022?\", \"assistant\": \"Sum the number of referrals for White students across the 5 years and divide by the number of years. (100 + 110 + 105 + 120 + 130) / 5 = 565 / 5 = 113.\\nAnswer: 113\" }, { \"user\": \"Compare the areas representing Hispanic and White students in 2020. Which group had more referrals and by how much?\", \"assistant\": \"Observe the heights of the areas for the year 2020. Hispanic students had 170 referrals, and White students had 105 referrals. The difference is 170 - 105 = 65.\\nAnswer: Hispanic students by 65\" }, { \"user\": \"What's the combined total of discipline referrals for Asian and Other students in 2021?\", \"assistant\": \"Sum the number of referrals for Asian and Other students in 2021. (70 + 95 = 165)\\nAnswer: 165\" }, { \"user\": \"What percentage of the total referrals in 2022 were from the White student group?\", \"assistant\": \"First, sum all the referrals for 2022. (240 + 150 + 130 + 65 + 100 = 685). Then, calculate the percentage that came from the White student group.",
    "id": "876473d8-a2a4-4d3f-90c6-f2758142b21b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "(70 + 95 = 165)\\nAnswer: 165\" }, { \"user\": \"What percentage of the total referrals in 2022 were from the White student group?\", \"assistant\": \"First, sum all the referrals for 2022. (240 + 150 + 130 + 65 + 100 = 685). Then, calculate the percentage that came from the White student group. (130 / 685)  100 = ~18.98%.\\nAnswer: ~18.98%\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 4, 5, 5, 5, 5 ] 4 [ 4, 2, 4, 4, 4, 4, 5, 4, 4, 4 ] 2 [ 5, 5, 5, 5, 5, 5, 5, 5, 4, 4 ] 4 [ 4, 4, 4, 4, 4, 4, 4, 4, 4, 5 ] 4 [ { \"user\": \"Which honey type has the highest mean value on the x-axis?\", \"assistant\": \"The mean value is calculated as the average of the Field Refractometer and Laboratory Analysis readings. On the Bland-Altman plot, we can see that the Chestnut honey type is located at the far right, indicating that it has the highest mean value.\\nAnswer: Chestnut\" }, { \"user\": \"What is the title of the plot?\", \"assistant\": \"The title of the plot is displayed at the top and it reads 'Bland-Altman Plot: Field Refractometer vs Laboratory Analysis'.\\nAnswer: Bland-Altman Plot: Field Refractometer vs Laboratory Analysis\" }, { \"user\": \"How many data points are there on the plot?",
    "id": "62d52035-9274-4b35-91f8-198dd5a07152",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The title of the plot is displayed at the top and it reads 'Bland-Altman Plot: Field Refractometer vs Laboratory Analysis'.\\nAnswer: Bland-Altman Plot: Field Refractometer vs Laboratory Analysis\" }, { \"user\": \"How many data points are there on the plot?\", \"assistant\": \"The plot includes one marker for each honey type, and there are 15 honey types listed in the data, so there should be 15 data points.\\nAnswer: 15\" }, { \"user\": \"What does the dashed line represent?\", \"assistant\": \"The dashed line in the Bland-Altman plot represents the mean difference between the Field Refractometer and Laboratory Analysis readings, shown in a consistent color and located centrally among the data points.\\nAnswer: Mean difference\" }, { \"user\": \"Which honey type has the largest positive difference on the y-axis?\", \"assistant\": \"The difference is calculated as the Field Refractometer reading minus the Laboratory Analysis reading. By looking at the highest point on the y-axis, we identify Clover having the largest positive difference because it is positioned highest among the data points.\\nAnswer: Clover\" }, { \"user\": \"What are the values of the limits of agreement?\", \"assistant\": \"The values of the limits of agreement are annotated on the plot with +1.96 SD and -1.96 SD labels. By locating these annotations, we can see that the values are approximately 1.18 and -0.82, respectively.\\nAnswer: +1.96 SD: 1.18, -1.96 SD: -0.82\" }, { \"user\": \"Which honey type is closest to the mean difference line?\", \"assistant\": \"The mean difference line is the dashed line, and the data point that is closest to this line is the one with a very small difference value.",
    "id": "ff9e964f-2e89-4a71-9be3-d8ae53a914f6",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The mean difference line is the dashed line, and the data point that is closest to this line is the one with a very small difference value. By inspection, Alfalfa is the closest to the mean difference line.\\nAnswer: Alfalfa\" }, { \"user\": \"How does the spread of differences change as the mean increases?\", \"assistant\": \"By observing the spread of points in the plot, we can determine that there appears to be a consistent spread of differences across all mean values, without a visible trend of increasing or decreasing differences with increasing mean values.\\nAnswer: Consistent spread\" }, { \"user\": \"Which honey type has the smallest negative difference on the y-axis?\", \"assistant\": \"To find the smallest negative difference, we look for the point closest to the dashed mean line but still under it. Acacia is slightly below the mean difference line, representing the smallest negative difference.\\nAnswer: Acacia\" }, { \"user\": \"What is the mean difference value?\", \"assistant\": \"The mean difference value is annotated on the plot's dashed line, which we can read directly as approximately 0.18.\\nAnswer: 0.18\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 4, 5, 5 ] 4 [ 4, 5, 5, 5, 4, 4, 4, 2, 4, 1 ] 1 [ 5, 5, 4, 4, 5, 5, 5, 4, 5, 5 ] 4 [ 4, 4, 4, 4, 4, 5, 5, 4, 4, 5 ] 4 [ { \"user\": \"What is the overall trend in the discovery of new lichen species from 1800 to 2022?",
    "id": "afcd239a-1c58-4a20-9e8f-fcebef8f693d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The graph shows an overall increasing trend, with a noticeable exponential growth in the number of new species discovered, especially from the 1900s onwards.\\nAnswer: Increasing trend\" }, { \"user\": \"During which period did the discovery of new lichen species start to significantly rise?\", \"assistant\": \"Observing the graph, the period around the early 1900s marks the beginning of a significant rise in the discovery of new lichen species, indicated by a steeper slope on the plot.\\nAnswer: Early 1900s\" }, { \"user\": \"How many more new lichen species were discovered in 2022 compared to 1900?\", \"assistant\": \"In 1900, there were 35 new species discovered and in 2022, 201 new species. The difference is calculated as 201 - 35 = 166.\\nAnswer: 166\" }, { \"user\": \"What is the average number of new lichen species discovered per year between 2000 and 2022?\", \"assistant\": \"The number of new species discovered in 2000 is 138 and 201 in 2022. There are 23 years between 2000 and 2022, inclusive. The average per year is (201 - 138) / 23 = 63 / 23 ‚âà 2.74.\\nAnswer: 2.74\" }, { \"user\": \"During which decade did the discovery of new lichen species exceed 100 for the first time?\", \"assistant\": \"The graph shows that the number of new species discovered exceeded 100 in 1980.\\nAnswer: 1980s\" }, { \"user\": \"Which year saw the highest number of new lichen species discoveries?\", \"assistant\": \"According to the graph, 2022 recorded the highest number of new lichen species discoveries, with 201 discoveries.\\nAnswer: 2022\" }, { \"user\": \"Compare the rate of discovery between the periods 1800-1900 and 1900-2000.",
    "id": "f1ff7978-dba5-4d76-8b80-a351b53dc163",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"According to the graph, 2022 recorded the highest number of new lichen species discoveries, with 201 discoveries.\\nAnswer: 2022\" }, { \"user\": \"Compare the rate of discovery between the periods 1800-1900 and 1900-2000. Which period shows a higher rate?\", \"assistant\": \"From 1800 to 1900, the discoveries increased from 5 to 35 (30 discoveries in 100 years). From 1900 to 2000, discoveries increased from 35 to 138 (103 discoveries in 100 years). The rate of discovery is higher in the 1900-2000 period.\\nAnswer: 1900-2000\" }, { \"user\": \"How does the number of new species discovered in 2010 compare to that in 2000?\", \"assistant\": \"In 2000, there were 138 new species discovered, and in 2010, there were 156. The increase is 156 - 138 = 18.\\nAnswer: Increase by 18\" }, { \"user\": \"What is the increase in the number of new lichen species discoveries from 1980 to 2020?\", \"assistant\": \"In 1980, 105 new species were discovered, and in 2020, the number was 189. The increase is 189 - 105 = 84.\\nAnswer: 84\" }, { \"user\": \"What can you infer about the future trend of lichen species discovery based on the graph?",
    "id": "45a9faa8-0fd9-4cc2-af64-09cba8730f25",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"In 1980, 105 new species were discovered, and in 2020, the number was 189. The increase is 189 - 105 = 84.\\nAnswer: 84\" }, { \"user\": \"What can you infer about the future trend of lichen species discovery based on the graph?\", \"assistant\": \"Based on the exponential growth trend shown in the graph, it can be inferred that the discovery of new lichen species may continue to increase at a rapid pace in the future.\\nAnswer: Continued increase\" } ] CoSyn-400k-chart [ 5, 5, 5, 1, 5, 5, 5, 5, 5, 4 ] 1 [ 2, 2, 2, 2, 2, 2, 2, 2, 2, 5 ] 2 [ 5, 5, 5, 4, 4, 5, 5, 5, 5, 4 ] 4 [ 4, 5, 5, 4, 4, 4, 5, 4, 5, 4 ] 4 [ { \"user\": \"What festival has the highest frequency of celebration?\", \"assistant\": \"Scan the plot for the data point with the largest radial distance, which corresponds to the highest frequency. The label should reveal the festival with the highest frequency.\\nAnswer: New Moon Ritual\" }, { \"user\": \"Which season sees the highest frequency of celebrations?\", \"assistant\": \"Identify the data points clustered within each seasonal segment on the polar plot. Sum the frequencies of the festivals within each season and compare.\\nAnswer: Winter\" }, { \"user\": \"How many festivals have a frequency of 8 celebrations?\", \"assistant\": \"Count the number of data points that have their radial distance indicating a frequency of 8. The hover names or labels can confirm the festivals.\\nAnswer: Two\" }, { \"user\": \"Which season hosts the Dragon Feast?",
    "id": "5b5730d9-45bb-4982-b196-2467b06dde9d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Sum the frequencies of the festivals within each season and compare.\\nAnswer: Winter\" }, { \"user\": \"How many festivals have a frequency of 8 celebrations?\", \"assistant\": \"Count the number of data points that have their radial distance indicating a frequency of 8. The hover names or labels can confirm the festivals.\\nAnswer: Two\" }, { \"user\": \"Which season hosts the Dragon Feast?\", \"assistant\": \"Locate the data point labeled \\\"Dragon Feast\\\" and determine which seasonal segment (Winter, Spring, Summer, Autumn) it falls within based on its angular position.\\nAnswer: Autumn\" }, { \"user\": \"What is the average frequency of the festivals in Summer?\", \"assistant\": \"Identify the festivals in the \\\"Summer\\\" section (Midsummer Festival) and calculate the average of their frequencies.\\nAnswer: 6\" }, { \"user\": \"Which festival is celebrated closest to the Spring Equinox?\", \"assistant\": \"Check the angular proximity of the data points to the \\\"Spring Equinox\\\" label and find the closest one.\\nAnswer: Knight's Joust\" }, { \"user\": \"Which festival has the lowest frequency of celebration?\", \"assistant\": \"Scan the plot for the data point with the smallest radial distance, which corresponds to the lowest frequency. The label should indicate the festival.\\nAnswer: Midsummer Festival\" }, { \"user\": \"What is the difference in frequency between the Winter Solstice and the Harvest Festival?\", \"assistant\": \"Identify and compare the frequencies of the \\\"Winter Solstice\\\" and \\\"Harvest Festival\\\" points from the plot.",
    "id": "f86b55b8-f3e8-4401-a21c-942688d27061",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Scan the plot for the data point with the smallest radial distance, which corresponds to the lowest frequency. The label should indicate the festival.\\nAnswer: Midsummer Festival\" }, { \"user\": \"What is the difference in frequency between the Winter Solstice and the Harvest Festival?\", \"assistant\": \"Identify and compare the frequencies of the \\\"Winter Solstice\\\" and \\\"Harvest Festival\\\" points from the plot. Calculate the difference.\\nAnswer: 1\" } ] CoSyn-400k-chart [ 3, 5, 5, 5, 5, 3, 5, 1 ] 1 [ 4, 2, 4, 3, 2, 2, 2, 2 ] 2 [ 5, 4, 4, 1, 4, 4, 5, 4 ] 1 [ 4, 4, 4, 4, 4, 4, 4, 5 ] 4 [ { \"user\": \"What is the title of the figure?\", \"assistant\": \"The title of the figure is located at the top center and provides an overview of what the figure represents. The title reads \\\"Age and Experience Distribution of Kaggle Participants.\\\"\\nAnswer: Age and Experience Distribution of Kaggle Participants\" }, { \"user\": \"Which experience level has the highest number of participants in the 25-34 age group?\", \"assistant\": \"By analyzing the bars representing the 25-34 age group, both male and female, we observe that the Intermediate level has the highest number of participants compared to Beginner and Expert levels.\\nAnswer: Intermediate\" }, { \"user\": \"For the 35-44 age group, how many more male Intermediate participants are there compared to male Beginners?\", \"assistant\": \"The figure shows that in the 35-44 age group, there are 20 male Intermediate participants and 12 male Beginner participants.",
    "id": "c50bbfe8-ad6a-460e-a4f5-ef2ba8e04e52",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The figure shows that in the 35-44 age group, there are 20 male Intermediate participants and 12 male Beginner participants. The difference is 20 - 12 = 8 participants.\\nAnswer: 8\" }, { \"user\": \"What is the total number of female participants in the 45-54 age group?\", \"assistant\": \"Summing up the values for females in the 45-54 age group: Beginners (5), Intermediate (9), and Expert (6), we get 5 + 9 + 6 = 20 participants.\\nAnswer: 20\" }, { \"user\": \"How does the number of male Expert participants differ between the 25-34 and 35-44 age groups?\", \"assistant\": \"The figure shows 9 male Expert participants in the 25-34 age group and 15 male Expert participants in the 35-44 age group. The difference is 15 - 9 = 6 participants.\\nAnswer: 6\" }, { \"user\": \"Which age group has the least number of female participants overall?\", \"assistant\": \"By observing the lengths of the bars corresponding to female participants across all age groups, the 55+ age group has the least number of female participants: Beginners (2), Intermediate (4), and Expert (3).\\nAnswer: 55+\" }, { \"user\": \"In the 18-24 age group, how does the number of female Beginners compare to female Intermediate participants?\", \"assistant\": \"The figure shows 12 female Beginners and 6 female Intermediate participants in the 18-24 age group. 12 is double the number of 6, indicating Beginners are twice as many.\\nAnswer: Beginners are double\" }, { \"user\": \"What is the average number of male participants for the Intermediate experience level across all age groups?",
    "id": "63afa3e2-d366-47bb-adb5-6feb1fafc407",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The figure shows 12 female Beginners and 6 female Intermediate participants in the 18-24 age group. 12 is double the number of 6, indicating Beginners are twice as many.\\nAnswer: Beginners are double\" }, { \"user\": \"What is the average number of male participants for the Intermediate experience level across all age groups?\", \"assistant\": \"Summing the number of male Intermediate participants across age groups: 8 (18-24), 22 (25-34), 20 (35-44), and 11 (45-54), and 5 (55+), we get 8 + 22 + 20 + 11 + 5 = 66. Dividing by the number of age groups (5), 66 / 5 = 13.2.\\nAnswer: 13.2\" }, { \"user\": \"Which experience level has the most participants in the 55+ age group?\", \"assistant\": \"By examining the bars for the 55+ age group, the Intermediate level has the most participants compared to Beginner and Expert levels for both males and females.\\nAnswer: Intermediate\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 4, 4, 5, 4, 4, 5, 4, 3 ] 3 [ 5, 5, 4, 4, 4, 5, 5, 4, 5 ] 4 [ 4, 4, 4, 4, 5, 5, 4, 4, 4 ] 4 [ { \"user\": \"What's the neighborhood with the highest average home price in 2022?",
    "id": "045e98f3-7bba-4501-b439-3f9159c31649",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Look for the line ending at the highest point in 2022 on the vertical scale.\\nAnswer: Hillcrest\" }, { \"user\": \"What's the total increase in average home price from 2018 to 2022 for Sunnyside?\", \"assistant\": \"Calculate the difference between the home prices in 2022 and 2018 for Sunnyside: 462000 - 355000 = 107000\\nAnswer: 107000\" }, { \"user\": \"Which neighborhood experienced the largest percentage increase in average home price from 2018 to 2022?\", \"assistant\": \"Calculate the percentage increase for each neighborhood using the formula: ((Price_2022 - Price_2018) / Price_2018)  100. Compare the results.\\nAnswer: Hillcrest\" }, { \"user\": \"How does the average home price trend in Downtown compare to Riverside between 2020 and 2022?\", \"assistant\": \"Examine the slope of the lines for Downtown and Riverside from 2020 to 2022. Downtown rises from 358000 to 412000; Riverside from 310000 to 372000. Both increased, but Downtown more steeply.\\nAnswer: Downtown increased more steeply\" }, { \"user\": \"Considering all neighborhoods, which one had the smallest change in average home price from 2018 to 2022?\", \"assistant\": \"Calculate the change for each neighborhood and compare them. Westend's change is 342000 - 265000 = 77000, this is the smallest.\\nAnswer: Westend\" }, { \"user\": \"Which two neighborhoods had average home prices closest to each other in 2022?\", \"assistant\": \"Compare the 2022 prices for all neighborhoods and find the smallest difference: Downtown (412000) and Lakeside (488000) have a difference of 76000.\\nAnswer: Downtown and Lakeside\" }, { \"user\": \"What was the average home price in all neighborhoods in 2018?",
    "id": "248e69e4-c3ff-49e1-91b6-b41003816446",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Compare the 2022 prices for all neighborhoods and find the smallest difference: Downtown (412000) and Lakeside (488000) have a difference of 76000.\\nAnswer: Downtown and Lakeside\" }, { \"user\": \"What was the average home price in all neighborhoods in 2018?\", \"assistant\": \"Add all prices for 2018 and divide by the number of neighborhoods: (325000 + 280000 + 410000 + 298000 + 355000 + 265000 + 388000 + 302000 + 442000 + 375000) / 10 = 3440000 / 10\\nAnswer: 344000\" }, { \"user\": \"Between Oak Hill and Maple Grove, which neighborhood had a higher average home price consistently throughout the years 2018-2022?\", \"assistant\": \"Compare the yearly prices for both neighborhoods and observe if any year Oak Hill's price is not greater than Maple Grove's. Oak Hill consistently has a higher price.\\nAnswer: Oak Hill\" } ] CoSyn-400k-chart [ 3, 5, 5, 5, 5, 5, 5, 5 ] 3 [ 2, 5, 4, 3, 4, 4, 2, 4 ] 2 [ 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 5, 4, 4, 5, 4, 4, 4, 5 ] 4 [ { \"user\": \"What type of fiber has the highest water usage per kilogram?\", \"assistant\": \"The figure shows water usage for different types of fibers. Cotton, labeled as natural fiber, uses 10,000 liters per kilogram, which is higher than both synthetic and recycled fibers.\\nAnswer: Cotton\" }, { \"user\": \"Which fiber type uses the least water per kilogram?",
    "id": "f4fdf67b-d37f-4fd9-a105-4be0c771fdcc",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The figure shows water usage for different types of fibers. Cotton, labeled as natural fiber, uses 10,000 liters per kilogram, which is higher than both synthetic and recycled fibers.\\nAnswer: Cotton\" }, { \"user\": \"Which fiber type uses the least water per kilogram?\", \"assistant\": \"Evaluating the three fiber types, Polyester (Synthetic) uses 265 L/kg, Cotton (Natural) uses 10,000 L/kg, and Recycled PET uses 85 L/kg. Recycled PET has the lowest water usage.\\nAnswer: Recycled PET\" }, { \"user\": \"What is the difference in water usage between the fiber with the highest and lowest water usage?\", \"assistant\": \"The highest water use is by Cotton at 10,000 L/kg and the lowest is by Recycled PET at 85 L/kg. The difference is 10,000 - 85 = 9,915 L/kg.\\nAnswer: 9,915 L/kg\" }, { \"user\": \"How much more water does Cotton use compared to Polyester?\", \"assistant\": \"Water usage for Cotton is 10,000 L/kg and for Polyester is 265 L/kg. The difference is 10,000 - 265 = 9,735 L/kg.\\nAnswer: 9,735 L/kg\" }, { \"user\": \"What is the percentage of water usage by Polyester compared to the total water usage by all three fiber types?\", \"assistant\": \"The total water usage is 10,000 (Cotton) + 265 (Polyester) + 85 (Recycled PET) = 10,350 L/kg. The percentage for Polyester is (265 / 10,350)  100% = 2.56%.\\nAnswer: 2.56%\" }, { \"user\": \"Which fiber types are represented by red and blue colors in the pie chart?",
    "id": "12f7cbb1-da75-43a8-a109-14eeea4f6fa9",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The total water usage is 10,000 (Cotton) + 265 (Polyester) + 85 (Recycled PET) = 10,350 L/kg. The percentage for Polyester is (265 / 10,350)  100% = 2.56%.\\nAnswer: 2.56%\" }, { \"user\": \"Which fiber types are represented by red and blue colors in the pie chart?\", \"assistant\": \"Cotton is represented by the red color and Polyester by the blue color in the pie chart, as indicated by the visuals.\\nAnswer: Cotton and Polyester\" }, { \"user\": \"What is the total water usage for all three fiber types combined?\", \"assistant\": \"Summing the water usage: Cotton (10,000 L/kg) + Polyester (265 L/kg) + Recycled PET (85 L/kg) = 10,350 L/kg.\\nAnswer: 10,350 L/kg\" }, { \"user\": \"By how many liters does Recycled PET exceed Polyester in terms of lower water usage?\", \"assistant\": \"Polyester uses 265 L/kg while Recycled PET uses 85 L/kg. The difference in water usage is 265 - 85 = 180 L/kg, meaning Recycled PET uses 180 liters less.\\nAnswer: 180 L/kg\" }, { \"user\": \"What is the average water usage per kilogram across all three fiber types?\", \"assistant\": \"The total water usage for the three fiber types is 10,350 L/kg.",
    "id": "72f28e07-6988-4c89-a068-ab924e7d4620",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The difference in water usage is 265 - 85 = 180 L/kg, meaning Recycled PET uses 180 liters less.\\nAnswer: 180 L/kg\" }, { \"user\": \"What is the average water usage per kilogram across all three fiber types?\", \"assistant\": \"The total water usage for the three fiber types is 10,350 L/kg. The average is calculated by dividing by the number of types: 10,350 / 3 = 3,450 L/kg.\\nAnswer: 3,450 L/kg\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 2, 4, 2, 4, 4, 4, 2, 3 ] 2 [ 5, 5, 4, 4, 4, 5, 3, 3, 3 ] 3 [ 5, 4, 5, 4, 5, 4, 4, 4, 4 ] 4 [ { \"user\": \"What is the title of the chart?\", \"assistant\": \"The title is usually located at the top of the chart and provides a summary of what the chart depicts. In this case, it gives an overview of the demographics being compared.\\nAnswer: Age and Gender Demographics: Seasonal Flu vs Novel Respiratory Illness\" }, { \"user\": \"Which age group has the highest percentage of females affected by the seasonal flu?\", \"assistant\": \"Look at the female bars for the seasonal flu and identify the age group with the longest bar.\\nAnswer: 80+\" }, { \"user\": \"How does the percentage of males aged 30-39 affected by the novel respiratory illness compare to the percentage affected by the seasonal flu in the same age group?",
    "id": "f9ba7119-8b09-4ed8-ac8e-a7de7087e965",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Look at the female bars for the seasonal flu and identify the age group with the longest bar.\\nAnswer: 80+\" }, { \"user\": \"How does the percentage of males aged 30-39 affected by the novel respiratory illness compare to the percentage affected by the seasonal flu in the same age group?\", \"assistant\": \"Compare the lengths of the bars representing males aged 30-39 for both illnesses; the bars going left for the novel respiratory illness and right for the seasonal flu.\\nAnswer: Novel respiratory illness is higher\" }, { \"user\": \"In the 60-69 age group, what is the combined percentage of males and females affected by the novel respiratory illness?\", \"assistant\": \"Add the percentages of males and females for the novel respiratory illness in the 60-69 age group.\\nAnswer: 34.7\" }, { \"user\": \"Which illness has a higher percentage of males in the 70-79 age group?\", \"assistant\": \"Compare the lengths of the bars representing males in the 70-79 age group for both illnesses.\\nAnswer: Seasonal Flu\" }, { \"user\": \"Are females in the 50-59 age group more affected by the seasonal flu or the novel respiratory illness?\", \"assistant\": \"Compare the percentages (bar lengths) for females aged 50-59 for both illnesses.\\nAnswer: Novel Respiratory Illness\" }, { \"user\": \"Which gender has a higher percentage of individuals affected by the novel respiratory illness in the 20-29 age group?\", \"assistant\": \"Compare the bars for males and females aged 20-29 for the novel respiratory illness.\\nAnswer: Female\" }, { \"user\": \"What can you infer about the impact of the novel respiratory illness on older age groups (70+) compared to the seasonal flu?",
    "id": "a269c711-0c96-485e-adb2-102841a49ebb",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"Compare the bars for males and females aged 20-29 for the novel respiratory illness.\\nAnswer: Female\" }, { \"user\": \"What can you infer about the impact of the novel respiratory illness on older age groups (70+) compared to the seasonal flu?\", \"assistant\": \"Analyze the bars for the 70-79 age group and 80+ age group for both illnesses to see how the percentages differ.\\nAnswer: Novel respiratory illness impacts the 70-79 age group less but the 80+ age group more than seasonal flu\" } ] CoSyn-400k-chart [ 5, 5, 4, 5, 4, 4, 5, 5 ] 4 [ 5, 2, 2, 4, 4, 2, 4, 2 ] 2 [ 5, 5, 5, 5, 5, 5, 4, 5 ] 4 [ 4, 5, 4, 4, 4, 4, 4, 4 ] 4 [ { \"user\": \"What is the largest source of funding for the community garden project?\", \"assistant\": \"The pie chart visually represents that \\\"Local Government Grants\\\" has the largest slice among all the sources of funding.\\nAnswer: Local Government Grants\" }, { \"user\": \"How much more funding does \\\"Local Government Grants\\\" provide compared to \\\"Community Fundraisers\\\"?\", \"assistant\": \"\\\"Local Government Grants\\\" provides $15,000 while \\\"Community Fundraisers\\\" provides $5,000. Subtracting $5,000 from $15,000 gives $10,000.\\nAnswer: $10,000\" }, { \"user\": \"Which source provides the least amount of funding?\", \"assistant\": \"The smallest slice in the pie chart corresponds to \\\"Crowdfunding Platforms\\\" which funds $3,000.\\nAnswer: Crowdfunding Platforms\" }, { \"user\": \"How does the funding from \\\"State Agricultural Programs\\\" compare to \\\"Local Businesses\\\"?",
    "id": "46895258-1163-487c-b104-29018b24b013",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Subtracting $5,000 from $15,000 gives $10,000.\\nAnswer: $10,000\" }, { \"user\": \"Which source provides the least amount of funding?\", \"assistant\": \"The smallest slice in the pie chart corresponds to \\\"Crowdfunding Platforms\\\" which funds $3,000.\\nAnswer: Crowdfunding Platforms\" }, { \"user\": \"How does the funding from \\\"State Agricultural Programs\\\" compare to \\\"Local Businesses\\\"?\", \"assistant\": \"The pie chart slice representation reveals that \\\"State Agricultural Programs\\\" funds $12,000, which is more than the $6,000 funded by \\\"Local Businesses\\\".\\nAnswer: More\" }, { \"user\": \"What is the total amount of funding provided by \\\"Private Donations\\\" and \\\"Nonprofit Organizations\\\"?\", \"assistant\": \"\\\"Private Donations\\\" provides $7,000 and \\\"Nonprofit Organizations\\\" provides $8,000. Adding these amounts results in $15,000.\\nAnswer: $15,000\" }, { \"user\": \"Which funding sources contribute exactly $10,000 each?\", \"assistant\": \"The pie chart shows that \\\"Corporate Sponsorships\\\" is the only source contributing exactly $10,000.\\nAnswer: Corporate Sponsorships\" }, { \"user\": \"What is the percentage of funding provided by \\\"Local Government Grants\\\" out of the total funding?\", \"assistant\": \"The total funding is the sum of all sources: $15,000 + $5,000 + $7,000 + $10,000 + $8,000 + $3,000 + $12,000 + $6,000 = $66,000. The percentage is calculated as ($15,000 / $66,000)  100% ‚âà 22.7%.\\nAnswer: 22.7%\" }, { \"user\": \"Compare the combined funding from \\\"Community Fundraisers\\\" and \\\"Crowdfunding Platforms\\\" to \\\"Corporate Sponsorships\\\".",
    "id": "9af52874-ab5d-49b6-a468-14d6c6b6e633",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The percentage is calculated as ($15,000 / $66,000)  100% ‚âà 22.7%.\\nAnswer: 22.7%\" }, { \"user\": \"Compare the combined funding from \\\"Community Fundraisers\\\" and \\\"Crowdfunding Platforms\\\" to \\\"Corporate Sponsorships\\\".\", \"assistant\": \"\\\"Community Fundraisers\\\" provides $5,000 and \\\"Crowdfunding Platforms\\\" provides $3,000. Combined, they provide $8,000. \\\"Corporate Sponsorships\\\" alone provide $10,000. Combined funding from the former is less than the latter.\\nAnswer: Less\" }, { \"user\": \"What is the difference in funding between \\\"Nonprofit Organizations\\\" and \\\"Local Businesses\\\"?\", \"assistant\": \"\\\"Nonprofit Organizations\\\" provide $8,000 while \\\"Local Businesses\\\" provide $6,000. The difference is $8,000 - $6,000 which equals $2,000.\\nAnswer: $2,000\" }, { \"user\": \"What is the average amount of funding contributed by all sources?\", \"assistant\": \"Sum all funding amounts to get $66,000 and divide by the number of sources (8). The average is $66,000 / 8 = $8,250.\\nAnswer: $8,250\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 4, 2, 2, 4, 2, 4, 4, 2, 2 ] 2 [ 5, 4, 5, 5, 3, 3, 5, 4, 5, 2 ] 2 [ 4, 5, 4, 4, 5, 4, 5, 5, 5, 5 ] 4 [ { \"user\": \"Which sector has the highest employment rate?",
    "id": "11b858e9-876a-4137-b0dd-cdfbd2c02479",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"By observing the heights of the bars, Manufacturing is the tallest bar, corresponding to the highest employment rate.\\nAnswer: Manufacturing\" }, { \"user\": \"Which sector has the lowest employment rate?\", \"assistant\": \"Looking at the heights of the bars, Transport and Logistics is the shortest, indicating it has the lowest employment rate.\\nAnswer: Transport and Logistics\" }, { \"user\": \"What is the combined employment rate for Agriculture and Retail?\", \"assistant\": \"The employment rates are 12.5% for Agriculture and 15.4% for Retail. Adding these together gives 12.5% + 15.4% = 27.9%.\\nAnswer: 27.9%\" }, { \"user\": \"How much greater is the employment rate in Manufacturing compared to Health Services?\", \"assistant\": \"The employment rate for Manufacturing is 23.8% and for Health Services is 8.9%. Subtracting these gives 23.8% - 8.9% = 14.9%.\\nAnswer: 14.9%\" }, { \"user\": \"Which sectors have employment rates over 10%?\", \"assistant\": \"Observing the chart, the sectors with employment rates over 10% are Agriculture, Manufacturing, Construction, Retail, and Professional Services.\\nAnswer: Agriculture, Manufacturing, Construction, Retail, Professional Services\" }, { \"user\": \"What is the difference in employment rates between Retail and Education?\", \"assistant\": \"The employment rate for Retail is 15.4% and for Education is 9.3%. Subtracting these gives 15.4% - 9.3% = 6.1%.\\nAnswer: 6.1%\" }, { \"user\": \"Is the employment rate in Professional Services higher or lower than in Agriculture?",
    "id": "c6ed6fa8-6254-4dd6-a6fd-dac04ebe6322",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", \"assistant\": \"The employment rate for Retail is 15.4% and for Education is 9.3%. Subtracting these gives 15.4% - 9.3% = 6.1%.\\nAnswer: 6.1%\" }, { \"user\": \"Is the employment rate in Professional Services higher or lower than in Agriculture?\", \"assistant\": \"Comparing the bar heights, the employment rate in Professional Services (12.1%) is slightly lower than in Agriculture (12.5%).\\nAnswer: Lower\" }, { \"user\": \"What is the average employment rate across all sectors?\", \"assistant\": \"Adding all employment rates: 12.5 + 23.8 + 10.7 + 15.4 + 8.9 + 9.3 + 12.1 + 7.2 = 99.9. There are 8 sectors, so dividing 99.9 by 8 gives an average of 12.49%.\\nAnswer: 12.49%\" }, { \"user\": \"How much greater is the employment rate in Professional Services than in Transport and Logistics?\", \"assistant\": \"The employment rate in Professional Services is 12.1% and in Transport and Logistics is 7.2%. Subtracting these gives 12.1% - 7.2% = 4.9%.\\nAnswer: 4.9%\" } ] CoSyn-400k-chart [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 2, 2, 2, 2, 2, 2, 2, 2, 4 ] 2 [ 5, 5, 5, 5, 5, 5, 5, 5, 5 ] 5 [ 4, 4, 5, 5, 4, 5, 4, 4, 5 ] 4 End of preview.",
    "id": "84a26f7b-0820-4549-bb5f-f3c460f264d8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Expand in Data Studio Previous 1 2 3 ... 1,169 Next Load the data Structure Categories Licensing Information Fine Vision FineVision is a massive collection of datasets with 17.3M images, 24.3M samples, 88.9M turns, and 9.5B answer tokens, designed for training state-of-the-art open Vision-Language-Models. More detail can be found in the blog post: https://huggingface.co/spaces/HuggingFaceM4/FineVision Load the data from datasets import load_dataset, get_dataset_config_names Get all subset names and load the first one available_subsets = get_dataset_config_names('HuggingFaceM4/FineVision') ds = load_dataset('HuggingFaceM4/FineVision', name=available_subsets[0], split='train', streaming=True) Inspect the first sample ds[0] Structure { 'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=387x194 at 0x7F8F0B308200>], 'texts': [{'user': 'Question: What is between the reticulum and the abomasum?\\nChoices:\\nA. Intestine\\nB. Omasum\\nC. Stomach\\nD. Rumen\\nAnswer with the letter.', 'assistant': 'Answer: B'}, {'user': 'Here is a diagram figure extracted from some Grade 1 - 6 science books.\\nPlease first describe the content of this figure in detail, including how the knowledge visually displayed in the diagram.\\nThen start with a section title \"related knowledge:\", briefly and concisely highlight the related domain knowledge and theories that underly this diagram. Note that you do not need to provide much detail. Simply cover the most important concepts.', 'assistant': \"The figure is a simple diagram of the four compartments of a ruminant animal's stomach, which are the rumen, reticulum, omasum, and abomasum.",
    "id": "52bc1cc5-4868-435c-afee-30f171490c75",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Note that you do not need to provide much detail. Simply cover the most important concepts.', 'assistant': \"The figure is a simple diagram of the four compartments of a ruminant animal's stomach, which are the rumen, reticulum, omasum, and abomasum. The diagram shows the relative size and position of each compartment within the stomach, with arrows indicating the direction of food flow from one compartment to the next. \\n\\nRelated Knowledge:\\n- Ruminant Digestion: Ruminants such as cows, sheep, and goats have a unique digestive system that allows them to break down fibrous plant material, like grass, that other animals cannot digest.\\n- Four Stomach Compartments: The rumen is the largest compartment and serves as a fermentation vat where microbes break down fibrous material. The reticulum traps foreign objects and also helps in fermentation. The omasum absorbs water and nutrients, and the abomasum is the true stomach where digestion occurs similarly to monogastric animals.\\n- Microbial Fermentation: The microbes in the rumen produce volatile fatty acids which are the primary energy source for ruminants. They also produce gases like methane, which are eructated (belched) out.\\n- Ruminant Nutrition: Ruminants rely on a high-fiber diet and have to consume large quantities of forage to meet their nutritional needs.\"}], 'source': 'original', 'image_correspondence_ratings': [4, 3], 'image_correspondence_min': 3, 'visual_dependency_ratings': [4, 5], 'visual_dependency_min': 4, 'formatting_ratings': [4, 4], 'formatting_min': 4, 'relevance_ratings': [5, 5], 'relevance_min': 5 } Categories Licensing Information Each of the publicly available sub-datasets present in FineVision are governed by specific licensing conditions. Therefore, when making use of them you must take into consideration each of the licenses governing each dataset.",
    "id": "a687a578-73f1-4114-a2e9-1763d7223a8a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Therefore, when making use of them you must take into consideration each of the licenses governing each dataset. To the extent we have any rights in the prompts, these are licensed under CC-BY-4.0. Downloads last month49,586 Use this dataset Size of the auto-converted Parquet files (First 5GB per split): 4.48 TBNumber of rows (First 5GB per split): 24,209,105Estimated number of rows: 24,242,174 Models trained or fine-tuned on HuggingFaceM4/FineVision waster76/Zod Updated 1 day ago System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: microsoft/VibeVoice-1.5B ¬∑ Hugging Face (https://huggingface.co/microsoft/VibeVoice-1.5B) === microsoft/VibeVoice-1.5B ¬∑ Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up microsoft / VibeVoice-1.5B like 1.56k Follow Microsoft 15.2k Text-to-Speech Transformers Safetensors English Chinese vibevoice text-generation Podcast arxiv: 2508.19205 arxiv: 2412.08635 License: mit Model card Files Files and versions xet Community 33 Train Deploy Use this model VibeVoice: A Frontier Open-Source Text-to-Speech Model Training Details Models Installation and Usage Responsible Usage Direct intended uses Out-of-scope uses Risks and limitations Recommendations Contact VibeVoice: A Frontier Open-Source Text-to-Speech Model VibeVoice is a novel framework designed for generating expressive, long-form, multi-speaker conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking. A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz.",
    "id": "1a4faa56-27b1-4bbf-b17b-9b57a544f2ce",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking. A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a next-token diffusion framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details. The model can synthesize speech up to 90 minutes long with up to 4 distinct speakers, surpassing the typical 1-2 speaker limits of many prior models. ‚û°Ô∏è Technical Report: VibeVoice Technical Report ‚û°Ô∏è Project Page: microsoft/VibeVoice ‚û°Ô∏è Code: microsoft/VibeVoice-Code Training Details Transformer-based Large Language Model (LLM) integrated with specialized acoustic and semantic tokenizers and a diffusion-based decoding head. LLM: Qwen2.5-1.5B for this release. Tokenizers: Acoustic Tokenizer: Based on a œÉ-VAE variant (proposed in LatentLM), with a mirror-symmetric encoder-decoder structure featuring 7 stages of modified Transformer blocks. Achieves 3200x downsampling from 24kHz input. Encoder/decoder components are ~340M parameters each. Semantic Tokenizer: Encoder mirrors the Acoustic Tokenizer's architecture (without VAE components). Trained with an ASR proxy task. Diffusion Head: Lightweight module (4 layers, ~123M parameters) conditioned on LLM hidden states. Predicts acoustic VAE features using a Denoising Diffusion Probabilistic Models (DDPM) process. Uses Classifier-Free Guidance (CFG) and DPM-Solver (and variants) during inference. Context Length: Trained with a curriculum increasing up to 65,536 tokens.",
    "id": "0f264572-46b9-42e1-9582-6369ab3e222d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Trained with an ASR proxy task. Diffusion Head: Lightweight module (4 layers, ~123M parameters) conditioned on LLM hidden states. Predicts acoustic VAE features using a Denoising Diffusion Probabilistic Models (DDPM) process. Uses Classifier-Free Guidance (CFG) and DPM-Solver (and variants) during inference. Context Length: Trained with a curriculum increasing up to 65,536 tokens. Training Stages: Tokenizer Pre-training: Acoustic and Semantic tokenizers are pre-trained separately. VibeVoice Training: Pre-trained tokenizers are frozen; only the LLM and diffusion head parameters are trained. A curriculum learning strategy is used for input sequence length (4k -> 16K -> 32K -> 64K). Text tokenizer not explicitly specified, but the LLM (Qwen2.5) typically uses its own. Audio is \"tokenized\" via the acoustic and semantic tokenizers. Models Model Context Length Generation Length Weight VibeVoice-0.5B-Streaming - - On the way VibeVoice-1.5B 64K ~90 min You are here. VibeVoice-Large 32K ~45 min HF link Installation and Usage Please refer to GitHub README Responsible Usage Direct intended uses The VibeVoice model is limited to research purpose use exploring highly realistic audio dialogue generation detailed in the tech report. Out-of-scope uses Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios: Voice impersonation without explicit, recorded consent ‚Äì cloning a real individual‚Äôs voice for satire, advertising, ransom, social‚Äëengineering, or authentication bypass. Disinformation or impersonation ‚Äì creating audio presented as genuine recordings of real people or events. Real‚Äëtime or low‚Äëlatency voice conversion ‚Äì telephone or video‚Äëconference ‚Äúlive deep‚Äëfake‚Äù applications.",
    "id": "f7c6d4c3-8a9a-417c-94f3-e1e05aa51550",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios: Voice impersonation without explicit, recorded consent ‚Äì cloning a real individual‚Äôs voice for satire, advertising, ransom, social‚Äëengineering, or authentication bypass. Disinformation or impersonation ‚Äì creating audio presented as genuine recordings of real people or events. Real‚Äëtime or low‚Äëlatency voice conversion ‚Äì telephone or video‚Äëconference ‚Äúlive deep‚Äëfake‚Äù applications. Unsupported language ‚Äì the model is trained only on English and Chinese data; outputs in other languages are unsupported and may be unintelligible or offensive. Generation of background ambience, Foley, or music ‚Äì VibeVoice is speech‚Äëonly and will not produce coherent non‚Äëspeech audio. Risks and limitations While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content. English and Chinese only: Transcripts in language other than English or Chinese may result in unexpected audio outputs. Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects. Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations. Recommendations We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only.",
    "id": "cbb5b891-6ffd-4e80-8608-070f4dbf9efc",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects. Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations. Recommendations We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly. To mitigate the risks of misuse, we have: Embedded an audible disclaimer (e.g. ‚ÄúThis segment was generated by AI‚Äù) automatically into every synthesized audio file. Added an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card. Logged inference requests (hashed) for abuse pattern detection and publishing aggregated statistics quarterly. Users are responsible for sourcing their datasets legally and ethically. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice. Users are reminded to be mindful of data privacy concerns. Contact This project was conducted by members of Microsoft Research. We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at VibeVoice@microsoft.com. If the team receives reports of undesired behavior or identifies issues independently, we will update this repository with appropriate mitigations. Downloads last month236,850 Safetensors Model size 2.7B params Tensor type BF16 ¬∑ Files info Inference Providers NEW Text-to-Speech This model isn't deployed by any Inference Provider.",
    "id": "b53d40cd-8624-4191-9d54-80e9863336f6",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at VibeVoice@microsoft.com. If the team receives reports of undesired behavior or identifies issues independently, we will update this repository with appropriate mitigations. Downloads last month236,850 Safetensors Model size 2.7B params Tensor type BF16 ¬∑ Files info Inference Providers NEW Text-to-Speech This model isn't deployed by any Inference Provider. üôã 35 Ask for provider support Model tree for microsoft/VibeVoice-1.5B Finetunes 3 models Quantizations 3 models Spaces using microsoft/VibeVoice-1.5B 30 üèÉ yasserrmd/VibeVoice üëÄ broadfield-dev/VibeVoice-demo-dev ‚≠ê ACloudCenter/Conference-Generator-VibeVoice ‚ö° umint/searchgpt ‚ö° umint/o4-mini ‚ö° umint/gemma-3-270m üëÅ Shubhvedi/Vibe-Voice-TTS üëÄ danhtran2mind/VibeVoice üëÄ SiddhJagani/Voice üëÄ pierreguillou/VibeVoice-demo üëÄ PunkTink/VibeVoice-mess ‚ö° umint/gpt-4.1-nano + 25 Spaces + 18 Spaces Collection including microsoft/VibeVoice-1.5B VibeVoice Collection Frontier Text-to-Speech Models https://microsoft.github.io/VibeVoice/ ‚Ä¢ 5 items ‚Ä¢ Updated 7 days ago ‚Ä¢ 108 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Hugging Face ‚Äì Pricing (https://huggingface.co/pricingspaces) === Hugging Face ‚Äì Pricing Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Pricing Leveling up AI collaboration and compute. Give your personal account or your organization the most advanced platform to build AI.",
    "id": "47f4bb1e-41da-416b-a391-b35de9dab044",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Give your personal account or your organization the most advanced platform to build AI. PRO PRO Account Boost your personal HF experience Subscribe for $9 per month Get PRO 10√ó private storage capacity 20√ó included inference credits 8√ó ZeroGPU quota and highest queue priority Spaces Dev Mode & ZeroGPU Spaces hosting Publish blog articles on your HF profile Dataset Viewer for private datasets Show your support with a Pro badge Team Instant setup for growing teams Subscribe for $20 per user per month Get Team (via credit card) SSO and SAML support Choose data location with Storage Regions Detailed action reviews with Audit Logs Granular access control via Resource Groups Repository usage Analytics Set auth policies and default repository visibility Centralized token control and approvals Dataset Viewer for private datasets Advanced compute options for Spaces All organization members get ZeroGPU and Inference Providers PRO benefits Enterprise Custom onboarding and enterprise features Starting at $50 per user per month Contact Sales All benefits from the Team plan Managed billing with annual commitments Legal and Compliance processes Personalized support Need support to adopt the HF Hub in your organization? View our Expert Support. Hugging Face Hub free The HF Hub is the central place to explore, experiment, collaborate and build technology with Machine Learning. Join the open source Machine Learning movement! ‚Üí Sign Up Create with ML Packed with ML features, like model eval, dataset viewer and much more. Collaborate Git based and designed for collaboration at its core. Play and learn Learn by experimenting and sharing with our awesome community. Build your ML portfolio Share your work with the world and build your own ML profile. Spaces Hardware Starting at $0 Spaces are one of the most popular ways to share ML applications and demos with the world.",
    "id": "a2b036dd-ecb2-41f2-b4d0-3643c56c35c2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "‚Üí Sign Up Create with ML Packed with ML features, like model eval, dataset viewer and much more. Collaborate Git based and designed for collaboration at its core. Play and learn Learn by experimenting and sharing with our awesome community. Build your ML portfolio Share your work with the world and build your own ML profile. Spaces Hardware Starting at $0 Spaces are one of the most popular ways to share ML applications and demos with the world. Upgrade your Spaces with our selection of custom on-demand hardware: ‚Üí Get started with Spaces Name CPU Memory Accelerator VRAM Hourly price CPU Basic 2 vCPU 16 GB - - FREE CPU Upgrade 8 vCPU 32 GB - - $0.03 Nvidia T4 - small 4 vCPU 15 GB Nvidia T4 16 GB $0.40 Nvidia T4 - medium 8 vCPU 30 GB Nvidia T4 16 GB $0.60 1x Nvidia L4 8 vCPU 30 GB Nvidia L4 24 GB $0.80 4x Nvidia L4 48 vCPU 186 GB Nvidia L4 96 GB $3.80 1x Nvidia L40S 8 vCPU 62 GB Nvidia L4 48 GB $1.80 4x Nvidia L40S 48 vCPU 382 GB Nvidia L4 192 GB $8.30 8x Nvidia L40S 192 vCPU 1534 GB Nvidia L4 384 GB $23.50 Nvidia A10G - small 4 vCPU 15 GB Nvidia A10G 24 GB $1.00 Nvidia A10G - large 12 vCPU 46 GB Nvidia A10G 24 GB $1.50 2x Nvidia A10G - large 24 vCPU 92 GB Nvidia A10G 48 GB $3.00 4x Nvidia A10G - large 48 vCPU 184 GB Nvidia A10G 96 GB $5.00 Nvidia A100 - large 12 vCPU 142 GB Nvidia A100 80 GB $2.50 Custom on demand on demand on demand on demand on demand ZeroGPU dynamic dynamic Nvidia H200 70 GB FREE Spaces Persistent Storage All Spaces get ephemeral storage for free but you can upgrade and add persistent storage at any time.",
    "id": "5adf918c-ac0a-4e9c-8e5e-7364c24e76d3",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Name Storage Monthly price Small 20 GB $5 Medium 150 GB $25 Large 1 TB $100 Building something cool as a side project? We also offer community GPU grants. Inference Endpoints Starting at $0.033/hour Inference Endpoints (dedicated) offers a secure production solution to easily deploy any ML model on dedicated and autoscaling infrastructure, right from the HF Hub. ‚ÜíLearn more CPU instances Provider Architecture vCPUs Memory Hourly rate aws Intel Sapphire Rapids 1 2GB $0.03 2 4GB $0.07 4 8GB $0.13 8 16GB $0.27 16 32GB $0.54 aws Intel Sapphire Rapids (overcommit) 16 32GB $0.01 azure Intel Xeon 1 2GB $0.06 2 4GB $0.12 4 8GB $0.24 8 16GB $0.48 gcp Intel Sapphire Rapids 1 2GB $0.05 2 4GB $0.10 4 8GB $0.20 8 16GB $0.40 Accelerator instances Provider Architecture Topology Accelerator Memory Hourly rate aws Inf2 Neuron x1 14.5GB $0.75 x12 760GB $12.00 gcp TPU v5e 1x1 16GB $1.20 2x2 64GB $4.75 2x4 128GB $9.50 GPU instances Provider Architecture GPUs GPU Memory Hourly rate aws NVIDIA T4 1 14GB $0.50 4 56GB $3.00 aws NVIDIA L4 1 24GB $0.80 4 96GB $3.80 aws NVIDIA L40S 1 48GB $1.80 4 192GB $8.30 8 384GB $23.50 aws NVIDIA A10G 1 24GB $1.00 4 96GB $5.",
    "id": "2e438e2b-849d-457c-a0ee-b98e00a9edab",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "50 4 56GB $3.00 aws NVIDIA L4 1 24GB $0.80 4 96GB $3.80 aws NVIDIA L40S 1 48GB $1.80 4 192GB $8.30 8 384GB $23.50 aws NVIDIA A10G 1 24GB $1.00 4 96GB $5.00 aws NVIDIA A100 1 80GB $2.50 2 160GB $5.00 4 320GB $10.00 8 640GB $20.00 aws NVIDIA H200 1 141GB $5.00 2 282GB $10.00 4 564GB $20.00 8 1128GB $40.00 gcp NVIDIA T4 1 16GB $0.50 gcp NVIDIA L4 1 24GB $0.70 4 96GB $3.80 gcp NVIDIA A100 1 80GB $3.60 2 160GB $7.20 4 320GB $14.40 8 640GB $28.80 gcp NVIDIA H100 1 80GB $10.00 2 160GB $20.00 4 320GB $40.00 8 640GB $80.00 PRO Account PRO A monthly subscription to access powerful features.",
    "id": "59e4ab5b-6a19-4e18-bb47-a19745a99ca2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "80 gcp NVIDIA A100 1 80GB $3.60 2 160GB $7.20 4 320GB $14.40 8 640GB $28.80 gcp NVIDIA H100 1 80GB $10.00 2 160GB $20.00 4 320GB $40.00 8 640GB $80.00 PRO Account PRO A monthly subscription to access powerful features. ‚Üí Get PRO ($9/month) Inference Providers: Get 20√ó included inference credits ZeroGPU: Get 8√ó usage quota and highest priority in queues Spaces Hosting: Create ZeroGPU Spaces with H200 hardware Spaces Dev Mode: Fast iterations via SSH/VS Code for Spaces Dataset Viewer: Activate and use it on private datasets Blog Articles: Publish articles on your HF profile Features Preview: Get early access to upcoming features PRO Badge: Show your support on your profile System theme Website Models Datasets Spaces Changelog Inference Endpoints HuggingChat Company About Brand assets Terms of service Privacy Jobs Press Resources Learn Documentation Blog Forum Service Status Social GitHub Twitter LinkedIn Discord === PAGE: openai/gpt-oss-120b ¬∑ Hugging Face (https://huggingface.co/openai/gpt-oss-120b) === openai/gpt-oss-120b ¬∑ Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up openai / gpt-oss-120b like 3.78k Follow OpenAI 20.3k Text Generation Transformers Safetensors gpt_oss vllm conversational 8-bit precision mxfp4 arxiv: 2508.10925 License: apache-2.0 Model card Files Files and versions xet Community 125 Train Deploy Use this model Highlights Inference examples Transformers vLLM PyTorch / Triton Ollama Download the model Reasoning levels Tool use Fine-tuning Citation Try gpt-oss ¬∑ Guides ¬∑ Model card ¬∑ OpenAI blog Welcome to the gpt-oss series, OpenAI‚Äôs open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.",
    "id": "da46c3b8-1add-4828-a7b9-ed0676476269",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "We‚Äôre releasing two flavors of these open models: gpt-oss-120b ‚Äî for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters) gpt-oss-20b ‚Äî for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters) Both models were trained on our harmony response format and should only be used with the harmony format as it will not work correctly otherwise. This model card is dedicated to the larger gpt-oss-120b model. Check out gpt-oss-20b for the smaller model. Highlights Permissive Apache 2.0 license: Build freely without copyleft restrictions or patent risk‚Äîideal for experimentation, customization, and commercial deployment. Configurable reasoning effort: Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs. Full chain-of-thought: Gain complete access to the model‚Äôs reasoning process, facilitating easier debugging and increased trust in outputs. It‚Äôs not intended to be shown to end users. Fine-tunable: Fully customize models to your specific use case through parameter fine-tuning. Agentic capabilities: Use the models‚Äô native capabilities for function calling, web browsing, Python code execution, and Structured Outputs. MXFP4 quantization: The models were post-trained with MXFP4 quantization of the MoE weights, making gpt-oss-120b run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the gpt-oss-20b model run within 16GB of memory. All evals were performed with the same MXFP4 quantization. Inference examples Transformers You can use gpt-oss-120b and gpt-oss-20b with Transformers. If you use the Transformers chat template, it will automatically apply the harmony response format.",
    "id": "4a91f7ee-4b11-4acf-9cf5-e15031a5cbfc",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "All evals were performed with the same MXFP4 quantization. Inference examples Transformers You can use gpt-oss-120b and gpt-oss-20b with Transformers. If you use the Transformers chat template, it will automatically apply the harmony response format. If you use model.generate directly, you need to apply the harmony format manually using the chat template or use our openai-harmony package. To get started, install the necessary dependencies to setup your environment: pip install -U transformers kernels torch Once, setup you can proceed to run the model by running the snippet below: from transformers import pipeline import torch model_id = \"openai/gpt-oss-120b\" pipe = pipeline( \"text-generation\", model=model_id, torch_dtype=\"auto\", device_map=\"auto\", ) messages = [ {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"}, ] outputs = pipe( messages, max_new_tokens=256, ) print(outputs[0][\"generated_text\"][-1]) Alternatively, you can run the model via Transformers Serve to spin up a OpenAI-compatible webserver: transformers serve transformers chat localhost:8000 --model-name-or-path openai/gpt-oss-120b Learn more about how to use gpt-oss with Transformers. vLLM vLLM recommends using uv for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server. uv pip install --pre vllm==0.10.1+gptoss \\ --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\ --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\ --index-strategy unsafe-best-match vllm serve openai/gpt-oss-120b Learn more about how to use gpt-oss with vLLM.",
    "id": "816a60f5-f424-485b-85d8-9ac3fa2cf77c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "uv pip install --pre vllm==0.10.1+gptoss \\ --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\ --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\ --index-strategy unsafe-best-match vllm serve openai/gpt-oss-120b Learn more about how to use gpt-oss with vLLM. PyTorch / Triton To learn about how to use this model with PyTorch and Triton, check out our reference implementations in the gpt-oss repository. Ollama If you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after installing Ollama. gpt-oss-120b ollama pull gpt-oss:120b ollama run gpt-oss:120b Learn more about how to use gpt-oss with Ollama. LM Studio If you are using LM Studio you can use the following commands to download. gpt-oss-120b lms get openai/gpt-oss-120b Check out our awesome list for a broader collection of gpt-oss resources and inference partners. Download the model You can download the model weights from the Hugging Face Hub directly from Hugging Face CLI: gpt-oss-120b huggingface-cli download openai/gpt-oss-120b --include \"original/\" --local-dir gpt-oss-120b/ pip install gpt-oss python -m gpt_oss.chat model/ Reasoning levels You can adjust the reasoning level that suits your task across three levels: Low: Fast responses for general dialogue. Medium: Balanced speed and detail. High: Deep and detailed analysis. The reasoning level can be set in the system prompts, e.g., \"Reasoning: high\".",
    "id": "afad57cb-f4ed-45f0-8167-9652986bf803",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Medium: Balanced speed and detail. High: Deep and detailed analysis. The reasoning level can be set in the system prompts, e.g., \"Reasoning: high\". Tool use The gpt-oss models are excellent for: Web browsing (using built-in browsing tools) Function calling with defined schemas Agentic operations like browser tasks Fine-tuning Both gpt-oss models can be fine-tuned for a variety of specialized use cases. This larger model gpt-oss-120b can be fine-tuned on a single H100 node, whereas the smaller gpt-oss-20b can even be fine-tuned on consumer hardware. Citation @misc{openai2025gptoss120bgptoss20bmodel, title={gpt-oss-120b & gpt-oss-20b Model Card}, author={OpenAI}, year={2025}, eprint={2508.10925}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2508.10925}, } Downloads last month3,001,497 Safetensors Model size 120B params Tensor type BF16 ¬∑U8 ¬∑ Chat template Files info Inference Providers NEW Together AI +5 Text Generation Examples Input a message to start chatting with openai/gpt-oss-120b.",
    "id": "073a578f-e785-4616-8e60-4ccd9d96e338",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Send View Code Snippets Open Playground Model tree for openai/gpt-oss-120b Adapters 20 models Finetunes 58 models Merges 2 models Quantizations 50 models Spaces using openai/gpt-oss-120b 100 üèÜ ginipick/FLUXllama üíª amd/gpt-oss-120b-chatbot üèÜ eduagarcia/open_pt_llm_leaderboard ‚ö° umint/ai üëÅ openfree/OpenAI-gpt-oss üåñ SustainabilityLabIITGN/VayuChat üìö abidlabs/openai-gpt-oss-120b-test üê¢ HPAI-BSC/TuRTLe-Leaderboard üëÅ VIDraft/gpt-oss-RAG üí¨ milwright/chatui-helper üåç fdaudens/gpt-oss-news-agent ‚ö° umint/searchgpt + 95 Spaces + 88 Spaces Collection including openai/gpt-oss-120b gpt-oss Collection Open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. ‚Ä¢ 2 items ‚Ä¢ Updated Aug 7 ‚Ä¢ 338 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Models compatible with the Transformers.js library ‚Äì Hugging Face (https://huggingface.co/models?library=transformers.js) === Models compatible with the Transformers.js library ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks Libraries 1 Languages Licenses Other Tasks Text Generation Any-to-Any Image-Text-to-Text Image-to-Text Image-to-Image Text-to-Image Text-to-Video Text-to-Speech + 42Parameters Reset Parameters < 1B 6B 12B 32B 128B > 500B < 1B > 500B Libraries Reset Libraries PyTorch google-tensorflow TensorFlow JAX Transformers Diffusers Safetensors ONNX GGUF Transformers.js MLX Keras + 41 Apps vLLM TGI llama.",
    "id": "6d46f8bc-0f8c-45b7-961d-f3fc65835378",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "js MLX Keras + 41 Apps vLLM TGI llama.cpp MLX LM LM Studio Ollama Jan + 13 Inference Providers Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale + 6 Apply filters Models 2,179 Full-text search Edit filters Sort: Trending Active filters: transformers.js Clear all onnx-community/FastVLM-0.5B-ONNX Image-Text-to-Text ‚Ä¢ Updated 6 days ago ‚Ä¢ 14.3k ‚Ä¢ 64 onnx-community/embeddinggemma-300m-ONNX Sentence Similarity ‚Ä¢ Updated 4 days ago ‚Ä¢ 4.51k ‚Ä¢ 15 nomic-ai/nomic-embed-text-v1.5 Sentence Similarity ‚Ä¢ 0.1B ‚Ä¢ Updated Jul 21 ‚Ä¢ 2.34M ‚Ä¢ 698 mixedbread-ai/mxbai-embed-large-v1 Feature Extraction ‚Ä¢ 0.3B ‚Ä¢ Updated Mar 13 ‚Ä¢ 2.26M ‚Ä¢ ‚Ä¢ 720 briaai/RMBG-2.0 Image Segmentation ‚Ä¢ 0.2B ‚Ä¢ Updated 7 days ago ‚Ä¢ 78.2k ‚Ä¢ 876 ZhengPeng7/BiRefNet Image Segmentation ‚Ä¢ 0.2B ‚Ä¢ Updated Mar 31 ‚Ä¢ 742k ‚Ä¢ 444 Alibaba-NLP/gte-modernbert-base Sentence Similarity ‚Ä¢ 0.1B ‚Ä¢ Updated Jul 4 ‚Ä¢ 135k ‚Ä¢ ‚Ä¢ 174 litert-community/gemma-3-270m-it Text Generation ‚Ä¢ Updated 18 days ago ‚Ä¢ 22 Xenova/bert-base-multilingual-cased-ner-hrl Token Classification ‚Ä¢ Updated Jul 11 ‚Ä¢ 694 ‚Ä¢ 4 jinaai/jina-embeddings-v2-base-code Feature Extraction ‚Ä¢ 0.2B ‚Ä¢ Updated Jan 6 ‚Ä¢ 111k ‚Ä¢ 120 briaai/RMBG-1.4 Image Segmentation ‚Ä¢ 0.0B ‚Ä¢ Updated Jul 6 ‚Ä¢ 349k ‚Ä¢ 1.",
    "id": "bc2c41d8-52ed-4276-8ef9-29fe4da7b3c8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2B ‚Ä¢ Updated Jan 6 ‚Ä¢ 111k ‚Ä¢ 120 briaai/RMBG-1.4 Image Segmentation ‚Ä¢ 0.0B ‚Ä¢ Updated Jul 6 ‚Ä¢ 349k ‚Ä¢ 1.84k HuggingFaceTB/SmolLM3-3B-Base Text Generation ‚Ä¢ 3B ‚Ä¢ Updated 25 days ago ‚Ä¢ 13.7k ‚Ä¢ 121 Xenova/distilbart-cnn-6-6 Summarization ‚Ä¢ Updated Jul 22 ‚Ä¢ 3.47k ‚Ä¢ 8 Xenova/distilbert-base-multilingual-cased-ner-hrl Token Classification ‚Ä¢ Updated Jul 11 ‚Ä¢ 880 ‚Ä¢ 2 Xenova/distilgpt2 Text Generation ‚Ä¢ Updated Jul 26 ‚Ä¢ 9.39k ‚Ä¢ 8 Xenova/nli-deberta-v3-base Zero-Shot Classification ‚Ä¢ Updated Jul 14 ‚Ä¢ 19 ‚Ä¢ 2 Xenova/whisper-large-v3 Automatic Speech Recognition ‚Ä¢ Updated Jun 26 ‚Ä¢ 1.33k ‚Ä¢ 10 distil-whisper/distil-small.en Automatic Speech Recognition ‚Ä¢ 0.2B ‚Ä¢ Updated Mar 25, 2024 ‚Ä¢ 15.7k ‚Ä¢ 110 Xenova/hubert-base-ls960 Feature Extraction ‚Ä¢ Updated Oct 8, 2024 ‚Ä¢ 6 ‚Ä¢ 1 AdamCodd/vit-base-nsfw-detector Image Classification ‚Ä¢ 0.1B ‚Ä¢ Updated Dec 3, 2024 ‚Ä¢ 583k ‚Ä¢ 62 Xenova/ms-marco-MiniLM-L-6-v2 Text Classification ‚Ä¢ Updated Jun 30 ‚Ä¢ 8.69k ‚Ä¢ 3 nomic-ai/nomic-embed-text-v1 Sentence Similarity ‚Ä¢ 0.1B ‚Ä¢ Updated Mar 31 ‚Ä¢ 588k ‚Ä¢ 538 mixedbread-ai/mxbai-rerank-xsmall-v1 Text Ranking ‚Ä¢ 0.1B ‚Ä¢ Updated Apr 2 ‚Ä¢ 769k ‚Ä¢ 46 Mozilla/distilvit Image-to-Text ‚Ä¢ 0.2B ‚Ä¢ Updated Nov 25,",
    "id": "61863ee4-9f6c-4ee0-b361-385ae44ef44a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "69k ‚Ä¢ 3 nomic-ai/nomic-embed-text-v1 Sentence Similarity ‚Ä¢ 0.1B ‚Ä¢ Updated Mar 31 ‚Ä¢ 588k ‚Ä¢ 538 mixedbread-ai/mxbai-rerank-xsmall-v1 Text Ranking ‚Ä¢ 0.1B ‚Ä¢ Updated Apr 2 ‚Ä¢ 769k ‚Ä¢ 46 Mozilla/distilvit Image-to-Text ‚Ä¢ 0.2B ‚Ä¢ Updated Nov 25, 2024 ‚Ä¢ 72 ‚Ä¢ 21 distil-whisper/distil-large-v3 Automatic Speech Recognition ‚Ä¢ 0.8B ‚Ä¢ Updated Mar 6 ‚Ä¢ 1.09M ‚Ä¢ 337 Snowflake/snowflake-arctic-embed-m Sentence Similarity ‚Ä¢ 0.1B ‚Ä¢ Updated Dec 13, 2024 ‚Ä¢ 386k ‚Ä¢ 157 Xenova/geoclip-large-patch14 Updated May 9, 2024 ‚Ä¢ 22 ‚Ä¢ 2 onnx-community/depth-anything-v2-small Depth Estimation ‚Ä¢ Updated Oct 8, 2024 ‚Ä¢ 2.84k ‚Ä¢ 20 jinaai/jina-reranker-v2-base-multilingual Text Classification ‚Ä¢ 0.3B ‚Ä¢ Updated Jan 6 ‚Ä¢ 1.23M ‚Ä¢ 305 onnx-community/whisper-base_timestamped Automatic Speech Recognition ‚Ä¢ Updated Mar 5 ‚Ä¢ 3.84k ‚Ä¢ 26 Previous 1 2 3 ... 73 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: openbmb/MiniCPM4.1-8B ¬∑ Hugging Face (https://huggingface.co/openbmb/MiniCPM4.1-8B) === openbmb/MiniCPM4.1-8B ¬∑ Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up openbmb / MiniCPM4.1-8B like 236 Follow OpenBMB 1.86k Text Generation Transformers Safetensors Chinese English minicpm conversational custom_code arxiv: 2506.",
    "id": "69b90570-2dc9-495d-9877-24fee66bb093",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "co/openbmb/MiniCPM4.1-8B) === openbmb/MiniCPM4.1-8B ¬∑ Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up openbmb / MiniCPM4.1-8B like 236 Follow OpenBMB 1.86k Text Generation Transformers Safetensors Chinese English minicpm conversational custom_code arxiv: 2506.07900 License: apache-2.0 Model card Files Files and versions xet Community Train Deploy Use this model What's New MiniCPM4 and MiniCPM4.1 Series Introduction Usage Inference with CPM.cu Hybird Reasoning Mode Inference with Transformers Inference with SGLang Speculative Decoding Standard Inference (Without Speculative Decoding) Inference with vLLM Speculative Decoding Standard Inference (Without Speculative Decoding) Comprehensive Evaluation Long Text Evaluation Evaluation Results Comprehensive Evaluation Long Text Evaluation Statement LICENSE Citation GitHub Repo | Technical Report | Join Us üëã Contact us in Discord and WeChat What's New [2025.09.05] MiniCPM4.1 series are released! This series is a hybrid reasoning model, which can be used in both deep reasoning mode and non-reasoning mode. üî•üî•üî• [2025.06.06] MiniCPM4 series are released! This model achieves ultimate efficiency improvements while maintaining optimal performance at the same scale! It can achieve over 5x generation acceleration on typical end-side chips! You can find technical report here.üî•üî•üî• MiniCPM4 and MiniCPM4.1 Series MiniCPM4 and MiniCPM4.1 series are highly efficient large language models (LLMs) designed explicitly for end-side devices, which achieves this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems.",
    "id": "57f39889-1346-46cb-8bda-ef04517e69a1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "It can achieve over 5x generation acceleration on typical end-side chips! You can find technical report here.üî•üî•üî• MiniCPM4 and MiniCPM4.1 Series MiniCPM4 and MiniCPM4.1 series are highly efficient large language models (LLMs) designed explicitly for end-side devices, which achieves this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. MiniCPM4.1-8B: The latest version of MiniCPM4, with 8B parameters, support fusion thinking. (<-- you are here) MiniCPM4.1-8B-GPTQ: MiniCPM4.1-8B in GPTQ format. MiniCPM4.1-8B-AutoAWQ: MiniCPM4.1-8B in AutoAWQ format. MiniCPM-4.1-8B-Marlin: MiniCPM4.1-8B in Marlin format. MiniCPM4.1-8B-GGUF: MiniCPM4.1-8B in GGUF format. MiniCPM4.1-8B-MLX: MiniCPM4.1-8B in MLX format. MiniCPM4.1-8B-Eagle3: Eagle3 model for MiniCPM4.1-8B.",
    "id": "c6ba89a2-114b-4e3c-8610-c7a4ef7ed0a4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "MiniCPM4.1-8B-GGUF: MiniCPM4.1-8B in GGUF format. MiniCPM4.1-8B-MLX: MiniCPM4.1-8B in MLX format. MiniCPM4.1-8B-Eagle3: Eagle3 model for MiniCPM4.1-8B. MiniCPM4 Series Click to expand all MiniCPM4 series models MiniCPM4-8B: The flagship model with 8B parameters, trained on 8T tokens MiniCPM4-0.5B: Lightweight version with 0.5B parameters, trained on 1T tokens MiniCPM4-8B-Eagle-FRSpec: Eagle head for FRSpec, accelerating speculative inference MiniCPM4-8B-Eagle-FRSpec-QAT-cpmcu: Eagle head with QAT for FRSpec, integrating speculation and quantization for ultra acceleration MiniCPM4-8B-Eagle-vLLM: Eagle head in vLLM format for speculative inference MiniCPM4-8B-marlin-Eagle-vLLM: Quantized Eagle head for vLLM format BitCPM4-0.5B: Extreme ternary quantization of MiniCPM4-0.5B, achieving 90% bit width reduction BitCPM4-1B: Extreme ternary quantization of MiniCPM3-1B, achieving 90% bit width reduction MiniCPM4-Survey: Generates trustworthy, long-form survey papers from user queries MiniCPM4-MCP: Integrates MCP tools to autonomously satisfy user requirements Introduction MiniCPM4 and MiniCPM4.1 are extremely efficient edge-side large model that has undergone efficient optimization across four dimensions: model architecture, learning algorithms, training data, and inference systems, achieving ultimate efficiency improvements.",
    "id": "4a027710-f020-409b-8adc-c0e074226ee1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "üèóÔ∏è Efficient Model Architecture: InfLLM v2 -- Trainable Sparse Attention Mechanism: Adopts a trainable sparse attention mechanism architecture where each token only needs to compute relevance with less than 5% of tokens in 128K long text processing, significantly reducing computational overhead for long texts üß† Efficient Learning Algorithms: Model Wind Tunnel 2.0 -- Efficient Predictable Scaling: Introduces scaling prediction methods for performance of downstream tasks, enabling more precise model training configuration search BitCPM -- Ultimate Ternary Quantization: Compresses model parameter bit-width to 3 values, achieving 90% extreme model bit-width reduction Efficient Training Engineering Optimization: Adopts FP8 low-precision computing technology combined with Multi-token Prediction training strategy üìö High-Quality Training Data: UltraClean -- High-quality Pre-training Data Filtering and Generation: Builds iterative data cleaning strategies based on efficient data verification, open-sourcing high-quality Chinese and English pre-training dataset UltraFinweb UltraChat v2 -- High-quality Supervised Fine-tuning Data Generation: Constructs large-scale high-quality supervised fine-tuning datasets covering multiple dimensions including knowledge-intensive data, reasoning-intensive data, instruction-following data, long text understanding data, and tool calling data ‚ö° Efficient Inference System: CPM.cu -- Lightweight and Efficient CUDA Inference Framework: Integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding ArkInfer -- Cross-platform Deployment System: Supports efficient deployment across multiple backend environments, providing flexible cross-platform adaptation capabilities Usage Inference with CPM.cu We recommend using CPM.cu for the inference of MiniCPM4 and MiniCPM4.1. CPM.cu is a CUDA inference framework developed by OpenBMB, which integrates efficient sparse, speculative sampling, and quantization techniques, fully leveraging the efficiency advantages of MiniCPM4 and MiniCPM4.1.",
    "id": "cc8ef743-85f2-4767-8590-b2e695adae62",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "CPM.cu is a CUDA inference framework developed by OpenBMB, which integrates efficient sparse, speculative sampling, and quantization techniques, fully leveraging the efficiency advantages of MiniCPM4 and MiniCPM4.1. You can install CPM.cu by running the following command: git clone https://github.com/OpenBMB/cpm.cu.git --recursive cd cpm.cu python3 setup.py install MiniCPM4.1 natively supports context lengths of up to 65,536(64k) tokens. To reproduce the long-text acceleration effect in the paper, we recommend using the LongRoPE factors that have been validated. Change the rope_scaling field in the config.json file as the following to enable LongRoPE. { ..., \"rope_scaling\": { \"rope_type\": \"longrope\", \"long_factor\": [0.9982316082870437, 1.033048153422584, 1.0749920956484724, 1.1255096879436193, 1.1863348602111476, 1.259543828902579, 1.3476188888731149, 1.4535223827776373, 1.5807816745852985, 1.7335856049489526, 1.9168922912975785, 2.1365471404135326, 2.3994084200118646, 2.713475511863602, 3.0880118452194134, 3.533650295140154, 4.062463396503134, 4.687974098908333, 5.425075306704039, 6.289818967956352, 7.29902962722721, 8.6357018163639, 10.210822723989212, 12.053807765671676, 14.193944598909404, 16.65780676784363, 19.463620727694074, 22.",
    "id": "e4ebc5ba-1d9e-4e08-82bb-b237c453162d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "062463396503134, 4.687974098908333, 5.425075306704039, 6.289818967956352, 7.29902962722721, 8.6357018163639, 10.210822723989212, 12.053807765671676, 14.193944598909404, 16.65780676784363, 19.463620727694074, 22.628311203524586, 26.150106147261315, 30.02526691405111, 34.23183327975347, 38.73811934094828, 43.502489489729555, 48.47627117965394, 53.61139491762471, 58.857366522037935, 64.16798299215064, 69.51359464319125, 74.86555458220285, 80.21497790341579, 85.55322183307433, 90.89611806932027, 96.26245306514224, 101.68269304046481, 107.18619510219668, 112.82253283014026, 118.63764063163615, 119.88866203644656, 120.9462882391725, 121.837565139014, 122.58663780572562, 123.2147719894291, 123.74049454862576, 124.17980424685767, 124.54641761955492, 124.85202548028222, 125.10654406389756, 125.31835105170659, 125.49450117164764, 125.64091910903052, 125.76256945356558, 125.86360463815589, 125.94749252260765, 126.01712561287873], \"short_factor\": [0.",
    "id": "34e433b7-3cb5-4e10-b45c-f800ecbfc2ba",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "124.54641761955492, 124.85202548028222, 125.10654406389756, 125.31835105170659, 125.49450117164764, 125.64091910903052, 125.76256945356558, 125.86360463815589, 125.94749252260765, 126.01712561287873], \"short_factor\": [0.9982316082870437, 1.033048153422584, 1.0749920956484724, 1.1255096879436193, 1.1863348602111476, 1.259543828902579, 1.3476188888731149, 1.4535223827776373, 1.5807816745852985, 1.7335856049489526, 1.9168922912975785, 2.1365471404135326, 2.3994084200118646, 2.713475511863602, 3.0880118452194134, 3.533650295140154, 4.062463396503134, 4.687974098908333, 5.425075306704039, 6.289818967956352, 7.29902962722721, 8.6357018163639, 10.210822723989212, 12.053807765671676, 14.193944598909404, 16.65780676784363, 19.463620727694074, 22.628311203524586, 26.150106147261315, 30.02526691405111, 34.23183327975347, 38.73811934094828, 43.502489489729555, 48.47627117965394, 53.61139491762471, 58.857366522037935, 64.16798299215064,",
    "id": "c060d7be-2187-42ae-8991-1d6bd1770f3f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "19.463620727694074, 22.628311203524586, 26.150106147261315, 30.02526691405111, 34.23183327975347, 38.73811934094828, 43.502489489729555, 48.47627117965394, 53.61139491762471, 58.857366522037935, 64.16798299215064, 69.51359464319125, 74.86555458220285, 80.21497790341579, 85.55322183307433, 90.89611806932027, 96.26245306514224, 101.68269304046481, 107.18619510219668, 112.82253283014026, 118.63764063163615, 119.88866203644656, 120.9462882391725, 121.837565139014, 122.58663780572562, 123.2147719894291, 123.74049454862576, 124.17980424685767, 124.54641761955492, 124.85202548028222, 125.10654406389756, 125.31835105170659, 125.49450117164764, 125.64091910903052, 125.76256945356558, 125.86360463815589, 125.94749252260765, 126.01712561287873], \"original_max_position_embeddings\": 65536 } } After modification, you can run the following command to reproduce the long-context acceleration effect (the script will automatically download the model weights from HuggingFace) python3 tests/test_generate.py You can run the following command to infer with EAGLE3 speculative decoding algorithm.",
    "id": "e7a0037f-7951-4fd4-b8f9-cbf8b960d7f4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "76256945356558, 125.86360463815589, 125.94749252260765, 126.01712561287873], \"original_max_position_embeddings\": 65536 } } After modification, you can run the following command to reproduce the long-context acceleration effect (the script will automatically download the model weights from HuggingFace) python3 tests/test_generate.py You can run the following command to infer with EAGLE3 speculative decoding algorithm. python3 -m cpmcu.cli \\ --model-path $BASE_MODEL_PATH \\ --draft-model-path $EAGLE3_DRAFT_MODEL_PATH \\ --prompt-text \"Write an article about Artificial Intelligence.\" \\ --use-eagle3 true For more details about CPM.cu, please refer to the repo CPM.cu. Hybird Reasoning Mode MiniCPM4.1 supports hybrid reasoning mode, which can be used in both deep reasoning mode and non-reasoning mode. To enable hybrid reasoning mode. User can set enable_thinking=True in tokenizer.apply_chat_template to enable hybrid reasoning mode, and set enable_thinking=False to enable non-reasoning mode. Similarly, user can directly add /no_think at the end of the query to enable non-reasoning mode. If not add any special token or add /think at the end of the query, the model will enable reasoning mode.",
    "id": "6e7dd012-9634-4842-8325-f57f939a7b87",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "To enable hybrid reasoning mode. User can set enable_thinking=True in tokenizer.apply_chat_template to enable hybrid reasoning mode, and set enable_thinking=False to enable non-reasoning mode. Similarly, user can directly add /no_think at the end of the query to enable non-reasoning mode. If not add any special token or add /think at the end of the query, the model will enable reasoning mode. Enable reasoning mode prompt_text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, enable_thinking=True ) Enable non-reasoning mode prompt_text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, enable_thinking=False ) Inference with Transformers from transformers import AutoModelForCausalLM, AutoTokenizer import torch torch.manual_seed(0) path = 'openbmb/MiniCPM4.1-8B' device = \"cuda\" tokenizer = AutoTokenizer.from_pretrained(path) model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map=device, trust_remote_code=True) User can directly use the chat interface responds, history = model.chat(tokenizer, \"Write an article about Artificial Intelligence.\", temperature=0.7, top_p=0.7) print(responds) User can also use the generate interface messages = [ {\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.",
    "id": "7f4f058c-fe8f-4e32-b06e-b937710db51f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\", temperature=0.7, top_p=0.7) print(responds) User can also use the generate interface messages = [ {\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"}, ] prompt_text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, ) model_inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(device) model_outputs = model.generate( model_inputs, max_new_tokens=32768, top_p=0.95, temperature=0.6 ) output_token_ids = [ model_outputs[i][len(model_inputs[i]):] for i in range(len(model_inputs['input_ids'])) ] responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0] print(responses) MiniCPM4.1-8B supports InfLLM v2, a sparse attention mechanism designed for efficient long-sequence inference. It requires the infllmv2_cuda_impl library. You can install it by running the following command: git clone -b feature_infer https://github.com/OpenBMB/infllmv2_cuda_impl.git cd infllmv2_cuda_impl git submodule update --init --recursive pip install -e . or python setup.py install To enable InfLLM v2, you need to add the sparse_config field in config.json: { ..., \"sparse_config\": { \"kernel_size\": 32, \"kernel_stride\": 16, \"init_blocks\": 1, \"block_size\": 64, \"window_size\": 2048, \"topk\": 64, \"use_nope\": false, \"dense_len\": 8192 } } These parameters control the behavior of InfLLM v2: kernel_size (default: 32): The size of semantic kernels. kernel_stride (default: 16): The stride between adjacent kernels. init_blocks (default: 1): The number of initial blocks that every query token attends to. This ensures attention to the beginning of the sequence. block_size (default: 64): The block size for key-value blocks.",
    "id": "ee466154-ccd4-442b-999c-4ae4cfa835a2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "kernel_stride (default: 16): The stride between adjacent kernels. init_blocks (default: 1): The number of initial blocks that every query token attends to. This ensures attention to the beginning of the sequence. block_size (default: 64): The block size for key-value blocks. window_size (default: 2048): The size of the local sliding window. topk (default: 64): The specifies that each token computes attention with only the top-k most relevant key-value blocks. use_nope (default: false): Whether to use the NOPE technique in block selection for improved performance. dense_len (default: 8192): Since Sparse Attention offers limited benefits for short sequences, the model can use standard (dense) attention for shorter texts. The model will use dense attention for sequences with a token length below dense_len and switch to sparse attention for sequences exceeding this length. Set this to -1 to always use sparse attention regardless of sequence length. MiniCPM4.1 natively supports context lengths of up to 65,536(64k) tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques for effective handling of long texts. We have validated the model's performance on context lengths of up to 131,072 tokens by modifying the LongRoPE factor. You can apply the LongRoPE factor modification by modifying the model files. Specifically, in the config.json file, adjust the rope_scaling fields. { ..., \"rope_scaling\": { \"rope_type\": \"longrope\", \"long_factor\": [0.9982316082870437, 1.033048153422584, 1.0749920956484724, 1.1255096879436193, 1.1863348602111476, 1.259543828902579, 1.3476188888731149, 1.4535223827776373, 1.5807816745852985,",
    "id": "a839ee92-4dcb-4bc8-89b5-69d1999776ce",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\"long_factor\": [0.9982316082870437, 1.033048153422584, 1.0749920956484724, 1.1255096879436193, 1.1863348602111476, 1.259543828902579, 1.3476188888731149, 1.4535223827776373, 1.5807816745852985, 1.7335856049489526, 1.9168922912975785, 2.1365471404135326, 2.3994084200118646, 2.713475511863602, 3.0880118452194134, 3.533650295140154, 4.062463396503134, 4.687974098908333, 5.425075306704039, 6.289818967956352, 7.29902962722721, 8.6357018163639, 10.210822723989212, 12.053807765671676, 14.193944598909404, 16.65780676784363, 19.463620727694074, 22.628311203524586, 26.150106147261315, 30.02526691405111, 34.23183327975347, 38.73811934094828, 43.502489489729555, 48.47627117965394, 53.61139491762471, 58.857366522037935, 64.16798299215064, 69.51359464319125, 74.86555458220285, 80.21497790341579, 85.55322183307433, 90.89611806932027, 96.26245306514224, 101.68269304046481, 107.18619510219668, 112.82253283014026, 118.63764063163615,",
    "id": "f97bbcf6-bb46-493b-99f9-c6da3e30a0c2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "64.16798299215064, 69.51359464319125, 74.86555458220285, 80.21497790341579, 85.55322183307433, 90.89611806932027, 96.26245306514224, 101.68269304046481, 107.18619510219668, 112.82253283014026, 118.63764063163615, 119.88866203644656, 120.9462882391725, 121.837565139014, 122.58663780572562, 123.2147719894291, 123.74049454862576, 124.17980424685767, 124.54641761955492, 124.85202548028222, 125.10654406389756, 125.31835105170659, 125.49450117164764, 125.64091910903052, 125.76256945356558, 125.86360463815589, 125.94749252260765, 126.01712561287873], \"short_factor\": [0.9982316082870437, 1.033048153422584, 1.0749920956484724, 1.1255096879436193, 1.1863348602111476, 1.259543828902579, 1.3476188888731149, 1.4535223827776373, 1.5807816745852985, 1.7335856049489526, 1.9168922912975785, 2.1365471404135326, 2.3994084200118646, 2.713475511863602, 3.0880118452194134, 3.533650295140154, 4.062463396503134, 4.687974098908333, 5.425075306704039,",
    "id": "548e01a3-5e87-4e61-a7ad-d40377f7a044",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "1.7335856049489526, 1.9168922912975785, 2.1365471404135326, 2.3994084200118646, 2.713475511863602, 3.0880118452194134, 3.533650295140154, 4.062463396503134, 4.687974098908333, 5.425075306704039, 6.289818967956352, 7.29902962722721, 8.6357018163639, 10.210822723989212, 12.053807765671676, 14.193944598909404, 16.65780676784363, 19.463620727694074, 22.628311203524586, 26.150106147261315, 30.02526691405111, 34.23183327975347, 38.73811934094828, 43.502489489729555, 48.47627117965394, 53.61139491762471, 58.857366522037935, 64.16798299215064, 69.51359464319125, 74.86555458220285, 80.21497790341579, 85.55322183307433, 90.89611806932027, 96.26245306514224, 101.68269304046481, 107.18619510219668, 112.82253283014026, 118.63764063163615, 119.88866203644656, 120.9462882391725, 121.837565139014, 122.58663780572562, 123.2147719894291, 123.74049454862576, 124.17980424685767, 124.54641761955492, 124.85202548028222, 125.10654406389756, 125.",
    "id": "3a440295-f276-493d-bb58-2dcd41ac4982",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "63764063163615, 119.88866203644656, 120.9462882391725, 121.837565139014, 122.58663780572562, 123.2147719894291, 123.74049454862576, 124.17980424685767, 124.54641761955492, 124.85202548028222, 125.10654406389756, 125.31835105170659, 125.49450117164764, 125.64091910903052, 125.76256945356558, 125.86360463815589, 125.94749252260765, 126.01712561287873], \"original_max_position_embeddings\": 65536 } } Inference with SGLang Speculative Decoding For accelerated inference with speculative decoding, follow these steps: 1. Download MiniCPM4.1 Draft Model First, download the MiniCPM4.1 draft model: cd /your_path git clone https://huggingface.co/openbmb/MiniCPM4.1-8B-Eagle3 2. Install EAGLE3-Compatible SGLang The EAGLE3 adaptation PR has been submitted. For now, use our repository for installation: git clone https://github.com/LDLINGLINGLING/sglang.git cd sglang pip install -e . 3.",
    "id": "d5cc6965-f98d-4073-a9b1-7030acfc4f20",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Install EAGLE3-Compatible SGLang The EAGLE3 adaptation PR has been submitted. For now, use our repository for installation: git clone https://github.com/LDLINGLINGLING/sglang.git cd sglang pip install -e . 3. Launch SGLang Server with Speculative Decoding Start the SGLang server with speculative decoding enabled: python -m sglang.launch_server \\ --model-path \"openbmb/MiniCPM4.1-8B\" \\ --host \"127.0.0.1\" \\ --port 30002 \\ --mem-fraction-static 0.9 \\ --speculative-algorithm EAGLE3 \\ --speculative-draft-model-path \"your/path/MiniCPM4_1-8B-Eagle3-bf16\" \\ --speculative-num-steps 3 \\ --speculative-eagle-topk 1 \\ --speculative-num-draft-tokens 32 \\ --temperature 0.7 4. Client Usage The client usage remains the same for both standard and speculative decoding: import openai client = openai.Client(base_url=f\"http://localhost:30002/v1\", api_key=\"None\") response = client.chat.completions.create( model=\"openbmb/MiniCPM4.1-8B\", messages=[ {\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"}, ], temperature=0.6, max_tokens=32768, ) print(response.choices[0].message.content) Note: Make sure to update the port number in the client code to match the server port (30002 in the speculative decoding example).",
    "id": "5cde3f40-f396-4f6c-a771-4e97060ed88c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\"}, ], temperature=0.6, max_tokens=32768, ) print(response.choices[0].message.content) Note: Make sure to update the port number in the client code to match the server port (30002 in the speculative decoding example). Configuration Parameters --speculative-algorithm EAGLE3: Enables EAGLE3 speculative decoding --speculative-draft-model-path: Path to the draft model for speculation --speculative-num-steps: Number of speculative steps (default: 3) --speculative-eagle-topk: Top-k parameter for EAGLE (default: 1) --speculative-num-draft-tokens: Number of draft tokens (default: 32) --mem-fraction-static: Memory fraction for static allocation (default: 0.9) Standard Inference (Without Speculative Decoding) For now, you need to install our forked version of SGLang. git clone -b openbmb https://github.com/OpenBMB/sglang.git cd sglang pip install --upgrade pip pip install -e \"python[all]\" You can start the inference server by running the following command: python -m sglang.launch_server --model openbmb/MiniCPM4.1-8B --trust-remote-code --port 30000 --chat-template chatml Then you can use the chat interface by running the following command: import openai client = openai.Client(base_url=f\"http://localhost:30000/v1\", api_key=\"None\") response = client.chat.completions.create( model=\"openbmb/MiniCPM4.1-8B\", messages=[ {\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"}, ], temperature=0.6, max_tokens=32768, ) print(response.choices[0].message.content) Inference with vLLM Speculative Decoding For accelerated inference with speculative decoding using vLLM, follow these steps: 1.",
    "id": "715acee9-4cd4-40a0-984e-e805b34bd9a2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\"}, ], temperature=0.6, max_tokens=32768, ) print(response.choices[0].message.content) Inference with vLLM Speculative Decoding For accelerated inference with speculative decoding using vLLM, follow these steps: 1. Download MiniCPM4.1 Draft Model First, download the MiniCPM4.1 draft model: cd /your_path git clone https://huggingface.co/openbmb/MiniCPM4.1-8B-Eagle3 2. Install EAGLE3-Compatible vLLM The EAGLE3 vLLM PR has been submitted. For now, use our repository for installation: git clone https://github.com/LDLINGLINGLING/vllm.git cd vllm pip install -e . 3. Launch vLLM Server with Speculative Decoding Start the vLLM inference server with speculative decoding enabled. Make sure to update the model path in the speculative-config to point to your downloaded MiniCPM4_1-8B-Eagle3-bf16 folder: VLLM_USE_V1=1 \\ vllm serve openbmb/MiniCPM4.1-8B \\ --seed 42 \\ --trust-remote-code \\ --speculative-config '{ \"model\": \"your/path/MiniCPM4_1-8B-Eagle3-bf16\", \"num_speculative_tokens\": 3, \"method\": \"eagle3\", \"draft_tensor_parallel_size\": 1 }' 4. Client Usage Example The client usage remains the same for both standard and speculative decoding: import openai client = openai.Client(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\") response = client.chat.completions.create( model=\"openbmb/MiniCPM4.1-8B\", messages=[ {\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.",
    "id": "fda51a98-0ecb-46b1-bd7f-2d223467ae98",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Client Usage Example The client usage remains the same for both standard and speculative decoding: import openai client = openai.Client(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\") response = client.chat.completions.create( model=\"openbmb/MiniCPM4.1-8B\", messages=[ {\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"}, ], temperature=0.6, max_tokens=32768, extra_body=dict(add_special_tokens=True), Ensures special tokens are added for chat template ) print(response.choices[0].message.content) vLLM Configuration Parameters VLLM_USE_V1=1: Enables vLLM v1 API --speculative-config: JSON configuration for speculative decoding model: Path to the draft model for speculation num_speculative_tokens: Number of speculative tokens (default: 3) method: Speculative decoding method (eagle3) draft_tensor_parallel_size: Tensor parallel size for draft model (default: 1) --seed: Random seed for reproducibility --trust-remote-code: Allow execution of remote code for custom models Standard Inference (Without Speculative Decoding) For now, you need to install the latest version of vLLM. pip install -U vllm \\ --pre \\ --extra-index-url https://wheels.vllm.ai/nightly Then you can inference MiniCPM4.1-8B with vLLM: from transformers import AutoTokenizer from vllm import LLM, SamplingParams model_name = \"openbmb/MiniCPM4.1-8B\" prompt = [{\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"}]",
    "id": "14962e08-e9c2-462d-aed3-6fd9d1aa1187",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "pip install -U vllm \\ --pre \\ --extra-index-url https://wheels.vllm.ai/nightly Then you can inference MiniCPM4.1-8B with vLLM: from transformers import AutoTokenizer from vllm import LLM, SamplingParams model_name = \"openbmb/MiniCPM4.1-8B\" prompt = [{\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"}] tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) input_text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True) llm = LLM( model=model_name, trust_remote_code=True, max_num_batched_tokens=65536, dtype=\"bfloat16\", gpu_memory_utilization=0.8, ) sampling_params = SamplingParams(top_p=0.95, temperature=0.6, max_tokens=32768) outputs = llm.generate(prompts=input_text, sampling_params=sampling_params) print(outputs[0].outputs[0].text) Also, you can start the inference server by running the following command: Note: In vLLM's chat API, add_special_tokens is False by default. This means important special tokens‚Äîsuch as the beginning-of-sequence (BOS) token‚Äîwill not be added automatically. To ensure the input prompt is correctly formatted for the model, you should explicitly set extra_body={\"add_special_tokens\": True}. vllm serve openbmb/MiniCPM4.1-8B Then you can use the chat interface by running the following code: import openai client = openai.Client(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\") response = client.chat.completions.create( model=\"openbmb/MiniCPM4.1-8B\", messages=[ {\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.",
    "id": "a3f22b94-06c3-4276-ab06-23fa4407598f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "vllm serve openbmb/MiniCPM4.1-8B Then you can use the chat interface by running the following code: import openai client = openai.Client(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\") response = client.chat.completions.create( model=\"openbmb/MiniCPM4.1-8B\", messages=[ {\"role\": \"user\", \"content\": \"Write an article about Artificial Intelligence.\"}, ], temperature=0.6, max_tokens=32768, extra_body=dict(add_special_tokens=True), Ensures special tokens are added for chat template ) print(response.choices[0].message.content) Evaluation Results On two typical end-side chips, Jetson AGX Orin and RTX 4090, MiniCPM4 demonstrates significantly faster processing speed compared to similar-size models in long text processing tasks. As text length increases, MiniCPM4's efficiency advantage becomes more pronounced. On the Jetson AGX Orin platform, compared to Qwen3-8B, MiniCPM4 achieves approximately 7x decoding speed improvement. MiniCPM4.1 achieves 3x decoding speed improvement in reasoning. Comprehensive Evaluation MiniCPM4.1 launches end-side versions with 8B parameter scale, both achieving best-in-class performance in their respective categories. Long Text Evaluation MiniCPM4 is pre-trained on 32K long texts and achieves length extension through YaRN technology. In the 128K long text needle-in-a-haystack task, MiniCPM4 demonstrates outstanding performance. Statement As a language model, MiniCPM generates content by learning from a vast amount of text. However, it does not possess the ability to comprehend or express personal opinions or value judgments. Any content generated by MiniCPM does not represent the viewpoints or positions of the model developers. Therefore, when using content generated by MiniCPM, users should take full responsibility for evaluating and verifying it on their own. LICENSE This repository and MiniCPM models are released under the Apache-2.0 License.",
    "id": "ec79f8bd-74ec-4931-8f7f-453560041334",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "However, it does not possess the ability to comprehend or express personal opinions or value judgments. Any content generated by MiniCPM does not represent the viewpoints or positions of the model developers. Therefore, when using content generated by MiniCPM, users should take full responsibility for evaluating and verifying it on their own. LICENSE This repository and MiniCPM models are released under the Apache-2.0 License. Citation Please cite our paper if you find our work valuable. @article{minicpm4, title={{MiniCPM4}: Ultra-Efficient LLMs on End Devices}, author={MiniCPM Team}, year={2025} } Downloads last month204 Safetensors Model size 8.19B params Tensor type BF16 ¬∑ Chat template Files info Inference Providers NEW Text Generation This model isn't deployed by any Inference Provider.",
    "id": "10f4aa48-42cd-437a-a9e9-704041a0f71d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Citation Please cite our paper if you find our work valuable. @article{minicpm4, title={{MiniCPM4}: Ultra-Efficient LLMs on End Devices}, author={MiniCPM Team}, year={2025} } Downloads last month204 Safetensors Model size 8.19B params Tensor type BF16 ¬∑ Chat template Files info Inference Providers NEW Text Generation This model isn't deployed by any Inference Provider. üôã 1 Ask for provider support Model tree for openbmb/MiniCPM4.1-8B Finetunes 3 models Quantizations 12 models Space using openbmb/MiniCPM4.1-8B 1 üåñ akhaliq/MiniCPM4.1-8B Collection including openbmb/MiniCPM4.1-8B MiniCPM4 Collection MiniCPM4: Ultra-Efficient LLMs on End Devices ‚Ä¢ 29 items ‚Ä¢ Updated about 14 hours ago ‚Ä¢ 75 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: FastVLM WebGPU - a Hugging Face Space by apple (https://huggingface.co/spaces/apple/fastvlm-webgpu) === FastVLM WebGPU - a Hugging Face Space by apple Spaces apple / fastvlm-webgpu like 351 Running App Files Files Community 6 Refreshing === PAGE: Writer (Writer) (https://huggingface.co/Writer) === Writer (Writer) Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Writer Enterprise company Verified https://writer.com/ Get_Writer writer Activity Feed Follow 329 AI & ML interests AGI, LLMs, Knowledge Graph, Palmyra, Domain Specific LLM Team members 167 +133 +120 +99 +89 +69 Organization Card Community About org cards Transform work with full-stack generative AI Build generative AI into any business process with the secure enterprise platform. Collections 2 Palmyra (apache 2.",
    "id": "4a48ca6a-dbd2-4dea-9849-624d73b23ff5",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Collections 2 Palmyra (apache 2.0 license) Writer/palmyra-small Text Generation ‚Ä¢ 0.2B ‚Ä¢ Updated Sep 1, 2023 ‚Ä¢ 54 ‚Ä¢ 20 Writer/palmyra-20b-chat Text Generation ‚Ä¢ Updated Aug 17, 2024 ‚Ä¢ 1.76k ‚Ä¢ 11 Writer/palmyra-med-20b Text Generation ‚Ä¢ Updated Aug 17, 2024 ‚Ä¢ 1.84k ‚Ä¢ 36 Writer/palmyra-base Text Generation ‚Ä¢ 5B ‚Ä¢ Updated Dec 24, 2024 ‚Ä¢ 1.79k ‚Ä¢ 45 Palmyra (Writer license) Palmyra LLMs under Writer license https://writer.com/legal/open-model-license/ Writer/Palmyra-Med-70B-32K Text Generation ‚Ä¢ 71B ‚Ä¢ Updated 7 days ago ‚Ä¢ 27 ‚Ä¢ 117 Writer/Palmyra-Med-70B Text Generation ‚Ä¢ 71B ‚Ä¢ Updated Oct 1, 2024 ‚Ä¢ 8 ‚Ä¢ 83 Palmyra (apache 2.0 license) Writer/palmyra-small Text Generation ‚Ä¢ 0.2B ‚Ä¢ Updated Sep 1, 2023 ‚Ä¢ 54 ‚Ä¢ 20 Writer/palmyra-20b-chat Text Generation ‚Ä¢ Updated Aug 17, 2024 ‚Ä¢ 1.76k ‚Ä¢ 11 Writer/palmyra-med-20b Text Generation ‚Ä¢ Updated Aug 17, 2024 ‚Ä¢ 1.84k ‚Ä¢ 36 Writer/palmyra-base Text Generation ‚Ä¢ 5B ‚Ä¢ Updated Dec 24, 2024 ‚Ä¢ 1.79k ‚Ä¢ 45 Palmyra (Writer license) Palmyra LLMs under Writer license https://writer.com/legal/open-model-license/ Writer/Palmyra-Med-70B-32K Text Generation ‚Ä¢ 71B ‚Ä¢ Updated 7 days ago ‚Ä¢ 27 ‚Ä¢ 117 Writer/Palmyra-Med-70B Text Generation ‚Ä¢ 71B ‚Ä¢ Updated Oct 1,",
    "id": "05d276e9-5e72-4e6f-b81f-32ebb2176be5",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2024 ‚Ä¢ 1.79k ‚Ä¢ 45 Palmyra (Writer license) Palmyra LLMs under Writer license https://writer.com/legal/open-model-license/ Writer/Palmyra-Med-70B-32K Text Generation ‚Ä¢ 71B ‚Ä¢ Updated 7 days ago ‚Ä¢ 27 ‚Ä¢ 117 Writer/Palmyra-Med-70B Text Generation ‚Ä¢ 71B ‚Ä¢ Updated Oct 1, 2024 ‚Ä¢ 8 ‚Ä¢ 83 spaces 5 Sort: Recently updated Running GRPO Any Model üåç Train and generate text using GRPO Writer May 26 Running 9 Financial LLM Performance Leaderboard üìà Expect the Unexpected: FailSafe Long Context QA for Finance Writer Feb 19 Running on CPU Upgrade 7 Token Counter üìà Count tokens for different models Writer Nov 1, 2024 Running 5 Paste To Markdown üëÅ Convert text to Markdown Writer May 12, 2023 models 21 Sort: Recently updated Writer/Palmyra-Med-70B-32K Text Generation ‚Ä¢ 71B ‚Ä¢ Updated 7 days ago ‚Ä¢ 27 ‚Ä¢ 117 Writer/colab Updated Jul 24 ‚Ä¢ 2 Writer/Palmyra-X-4.3-73B 73B ‚Ä¢ Updated May 21 ‚Ä¢ 18 ‚Ä¢ 1 Writer/Palmyra-local-1_7B 2B ‚Ä¢ Updated Apr 7 ‚Ä¢ 5 ‚Ä¢ 1 Writer/palmyra-large Text Generation ‚Ä¢ 21B ‚Ä¢ Updated Feb 25 ‚Ä¢ 1.75k ‚Ä¢ 23 Writer/palmyra-vision-dummy-weights 8B ‚Ä¢ Updated Jan 28 Writer/bdiff-test Updated Jan 17 Writer/palmyra-base Text Generation ‚Ä¢ 5B ‚Ä¢ Updated Dec 24, 2024 ‚Ä¢ 1.79k ‚Ä¢ 45 Writer/palmyra-creative-dummy-weights 122B ‚Ä¢ Updated Dec 18, 2024 Writer/palmyra-x-4.3-long-cite Updated Dec 13,",
    "id": "df7a10bc-501f-461e-9176-716649a15d0a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2024 ‚Ä¢ 1.79k ‚Ä¢ 45 Writer/palmyra-creative-dummy-weights 122B ‚Ä¢ Updated Dec 18, 2024 Writer/palmyra-x-4.3-long-cite Updated Dec 13, 2024 View 21 models datasets 7 Sort: Recently updated Writer/InfiniteBenchFixed Viewer ‚Ä¢ Updated Jul 21 ‚Ä¢ 3.95k ‚Ä¢ 241 ‚Ä¢ 1 Writer/FailSafeQA Viewer ‚Ä¢ Updated Feb 13 ‚Ä¢ 220 ‚Ä¢ 468 ‚Ä¢ 9 Writer/writing-in-the-margins-multihoprag Viewer ‚Ä¢ Updated Sep 5, 2024 ‚Ä¢ 1k ‚Ä¢ 12 Writer/TinyStoriesInstruct-v0-32k-0.2 Viewer ‚Ä¢ Updated Jul 13, 2024 ‚Ä¢ 15.7k ‚Ä¢ 33 ‚Ä¢ 3 Writer/TinyStoriesInstruct-v0-32k-0.1 Viewer ‚Ä¢ Updated Jul 13, 2024 ‚Ä¢ 15.7k ‚Ä¢ 31 Writer/omniact Updated Apr 29, 2024 ‚Ä¢ 496 ‚Ä¢ 39 Writer/Palmyra-instract-30 Preview ‚Ä¢ Updated Sep 30, 2023 ‚Ä¢ 1 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Text-to-Video Models ‚Äì Hugging Face (https://huggingface.co/models?pipeline_tag=text-to-video) === Text-to-Video Models ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks 1 Libraries Languages Licenses Other Tasks Reset Tasks Text Generation Any-to-Any Image-Text-to-Text Image-to-Text Image-to-Image Text-to-Image Text-to-Video Text-to-Speech + 42Parameters Reset Parameters < 1B 6B 12B 32B 128B > 500B < 1B > 500B Libraries PyTorch google-tensorflow TensorFlow JAX Transformers Diffusers Safetensors ONNX GGUF Transformers.",
    "id": "66e6d110-9f87-48bd-b1c1-9fbff452af3c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "js MLX Keras + 41 Apps vLLM TGI llama.cpp MLX LM LM Studio Ollama Jan + 13 Inference Providers Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale + 6 Apply filters Models 645 Full-text search Edit filters Sort: Trending Active filters: text-to-video Clear all lightx2v/Wan2.2-Lightning Text-to-Video ‚Ä¢ Updated 26 days ago ‚Ä¢ 344 Wan-AI/Wan2.2-T2V-A14B Text-to-Video ‚Ä¢ Updated Aug 7 ‚Ä¢ 11.7k ‚Ä¢ ‚Ä¢ 270 Wan-AI/Wan2.2-TI2V-5B Text-to-Video ‚Ä¢ Updated Aug 7 ‚Ä¢ 14.9k ‚Ä¢ ‚Ä¢ 340 Cseti/wan2.2-14B-Kinestasis_concept-lora-v1 Text-to-Video ‚Ä¢ Updated 6 days ago ‚Ä¢ 8 tencent/HunyuanVideo Text-to-Video ‚Ä¢ Updated Mar 6 ‚Ä¢ 1.42k ‚Ä¢ ‚Ä¢ 2.03k QuantStack/Wan2.2-S2V-14B-GGUF Text-to-Video ‚Ä¢ 16B ‚Ä¢ Updated 10 days ago ‚Ä¢ 24.8k ‚Ä¢ 46 QuantStack/Wan2.2-T2V-A14B-GGUF Text-to-Video ‚Ä¢ 14B ‚Ä¢ Updated Jul 29 ‚Ä¢ 182k ‚Ä¢ 101 deadman44/Wan2.2_T2i_T2v_LoRA Text-to-Video ‚Ä¢ Updated 4 days ago ‚Ä¢ 15 calcuis/wan2-gguf Text-to-Video ‚Ä¢ 0.1B ‚Ä¢ Updated 10 days ago ‚Ä¢ 16.5k ‚Ä¢ 19 BowenXue/Stand-In Text-to-Video ‚Ä¢ Updated 25 days ago ‚Ä¢ 72 zai-org/CogVideoX1.5-5B Text-to-Video ‚Ä¢ Updated Mar 28 ‚Ä¢ 10.7k ‚Ä¢ 48 Wan-AI/Wan2.",
    "id": "351cb0ef-9170-4cdd-8366-1efcd3971097",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "1B ‚Ä¢ Updated 10 days ago ‚Ä¢ 16.5k ‚Ä¢ 19 BowenXue/Stand-In Text-to-Video ‚Ä¢ Updated 25 days ago ‚Ä¢ 72 zai-org/CogVideoX1.5-5B Text-to-Video ‚Ä¢ Updated Mar 28 ‚Ä¢ 10.7k ‚Ä¢ 48 Wan-AI/Wan2.1-T2V-14B Text-to-Video ‚Ä¢ Updated Mar 12 ‚Ä¢ 36.1k ‚Ä¢ ‚Ä¢ 1.4k vrgamedevgirl84/Wan14BT2VFusioniX Text-to-Video ‚Ä¢ Updated Jun 21 ‚Ä¢ 553 Wan-AI/Wan2.2-TI2V-5B-Diffusers Text-to-Video ‚Ä¢ Updated about 1 month ago ‚Ä¢ 14.5k ‚Ä¢ 73 QuantStack/Wan2.2-TI2V-5B-GGUF Text-to-Video ‚Ä¢ 5B ‚Ä¢ Updated Jul 31 ‚Ä¢ 30.4k ‚Ä¢ 81 genmo/mochi-1-preview Text-to-Video ‚Ä¢ Updated 4 days ago ‚Ä¢ 12k ‚Ä¢ ‚Ä¢ 1.26k Eyeline-Labs/CineScale Text-to-Video ‚Ä¢ Updated 8 days ago ‚Ä¢ 6 Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8 Text-to-Video ‚Ä¢ Updated Jul 16 ‚Ä¢ 6.36k ‚Ä¢ 21 bullerwins/Wan2.2-T2V-A14B-GGUF Text-to-Video ‚Ä¢ 14B ‚Ä¢ Updated Jul 28 ‚Ä¢ 81k ‚Ä¢ 47 deadman44/Wan2.2_Workflow_for_myxx_series_LoRA Text-to-Video ‚Ä¢ Updated 3 days ago ‚Ä¢ 13 CCP6/FakeVace2.2 Text-to-Video ‚Ä¢ Updated 30 days ago ‚Ä¢ 22 QuantStack/Wan2.2-Fun-A14B-Control-GGUF Text-to-Video ‚Ä¢ 14B ‚Ä¢ Updated 26 days ago ‚Ä¢ 46.",
    "id": "43f37b92-1cc3-41a5-be69-526bc026583d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2_Workflow_for_myxx_series_LoRA Text-to-Video ‚Ä¢ Updated 3 days ago ‚Ä¢ 13 CCP6/FakeVace2.2 Text-to-Video ‚Ä¢ Updated 30 days ago ‚Ä¢ 22 QuantStack/Wan2.2-Fun-A14B-Control-GGUF Text-to-Video ‚Ä¢ 14B ‚Ä¢ Updated 26 days ago ‚Ä¢ 46.1k ‚Ä¢ 28 valiantcat/one_finger_wan21_t2v_lora Text-to-Video ‚Ä¢ Updated 15 days ago ‚Ä¢ 24 ‚Ä¢ 2 ali-vilab/text-to-video-ms-1.7b Text-to-Video ‚Ä¢ Updated Dec 1, 2023 ‚Ä¢ 13.5k ‚Ä¢ 639 cerspense/zeroscope_v2_576w Text-to-Video ‚Ä¢ Updated Jul 1, 2023 ‚Ä¢ 9.15k ‚Ä¢ 486 ByteDance/AnimateDiff-Lightning Text-to-Video ‚Ä¢ Updated Jan 6 ‚Ä¢ 37.7k ‚Ä¢ 959 nagayama0706/video_generation_model Text-to-Video ‚Ä¢ 7B ‚Ä¢ Updated Apr 16, 2024 ‚Ä¢ 53 ‚Ä¢ 6 zai-org/CogVideoX-5b Text-to-Video ‚Ä¢ Updated Nov 23, 2024 ‚Ä¢ 36.1k ‚Ä¢ ‚Ä¢ 640 city96/HunyuanVideo-gguf Text-to-Video ‚Ä¢ 13B ‚Ä¢ Updated Dec 17, 2024 ‚Ä¢ 3.92k ‚Ä¢ 179 a-r-r-o-w/LTX-Video-0.9.1-diffusers Text-to-Video ‚Ä¢ Updated Mar 18 ‚Ä¢ 456 ‚Ä¢ 8 Previous 1 2 3 ... 22 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Transformers.js (https://huggingface.co/docs/transformers.js) === Transformers.js Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Transformers.js documentation Transformers.js Transformers.",
    "id": "0152169e-2ee4-4237-a820-053e94decff0",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "9.1-diffusers Text-to-Video ‚Ä¢ Updated Mar 18 ‚Ä¢ 456 ‚Ä¢ 8 Previous 1 2 3 ... 22 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Transformers.js (https://huggingface.co/docs/transformers.js) === Transformers.js Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Transformers.js documentation Transformers.js Transformers.js üè° View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm Search documentation mainv3.0.0v2.17.2 EN ü§ó Transformers.js Get started Installation The pipeline API Custom usage Tutorials Building a Vanilla JS Application Building a React Application Building a Next.js Application Building a Browser Extension Building an Electron Application Server-side Inference in Node.js Developer Guides Running models on WebGPU Using quantized models (dtypes) Accessing Private/Gated Models Server-side Audio Processing API Reference Index Pipelines Models Tokenizers Processors Configs Environment variables Backends Generation Utilities You are viewing main version, which requires installation from source. If you'd like regular npm install, checkout the latest stable version (v3.0.0). Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Transformers.js State-of-the-art Machine Learning for the Web Run ü§ó Transformers directly in your browser, with no need for a server!",
    "id": "9ca58686-3ab3-4c2b-8037-5f8a4235c500",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "which requires installation from source. If you'd like regular npm install, checkout the latest stable version (v3.0.0). Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Transformers.js State-of-the-art Machine Learning for the Web Run ü§ó Transformers directly in your browser, with no need for a server! Transformers.js is designed to be functionally equivalent to Hugging Face‚Äôs transformers python library, meaning you can run the same pretrained models using a very similar API. These models support common tasks in different modalities, such as: üìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation. üñºÔ∏è Computer Vision: image classification, object detection, segmentation, and depth estimation. üó£Ô∏è Audio: automatic speech recognition, audio classification, and text-to-speech. üêô Multimodal: embeddings, zero-shot audio classification, zero-shot image classification, and zero-shot object detection. Transformers.js uses ONNX Runtime to run models in the browser. The best part about it, is that you can easily convert your pretrained PyTorch, TensorFlow, or JAX models to ONNX using ü§ó Optimum. For more information, check out the full documentation. Quick tour It‚Äôs super simple to translate from existing code! Just like the python library, we support the pipeline API. Pipelines group together a pretrained model with preprocessing of inputs and postprocessing of outputs, making it the easiest way to run models with the library. Python (original) Javascript (ours) Copied from transformers import pipeline Allocate a pipeline for sentiment-analysis pipe = pipeline('sentiment-analysis') out = pipe('I love transformers!')",
    "id": "91299f2a-aaef-468a-b703-eff58ba4b434",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Quick tour It‚Äôs super simple to translate from existing code! Just like the python library, we support the pipeline API. Pipelines group together a pretrained model with preprocessing of inputs and postprocessing of outputs, making it the easiest way to run models with the library. Python (original) Javascript (ours) Copied from transformers import pipeline Allocate a pipeline for sentiment-analysis pipe = pipeline('sentiment-analysis') out = pipe('I love transformers!') [{'label': 'POSITIVE', 'score': 0.999806941}] Copied import { pipeline } from '@huggingface/transformers'; // Allocate a pipeline for sentiment-analysis const pipe = await pipeline('sentiment-analysis'); const out = await pipe('I love transformers!'); // [{'label': 'POSITIVE', 'score': 0.999817686}] You can also use a different model by specifying the model id or path as the second argument to the pipeline function. For example: Copied // Use a different model for sentiment-analysis const pipe = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment'); By default, when running in the browser, the model will be run on your CPU (via WASM). If you would like to run the model on your GPU (via WebGPU), you can do this by setting device: 'webgpu', for example: Copied // Run the model on WebGPU const pipe = await pipeline('sentiment-analysis', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english', { device: 'webgpu', }); For more information, check out the WebGPU guide. The WebGPU API is still experimental in many browsers, so if you run into any issues, please file a bug report. In resource-constrained environments, such as web browsers, it is advisable to use a quantized version of the model to lower bandwidth and optimize performance. This can be achieved by adjusting the dtype option, which allows you to select the appropriate data type for your model.",
    "id": "1435c34b-aec5-426c-8ae3-95d17f6860a3",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The WebGPU API is still experimental in many browsers, so if you run into any issues, please file a bug report. In resource-constrained environments, such as web browsers, it is advisable to use a quantized version of the model to lower bandwidth and optimize performance. This can be achieved by adjusting the dtype option, which allows you to select the appropriate data type for your model. While the available options may vary depending on the specific model, typical choices include \"fp32\" (default for WebGPU), \"fp16\", \"q8\" (default for WASM), and \"q4\". For more information, check out the quantization guide. Copied // Run the model at 4-bit quantization const pipe = await pipeline('sentiment-analysis', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english', { dtype: 'q4', }); Contents The documentation is organized into 4 sections: GET STARTED provides a quick tour of the library and installation instructions to get up and running. TUTORIALS are a great place to start if you‚Äôre a beginner! We also include sample applications for you to play around with! DEVELOPER GUIDES show you how to use the library to achieve a specific goal. API REFERENCE describes all classes and functions, as well as their available parameters and types. Examples Want to jump straight in? Get started with one of our sample applications/templates, which can be found here.",
    "id": "d53ec54f-2fb9-4da7-ac45-e70100a0368a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "TUTORIALS are a great place to start if you‚Äôre a beginner! We also include sample applications for you to play around with! DEVELOPER GUIDES show you how to use the library to achieve a specific goal. API REFERENCE describes all classes and functions, as well as their available parameters and types. Examples Want to jump straight in? Get started with one of our sample applications/templates, which can be found here. Name Description Links Whisper Web Speech recognition w/ Whisper code, demo Doodle Dash Real-time sketch-recognition game blog, code, demo Code Playground In-browser code completion website code, demo Semantic Image Search (client-side) Search for images with text code, demo Semantic Image Search (server-side) Search for images with text (Supabase) code, demo Vanilla JavaScript In-browser object detection video, code, demo React Multilingual translation website code, demo Text to speech (client-side) In-browser speech synthesis code, demo Browser extension Text classification extension code Electron Text classification application code Next.js (client-side) Sentiment analysis (in-browser inference) code, demo Next.js (server-side) Sentiment analysis (Node.js inference) code, demo Node.js Sentiment analysis API code Demo site A collection of demos code, demo Check out the Transformers.js template on Hugging Face to get started in one click! Supported tasks/models Here is the list of all tasks and architectures currently supported by Transformers.js. If you don‚Äôt see your task/model listed here or it is not yet supported, feel free to open up a feature request here. To find compatible models on the Hub, select the ‚Äútransformers.js‚Äù library tag in the filter menu (or visit this link). You can refine your search by selecting the task you‚Äôre interested in (e.g., text-classification). Tasks Natural Language Processing Task ID Description Supported? Fill-Mask fill-mask Masking some of the words in a sentence and predicting which words should replace those masks. ‚úÖ (docs)(models) Question Answering question-answering Retrieve the answer to a question from a given text.",
    "id": "eae65dfd-6d5a-4d72-adb4-a6428369482f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "You can refine your search by selecting the task you‚Äôre interested in (e.g., text-classification). Tasks Natural Language Processing Task ID Description Supported? Fill-Mask fill-mask Masking some of the words in a sentence and predicting which words should replace those masks. ‚úÖ (docs)(models) Question Answering question-answering Retrieve the answer to a question from a given text. ‚úÖ (docs)(models) Sentence Similarity sentence-similarity Determining how similar two texts are. ‚úÖ (docs)(models) Summarization summarization Producing a shorter version of a document while preserving its important information. ‚úÖ (docs)(models) Table Question Answering table-question-answering Answering a question about information from a given table. ‚ùå Text Classification text-classification or sentiment-analysis Assigning a label or class to a given text. ‚úÖ (docs)(models) Text Generation text-generation Producing new text by predicting the next word in a sequence. ‚úÖ (docs)(models) Text-to-text Generation text2text-generation Converting one text sequence into another text sequence. ‚úÖ (docs)(models) Token Classification token-classification or ner Assigning a label to each token in a text. ‚úÖ (docs)(models) Translation translation Converting text from one language to another. ‚úÖ (docs)(models) Zero-Shot Classification zero-shot-classification Classifying text into classes that are unseen during training. ‚úÖ (docs)(models) Feature Extraction feature-extraction Transforming raw data into numerical features that can be processed while preserving the information in the original dataset. ‚úÖ (docs)(models) Vision Task ID Description Supported? Background Removal background-removal Isolating the main subject of an image by removing or making the background transparent. ‚úÖ (docs)(models) Depth Estimation depth-estimation Predicting the depth of objects present in an image. ‚úÖ (docs)(models) Image Classification image-classification Assigning a label or class to an entire image.",
    "id": "7faba120-564a-4d1d-880a-30a7b89e0a26",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "‚úÖ (docs)(models) Vision Task ID Description Supported? Background Removal background-removal Isolating the main subject of an image by removing or making the background transparent. ‚úÖ (docs)(models) Depth Estimation depth-estimation Predicting the depth of objects present in an image. ‚úÖ (docs)(models) Image Classification image-classification Assigning a label or class to an entire image. ‚úÖ (docs)(models) Image Segmentation image-segmentation Divides an image into segments where each pixel is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation. ‚úÖ (docs)(models) Image-to-Image image-to-image Transforming a source image to match the characteristics of a target image or a target image domain. ‚úÖ (docs)(models) Mask Generation mask-generation Generate masks for the objects in an image. ‚ùå Object Detection object-detection Identify objects of certain defined classes within an image. ‚úÖ (docs)(models) Video Classification n/a Assigning a label or class to an entire video. ‚ùå Unconditional Image Generation n/a Generating images with no condition in any context (like a prompt text or another image). ‚ùå Image Feature Extraction image-feature-extraction Transforming raw data into numerical features that can be processed while preserving the information in the original image. ‚úÖ (docs)(models) Audio Task ID Description Supported? Audio Classification audio-classification Assigning a label or class to a given audio. ‚úÖ (docs)(models) Audio-to-Audio n/a Generating audio from an input audio source. ‚ùå Automatic Speech Recognition automatic-speech-recognition Transcribing a given audio into text. ‚úÖ (docs)(models) Text-to-Speech text-to-speech or text-to-audio Generating natural-sounding speech given text input. ‚úÖ (docs)(models) Tabular Task ID Description Supported? Tabular Classification n/a Classifying a target category (a group) based on set of attributes.",
    "id": "b4723e87-07f9-416d-8fc9-97e8f508af55",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "‚ùå Automatic Speech Recognition automatic-speech-recognition Transcribing a given audio into text. ‚úÖ (docs)(models) Text-to-Speech text-to-speech or text-to-audio Generating natural-sounding speech given text input. ‚úÖ (docs)(models) Tabular Task ID Description Supported? Tabular Classification n/a Classifying a target category (a group) based on set of attributes. ‚ùå Tabular Regression n/a Predicting a numerical value given a set of attributes. ‚ùå Multimodal Task ID Description Supported? Document Question Answering document-question-answering Answering questions on document images. ‚úÖ (docs)(models) Image-to-Text image-to-text Output text from a given image. ‚úÖ (docs)(models) Text-to-Image text-to-image Generates images from input text. ‚ùå Visual Question Answering visual-question-answering Answering open-ended questions based on an image. ‚ùå Zero-Shot Audio Classification zero-shot-audio-classification Classifying audios into classes that are unseen during training. ‚úÖ (docs)(models) Zero-Shot Image Classification zero-shot-image-classification Classifying images into classes that are unseen during training. ‚úÖ (docs)(models) Zero-Shot Object Detection zero-shot-object-detection Identify objects of classes that are unseen during training. ‚úÖ (docs)(models) Reinforcement Learning Task ID Description Supported? Reinforcement Learning n/a Learning from actions by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback. ‚úÖ Models ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.",
    "id": "6a2a7f44-9810-4387-bcdc-333bd6b61dbd",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Reinforcement Learning n/a Learning from actions by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback. ‚úÖ Models ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. Arcee (from Arcee AI) released with the blog post Announcing Arcee Foundation Models by Fernando Fernandes, Varun Singh, Charles Goddard, Lucas Atkins, Mark McQuade, Maziyar Panahi, Conner Stewart, Colin Kealty, Raghav Ravishankar, Lucas Krauss, Anneketh Vij, Pranav Veldurthi, Abhishek Thakur, Julien Simon, Scott Zembsch, Benjamin Langer, Aleksiej Cecocho, Maitri Patel. Audio Spectrogram Transformer (from MIT) released with the paper AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass. BART (from Facebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer. BEiT (from Microsoft) released with the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong, Furu Wei. BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.",
    "id": "f2d82fa1-24da-40d5-9952-4541e2b61c83",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "BEiT (from Microsoft) released with the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong, Furu Wei. BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. Blenderbot (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston. BlenderbotSmall (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston. BLOOM (from BigScience workshop) released by the BigScience Workshop. CamemBERT (from Inria/Facebook/Sorbonne) released with the paper CamemBERT: a Tasty French Language Model by Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su√°rez, Yoann Dupont, Laurent Romary, √âric Villemonte de la Clergerie, Djam√© Seddah and Beno√Æt Sagot. Chinese-CLIP (from OFA-Sys) released with the paper Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou. CLAP (from LAION-AI) released with the paper Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.",
    "id": "a50a3495-68f0-478d-86a9-472f86fdfc6c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "CLAP (from LAION-AI) released with the paper Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov. CLIP (from OpenAI) released with the paper Learning Transferable Visual Models From Natural Language Supervision by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIPSeg (from University of G√∂ttingen) released with the paper Image Segmentation Using Text and Image Prompts by Timo L√ºddecke and Alexander Ecker. CodeGen (from Salesforce) released with the paper A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. CodeLlama (from MetaAI) released with the paper Code Llama: Open Foundation Models for Code by Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D√©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve. Cohere (from Cohere) released with the paper Command-R: Retrieval Augmented Generation at Production Scale by Cohere.",
    "id": "1e8e7354-b1b3-4ed7-8bfc-43d44484ff8b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Cohere (from Cohere) released with the paper Command-R: Retrieval Augmented Generation at Production Scale by Cohere. ConvBERT (from YituTech) released with the paper ConvBERT: Improving BERT with Span-based Dynamic Convolution by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan. ConvNeXT (from Facebook AI) released with the paper A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. ConvNeXTV2 (from Facebook AI) released with the paper ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie. D-FINE (from University of Science and Technology of China) released with the paper D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement by Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, Feng Wu. DAC (from Descript) released with the paper Descript Audio Codec: High-Fidelity Audio Compression with Improved RVQGAN by Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, Kundan Kumar. DeBERTa (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. DeBERTa-v2 (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.",
    "id": "766c0221-bc0c-4f6d-8f2c-fdcb6035a429",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "DeBERTa (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. DeBERTa-v2 (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. Decision Transformer (from Berkeley/Facebook/Google) released with the paper Decision Transformer: Reinforcement Learning via Sequence Modeling by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch. DeiT (from Facebook) released with the paper Training data-efficient image transformers & distillation through attention by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv√© J√©gou. Depth Anything (from University of Hong Kong and TikTok) released with the paper Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data by Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao. Depth Pro (from Apple) released with the paper Depth Pro: Sharp Monocular Metric Depth in Less Than a Second by Aleksei Bochkovskii, Ama√´l Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun. DETR (from Facebook) released with the paper End-to-End Object Detection with Transformers by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.",
    "id": "ebcc50ac-3c63-42b4-8bb1-7963a617895f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "DETR (from Facebook) released with the paper End-to-End Object Detection with Transformers by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko. DINOv2 (from Meta AI) released with the paper DINOv2: Learning Robust Visual Features without Supervision by Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv√© Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski. DINOv2 with Registers (from Meta AI) released with the paper DINOv2 with Registers by Timoth√©e Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski. DINOv3 (from Meta AI) released with the paper DINOv3 by Oriane Sim√©oni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Micha√´l Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timoth√©e Darcet, Th√©o Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herv√© J√©gou, Patrick Labatut, Piotr Bojanowski.",
    "id": "0c8db46f-726f-4281-8e4b-88b779012c68",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "DistilBERT (from HuggingFace), released together with the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into DistilGPT2, RoBERTa into DistilRoBERTa, Multilingual BERT into DistilmBERT and a German version of DistilBERT. DiT (from Microsoft Research) released with the paper DiT: Self-supervised Pre-training for Document Image Transformer by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei. Donut (from NAVER), released together with the paper OCR-free Document Understanding Transformer by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park. DPT (from Intel Labs) released with the paper Vision Transformers for Dense Prediction by Ren√© Ranftl, Alexey Bochkovskiy, Vladlen Koltun. EfficientNet (from Google Brain) released with the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks by Mingxing Tan, Quoc V. Le. ELECTRA (from Google Research/Stanford University) released with the paper ELECTRA: Pre-training text encoders as discriminators rather than generators by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. ERNIE-4.5 (from Baidu ERNIE Team) released with the blog post Announcing the Open Source Release of the ERNIE 4.5 Model Family by the Baidu ERNIE Team. ESM (from Meta AI) are transformer protein language models.",
    "id": "c9003398-0b11-43e8-96fe-adb37a6cdc92",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "ERNIE-4.5 (from Baidu ERNIE Team) released with the blog post Announcing the Open Source Release of the ERNIE 4.5 Model Family by the Baidu ERNIE Team. ESM (from Meta AI) are transformer protein language models. ESM-1b was released with the paper Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. ESM-1v was released with the paper Language models enable zero-shot prediction of the effects of mutations on protein function by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. ESM-2 and ESMFold were released with the paper Language models of protein sequences at the scale of evolution enable accurate structure prediction by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives. EXAONE (from LG AI Research) released with the papers EXAONE 3.0 7.8B Instruction Tuned Language Model and EXAONE 3.5: Series of Large Language Models for Real-world Use Cases by the LG AI Research team. Falcon (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.",
    "id": "57536bf3-bc5b-402e-bb2f-90672011a40d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "FastViT (from Apple) released with the paper FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization by Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel and Anurag Ranjan. FLAN-T5 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei Florence2 (from Microsoft) released with the paper Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks by Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan. Gemma (from Google) released with the paper Gemma: Open Models Based on Gemini Technology and Research by the Gemma Google team. Gemma2 (from Google) released with the paper Gemma2: Open Models Based on Gemini Technology and Research by the Gemma Google team. Gemma3 (from Google) released with the paper Introducing Gemma 3: The most capable model you can run on a single GPU or TPU by the Gemma Google team. Gemma3n (from Google) released with the paper Announcing Gemma 3n preview: powerful, efficient, mobile-first AI by the Gemma Google team.",
    "id": "d13e7878-818d-4678-a78f-9f12181f395c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Gemma3 (from Google) released with the paper Introducing Gemma 3: The most capable model you can run on a single GPU or TPU by the Gemma Google team. Gemma3n (from Google) released with the paper Announcing Gemma 3n preview: powerful, efficient, mobile-first AI by the Gemma Google team. GLM (from the GLM Team, THUDM & ZhipuAI) released with the paper ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools by Team GLM: Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Jingyu Sun, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang. GLPN (from KAIST) released with the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.",
    "id": "80f0a8c2-eac0-4607-a75e-ba746744bf10",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "GLPN (from KAIST) released with the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim. GPT Neo (from EleutherAI) released in the repository EleutherAI/gpt-neo by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. GPT NeoX (from EleutherAI) released with the paper GPT-NeoX-20B: An Open-Source Autoregressive Language Model by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodeiand Ilya Sutskever. GPT-J (from EleutherAI) released in the repository kingoflolz/mesh-transformer-jax by Ben Wang and Aran Komatsuzaki. GPTBigCode (from BigCode) released with the paper SantaCoder: don‚Äôt reach for the stars!",
    "id": "ab813084-ccfd-4f6c-b313-0d34850bb0b3",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "GPT-J (from EleutherAI) released in the repository kingoflolz/mesh-transformer-jax by Ben Wang and Aran Komatsuzaki. GPTBigCode (from BigCode) released with the paper SantaCoder: don‚Äôt reach for the stars! by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc√≠a del R√≠o, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra. Granite (from IBM) released with the paper Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, Rameswar Panda. Grounding DINO (from IDEA-Research) released with the paper Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang.",
    "id": "89e3fe7a-339b-4ea1-88bf-210169ed398c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Grounding DINO (from IDEA-Research) released with the paper Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang. GroupViT (from UCSD, NVIDIA) released with the paper GroupViT: Semantic Segmentation Emerges from Text Supervision by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang. Helium (from the Kyutai Team) released with the blog post Announcing Helium-1 Preview by the Kyutai Team. HerBERT (from Allegro.pl, AGH University of Science and Technology) released with the paper KLEJ: Comprehensive Benchmark for Polish Language Understanding by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik. Hiera (from Meta) released with the paper Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles by Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer. Hubert (from Facebook) released with the paper HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.",
    "id": "f3bc51b5-846c-436e-a7bf-058b9ab76ce7",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Hubert (from Facebook) released with the paper HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed. I-JEPA (from Meta) released with the paper Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture by Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas. Idefics3 (from Hugging Face) released with the paper Building and better understanding vision-language models: insights and future directions by Hugo Lauren√ßon, Andr√©s Marafioti, Victor Sanh, L√©o Tronchon. JAIS (from Core42) released with the paper Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models by Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, William Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming Chen, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Xudong Han, Sondos Mahmoud Bsharat, Alham Fikri Aji, Zhiqiang Shen, Zhengzhong Liu, Natalia Vassilieva, Joel Hestness, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Hector Xuguang Ren, Preslav Nakov, Timothy Baldwin, Eric Xing.",
    "id": "e9128753-b265-4df0-9339-99f9f233aeb1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Janus (from DeepSeek) released with the paper Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, Ping Luo. JinaCLIP (from Jina AI) released with the paper Jina CLIP: Your CLIP Model Is Also Your Text Retriever by Andreas Koukounas, Georgios Mastrapas, Michael G√ºnther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Mart√≠nez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, Han Xiao. LiteWhisper (from University of Washington, Kotoba Technologies) released with the paper LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation by Keisuke Kamahori, Jungo Kasai, Noriyuki Kojima, Baris Kasikci. LongT5 (from Google AI) released with the paper LongT5: Efficient Text-To-Text Transformer for Long Sequences by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang. LFM2 (from Liquid AI) released with the blog post Introducing LFM2: The Fastest On-Device Foundation Models on the Market by the Liquid AI Team.",
    "id": "3e0cb890-eeb3-4457-a2e4-b5efe672de6d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "LongT5 (from Google AI) released with the paper LongT5: Efficient Text-To-Text Transformer for Long Sequences by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang. LFM2 (from Liquid AI) released with the blog post Introducing LFM2: The Fastest On-Device Foundation Models on the Market by the Liquid AI Team. LLaMA (from The FAIR team of Meta AI) released with the paper LLaMA: Open and Efficient Foundation Language Models by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.",
    "id": "3a779856-c39f-4cbe-8a08-56afa74917e2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Llama2 (from The FAIR team of Meta AI) released with the paper Llama2: Open Foundation and Fine-Tuned Chat Models by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom. LLaVa (from Microsoft Research & University of Wisconsin-Madison) released with the paper Visual Instruction Tuning by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.",
    "id": "87085ad8-ce80-4ac9-94f8-bedfdb24505c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "LLaVa (from Microsoft Research & University of Wisconsin-Madison) released with the paper Visual Instruction Tuning by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee. LLaVA-OneVision (from ByteDance & NTU & CUHK & HKUST) released with the paper LLaVA-OneVision: Easy Visual Task Transfer by Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li M2M100 (from Facebook) released with the paper Beyond English-Centric Multilingual Machine Translation by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin. MarianMT Machine translation models trained using OPUS data by J√∂rg Tiedemann. The Marian Framework is being developed by the Microsoft Translator Team. MaskFormer (from Meta and UIUC) released with the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov. mBART (from Facebook) released with the paper Multilingual Denoising Pre-training for Neural Machine Translation by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer. mBART-50 (from Facebook) released with the paper Multilingual Translation with Extensible Multilingual Pretraining and Finetuning by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.",
    "id": "e12506b1-7f30-470f-9c1d-850c54a3c96c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "mBART-50 (from Facebook) released with the paper Multilingual Translation with Extensible Multilingual Pretraining and Finetuning by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan. Metric3D released with the paper Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image by Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, Chunhua Shen. Metric3Dv2 released with the paper Metric3Dv2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation by Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Kaixuan Wang, Hao Chen, Gang Yu, Chunhua Shen, Shaojie Shen. MusicGen (from Meta) released with the paper Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre D√©fossez. MGP-STR (from Alibaba Research) released with the paper Multi-Granularity Prediction for Scene Text Recognition by Peng Wang, Cheng Da, and Cong Yao. Mimi (from Kyutai) released with the paper Moshi: a speech-text foundation model for real-time dialogue by Alexandre D√©fossez, Laurent Mazar√©, Manu Orsini, Am√©lie Royer, Patrick P√©rez, Herv√© J√©gou, Edouard Grave and Neil Zeghidour.",
    "id": "5888f5a1-e265-4d97-8049-951b5661be40",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Mimi (from Kyutai) released with the paper Moshi: a speech-text foundation model for real-time dialogue by Alexandre D√©fossez, Laurent Mazar√©, Manu Orsini, Am√©lie Royer, Patrick P√©rez, Herv√© J√©gou, Edouard Grave and Neil Zeghidour. Mistral (from Mistral AI) by The Mistral AI team: Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L√©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed. MMS (from Facebook) released with the paper Scaling Speech Technology to 1,000+ Languages by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli. MobileBERT (from CMU/Google Brain) released with the paper MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. MobileCLIP (from Apple) released with the paper MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training by Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel.",
    "id": "ab9d9d38-0468-4e08-ba8b-bd1170351813",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "MobileCLIP (from Apple) released with the paper MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training by Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel. MobileLLM (from Meta) released with the paper MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases by Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra. MobileNetV1 (from Google Inc.) released with the paper MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam. MobileNetV2 (from Google Inc.) released with the paper MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNetV3 (from Google Inc.) released with the paper Searching for MobileNetV3 by Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. MobileNetV4 (from Google Inc.)",
    "id": "4f935ccd-071a-4f1f-b237-54667b952f37",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "MobileNetV3 (from Google Inc.) released with the paper Searching for MobileNetV3 by Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. MobileNetV4 (from Google Inc.) released with the paper MobileNetV4 - Universal Models for the Mobile Ecosystem by Danfeng Qin, Chas Leichner, Manolis Delakis, Marco Fornoni, Shixin Luo, Fan Yang, Weijun Wang, Colby Banbury, Chengxi Ye, Berkin Akin, Vaibhav Aggarwal, Tenghui Zhu, Daniele Moro, Andrew Howard. MobileViT (from Apple) released with the paper MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari. MobileViTV2 (from Apple) released with the paper Separable Self-attention for Mobile Vision Transformers by Sachin Mehta and Mohammad Rastegari. ModernBERT (from Answer.AI and LightOn) released with the paper Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference by Benjamin Warner, Antoine Chaffin, Benjamin Clavi√©, Orion Weller, Oskar Hallstr√∂m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli. ModernBERT Decoder (from Johns Hopkins University and LightOn) released with the paper Seq vs Seq: An Open Suite of Paired Encoders and Decoders by Orion Weller, Kathryn Ricci, Marc Marone, Antoine Chaffin, Dawn Lawrie, Benjamin Van Durme. Moondream1 released in the repository moondream by vikhyat.",
    "id": "255ebb49-87fc-4691-af8c-d7290237c9d6",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "ModernBERT Decoder (from Johns Hopkins University and LightOn) released with the paper Seq vs Seq: An Open Suite of Paired Encoders and Decoders by Orion Weller, Kathryn Ricci, Marc Marone, Antoine Chaffin, Dawn Lawrie, Benjamin Van Durme. Moondream1 released in the repository moondream by vikhyat. Moonshine (from Useful Sensors) released with the paper Moonshine: Speech Recognition for Live Transcription and Voice Commands by Nat Jeffries, Evan King, Manjunath Kudlur, Guy Nicholson, James Wang, Pete Warden. MPNet (from Microsoft Research) released with the paper MPNet: Masked and Permuted Pre-training for Language Understanding by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu. MPT (from MosaicML) released with the repository llm-foundry by the MosaicML NLP Team. MT5 (from Google AI) released with the paper mT5: A massively multilingual pre-trained text-to-text transformer by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. NeoBERT (from Chandar Research Lab) released with the paper NeoBERT: A Next-Generation BERT by Lola Le Breton, Quentin Fournier, Mariam El Mezouar, John X. Morris, Sarath Chandar. NLLB (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team. Nougat (from Meta AI) released with the paper Nougat: Neural Optical Understanding for Academic Documents by Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic.",
    "id": "0856305d-ec5c-4b89-a02f-c9d541c27bb5",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "NLLB (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team. Nougat (from Meta AI) released with the paper Nougat: Neural Optical Understanding for Academic Documents by Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic. OLMo (from Ai2) released with the paper OLMo: Accelerating the Science of Language Models by Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi. OLMo2 (from Ai2) released with the blog OLMo 2: The best fully open language model to date by the Ai2 OLMo team.",
    "id": "9cdc85e2-47dc-4e0e-b8ad-c7a5762b2666",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "OLMo2 (from Ai2) released with the blog OLMo 2: The best fully open language model to date by the Ai2 OLMo team. OpenELM (from Apple) released with the paper OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework by Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari. OPT (from Meta AI) released with the paper OPT: Open Pre-trained Transformer Language Models by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al. OWL-ViT (from Google AI) released with the paper Simple Open-Vocabulary Object Detection with Vision Transformers by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWLv2 (from Google AI) released with the paper Scaling Open-Vocabulary Object Detection by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. PaliGemma (from Google) released with the papers PaliGemma: A versatile 3B VLM for transfer and PaliGemma 2: A Family of Versatile VLMs for Transfer by the PaliGemma Google team. PatchTSMixer (from IBM) released with the paper TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.",
    "id": "d42762ef-0b48-43d0-903c-8004e0d01256",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "PatchTSMixer (from IBM) released with the paper TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. PatchTST (from Princeton University, IBM) released with the paper A Time Series is Worth 64 Words: Long-term Forecasting with Transformers by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. Phi (from Microsoft) released with the papers - Textbooks Are All You Need by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S√©bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, Textbooks Are All You Need II: phi-1.5 technical report by Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee. Phi3 (from Microsoft) released with the paper Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone by Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S√©bastien Bubeck, Martin Cai, Caio C√©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra,",
    "id": "47e4c0a4-fb61-4b57-9363-7a4e219ba3ad",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S√©bastien Bubeck, Martin Cai, Caio C√©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou.",
    "id": "c99eae33-2371-4600-a109-058d0215bf76",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou. Phi3V (from Microsoft) released with the paper Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone by Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S√©bastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu,",
    "id": "b5d9413b-5d2e-4d37-a4e9-f420871c056e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio C√©sar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang , Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou. PVT (from Nanjing University, The University of Hong Kong etc.)",
    "id": "6e86c948-720b-4317-9d9f-f37643ad83f4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou. PVT (from Nanjing University, The University of Hong Kong etc.) released with the paper Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. PyAnnote released in the repository pyannote/pyannote-audio by Herv√© Bredin. Qwen2 (from the Qwen team, Alibaba Group) released with the paper Qwen Technical Report by Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou and Tianhang Zhu.",
    "id": "12b7b8d3-70b8-430f-8a7c-3e0ed938dcab",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Qwen2-VL (from the Qwen team, Alibaba Group) released with the paper Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond by Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou. Qwen3 (from the Qwen team, Alibaba Group) released with the blog post Qwen3: Think Deeper, Act Faster by the Qwen team. ResNet (from Microsoft Research) released with the paper Deep Residual Learning for Image Recognition by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. RF-DETR (from Roboflow) released with the blog post RF-DETR: A SOTA Real-Time Object Detection Model by Peter Robicheaux, James Gallagher, Joseph Nelson, Isaac Robinson. RoBERTa (from Facebook), released together with the paper RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. RoFormer (from ZhuiyiTechnology), released together with the paper RoFormer: Enhanced Transformer with Rotary Position Embedding by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu. RT-DETR (from Baidu), released together with the paper DETRs Beat YOLOs on Real-time Object Detection by Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen.",
    "id": "354582ce-a0d6-4e66-9637-1e06dce6fb2d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "RT-DETR (from Baidu), released together with the paper DETRs Beat YOLOs on Real-time Object Detection by Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen. RT-DETRv2 (from Baidu), released together with the paper RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer by Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, Yi Liu. Sapiens (from Meta AI) released with the paper Sapiens: Foundation for Human Vision Models by Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, Shunsuke Saito. SegFormer (from NVIDIA) released with the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo. Segment Anything (from Meta AI) released with the paper Segment Anything by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick. SigLIP (from Google AI) released with the paper Sigmoid Loss for Language Image Pre-Training by Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer. SmolLM3 (from Hugging Face) released with the blog post SmolLM3: smol, multilingual, long-context reasoner by the Hugging Face TB Research team.",
    "id": "1914535d-7d7b-4d32-93f0-4cfa02aa1d70",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "SigLIP (from Google AI) released with the paper Sigmoid Loss for Language Image Pre-Training by Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer. SmolLM3 (from Hugging Face) released with the blog post SmolLM3: smol, multilingual, long-context reasoner by the Hugging Face TB Research team. SmolVLM (from Hugging Face) released with the blog posts SmolVLM - small yet mighty Vision Language Model and SmolVLM Grows Smaller ‚Äì Introducing the 250M & 500M Models! by the Hugging Face TB Research team. SNAC (from Papla Media, ETH Zurich) released with the paper SNAC: Multi-Scale Neural Audio Codec by Hubert Siuzdak, Florian Gr√∂tschla, Luca A. Lanzend√∂rfer. SpeechT5 (from Microsoft Research) released with the paper SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei. SqueezeBERT (from Berkeley) released with the paper SqueezeBERT: What can computer vision teach NLP about efficient neural networks? by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. StableLm (from Stability AI) released with the paper StableLM 3B 4E1T (Technical Report) by Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme Ruiz, Duy Phung, Maksym Zhuravinskyi, Nathan Cooper, Nikhil Pinnaparaju, Reshinth Adithyan, and James Baicoianu.",
    "id": "fdff71d8-7511-405a-ae49-2a7a3e3c6b9f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "StableLm (from Stability AI) released with the paper StableLM 3B 4E1T (Technical Report) by Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme Ruiz, Duy Phung, Maksym Zhuravinskyi, Nathan Cooper, Nikhil Pinnaparaju, Reshinth Adithyan, and James Baicoianu. Starcoder2 (from BigCode team) released with the paper StarCoder 2 and The Stack v2: The Next Generation by Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau√ü, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu√±oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.",
    "id": "2feca306-23c3-4c8d-93eb-2f2abac41c61",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "StyleTTS 2 (from Columbia University) released with the paper StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models by Yinghao Aaron Li, Cong Han, Vinay S. Raghavan, Gavin Mischler, Nima Mesgarani. Swin Transformer (from Microsoft) released with the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. Swin2SR (from University of W√ºrzburg) released with the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte. T5 (from Google AI) released with the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu. T5v1.1 (from Google AI) released in the repository google-research/text-to-text-transfer-transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu. Table Transformer (from Microsoft Research) released with the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Brandon Smock, Rohith Pesala, Robin Abraham. TrOCR (from Microsoft), released together with the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.",
    "id": "14f9e1fb-4023-4e84-9a16-cd88c4e2a5bb",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "TrOCR (from Microsoft), released together with the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei. Ultravox (from Fixie.ai) released with the repository fixie-ai/ultravox by the Fixie.ai team. UniSpeech (from Microsoft Research) released with the paper UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang. UniSpeechSat (from Microsoft Research) released with the paper UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu. Vision Transformer (ViT) (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. ViTMAE (from Meta AI) released with the paper Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, Ross Girshick.",
    "id": "bd4f4c89-0dc2-4016-99f8-aafa6d102fc7",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "ViTMAE (from Meta AI) released with the paper Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, Ross Girshick. ViTMatte (from HUST-VL) released with the paper ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang. ViTMSN (from Meta AI) released with the paper Masked Siamese Networks for Label-Efficient Learning by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas. ViTPose (from The University of Sydney) released with the paper ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation by Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao. VITS (from Kakao Enterprise) released with the paper Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech by Jaehyeon Kim, Jungil Kong, Juhee Son. Voxtral (from Mistral AI) released with the paper Voxtral by Alexander H. Liu, Andy Ehrenberg, Andy Lo, Cl√©ment Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, Sanchit Gandhi, Soham Ghosh, Srijan Mishra, Thomas Foubert, Abhinav Rastogi, Adam Yang, Albert Q. Jiang, Alexandre Sablayrolles, Am√©lie H√©liou, Am√©lie Martin, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozi√®re,",
    "id": "625f1174-927d-47f5-bf7e-6508cab29ba0",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Patrick von Platen, Pavankumar Reddy Muddireddy, Sanchit Gandhi, Soham Ghosh, Srijan Mishra, Thomas Foubert, Abhinav Rastogi, Adam Yang, Albert Q. Jiang, Alexandre Sablayrolles, Am√©lie H√©liou, Am√©lie Martin, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozi√®re, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Cl√©mence Lanfranchi, Darius Dabert, Devendra Singh Chaplot, Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gabrielle Berrada, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jason Rute, Jean-Hadrien Chabran, Jessica Chudnovsky, Joachim Studnia, Joep Barmentlo, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Karmesh Yadav, Kartik Khandelwal, Kush Jain, L√©lio Renard Lavaud, L√©onard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Matthieu Dinot, Maxime Darrin, Maximilian Augustin, Micka√´l Seznec, Neha Gupta, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Philom√®ne Chagniot, Pierre Stock, Pravesh Agrawal, R√©mi Delacourt, Romain Sauvestre, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian,",
    "id": "78e7431a-9b18-444f-8f20-3a06677123b9",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Maximilian Augustin, Micka√´l Seznec, Neha Gupta, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Philom√®ne Chagniot, Pierre Stock, Pravesh Agrawal, R√©mi Delacourt, Romain Sauvestre, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Shashwat Dalal, Siddharth Gandhi, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Robert, Thomas Wang, Timoth√©e Lacroix, Tom Bewley, Valeriia Nemychnikova, Victor Paltz , Virgile Richard, Wen-Ding Li, William Marshall, Xuanyu Zhang, Yihan Wan, Yunhao Tang. Wav2Vec2 (from Facebook AI) released with the paper wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. Wav2Vec2-BERT (from Meta AI) released with the paper Seamless: Multilingual Expressive and Streaming Speech Translation by the Seamless Communication team. WavLM (from Microsoft Research) released with the paper WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.",
    "id": "66d1689e-fc93-40ca-84b6-8219056ca781",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Whisper (from OpenAI) released with the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever. XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau. XLM-RoBERTa (from Facebook AI), released together with the paper Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. YOLOS (from Huazhong University of Science & Technology) released with the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu. < > Update on GitHub Installation‚Üí Transformers.js Quick tour Contents Examples Supported tasks/models Tasks Natural Language Processing Vision Audio Tabular Multimodal Reinforcement Learning Models === PAGE: Hugging Face - Learn (https://huggingface.",
    "id": "302217f6-6d93-4510-a0ef-0f2266b5f697",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "< > Update on GitHub Installation‚Üí Transformers.js Quick tour Contents Examples Supported tasks/models Tasks Natural Language Processing Vision Audio Tabular Multimodal Reinforcement Learning Models === PAGE: Hugging Face - Learn (https://huggingface.co/learn) === Hugging Face - Learn Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Learn LLM Course This course will teach you about large language models using libraries from the HF ecosystem MCP Course This course will teach you about Model Context Protocol a smol course This smollest course on post-training AI models Agents Course Learn to build and deploy your own AI agents Deep RL Course This course will teach you about deep reinforcement learning using libraries from the HF ecosystem Community Computer Vision Course This course will teach you about computer vision ML using libraries and models from the HF ecosystem Audio Course Learn to apply transformers to audio data using libraries from the HF ecosystem Open-Source AI Cookbook A collection of open-source-powered notebooks by AI builders, for AI builders ML for Games Course This course will teach you about integrating AI models your game and using AI tools in your game development workflow Diffusion Course Learn about diffusion models & how to use them with diffusers ML for 3D Course Learn about 3D ML with libraries from the HF ecosystem System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Wan2.2 14B Fast - a Hugging Face Space by zerogpu-aoti (https://huggingface.co/spaces/zerogpu-aoti/wan2-2-fp8da-aoti-faster) === Wan2.2 14B Fast - a Hugging Face Space by zerogpu-aoti Spaces Duplicated from zerogpu-aoti/wan2-2-fp8da-aoti zerogpu-aoti / wan2-2-fp8da-aoti-faster like 812 MCP Running on Zero App Files Files Community 13 Fetching metadata from the HF Docker repository... Refreshing === PAGE: Models ‚Äì Hugging Face (https://huggingface.",
    "id": "8e31fad8-35b0-4d57-94d2-d8434b19a13e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2 14B Fast - a Hugging Face Space by zerogpu-aoti Spaces Duplicated from zerogpu-aoti/wan2-2-fp8da-aoti zerogpu-aoti / wan2-2-fp8da-aoti-faster like 812 MCP Running on Zero App Files Files Community 13 Fetching metadata from the HF Docker repository... Refreshing === PAGE: Models ‚Äì Hugging Face (https://huggingface.co/models?other=mlx-lm) === Models ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks Libraries Languages Licenses Other 1 Apps llama.cpp LM Studio Jan Backyard AI Draw Things DiffusionBee Jellybox RecurseChat Msty Sanctum Invoke JoyFusion LocalAI vLLM node-llama-cpp Ollama TGI MLX LM Docker Model Runner Lemonade Inference Providers Select all Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale SambaNova fal Featherless AI Cohere Replicate HF Inference API Misc Reset Misc mlx-lm Inference Endpoints text-generation-inference Eval Results Merge 4-bit precision custom_code 8-bit precision text-embeddings-inference Mixture of Experts Carbon Emissions Apply filters Models 8 Full-text search Edit filters Sort: Trending Active filters: mlx-lm Clear all halley-ai/gpt-oss-120b-MLX-8bit-gs32 Text Generation ‚Ä¢ 117B ‚Ä¢ Updated about 12 hours ago ‚Ä¢ 4 ‚Ä¢ 1 halley-ai/gpt-oss-120b-MLX-bf16 Text Generation ‚Ä¢ 117B ‚Ä¢ Updated about 12 hours ago ‚Ä¢ 3 ‚Ä¢ 1 halley-ai/gpt-oss-120b-MLX-6bit-gs64 Text Generation ‚Ä¢ 117B ‚Ä¢ Updated about 12 hours ago ‚Ä¢ 1 kacperbb/phi-3.5-mlx-finetuned Updated Jul 22 ‚Ä¢ 13 mlx-community/GLM-4.",
    "id": "a65bedab-93d9-4a7e-bc62-66c7d915fbe8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "5-mlx-finetuned Updated Jul 22 ‚Ä¢ 13 mlx-community/GLM-4.5-MLX-8bit Text Generation ‚Ä¢ Updated 23 days ago ‚Ä¢ 2 PKSGIN/MyGemma270_Shellcommands Updated 14 days ago ‚Ä¢ 1 fourbic/disarm-ew-llama3-finetuned Text Generation ‚Ä¢ 8B ‚Ä¢ Updated 13 days ago ‚Ä¢ 3 alexander-model/gpt_model_safe Text Generation ‚Ä¢ 21B ‚Ä¢ Updated 11 days ago ‚Ä¢ 43 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Text Generation Models ‚Äì Hugging Face (https://huggingface.co/models?pipeline_tag=text-generation) === Text Generation Models ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks 1 Libraries Languages Licenses Other Tasks Reset Tasks Text Generation Any-to-Any Image-Text-to-Text Image-to-Text Image-to-Image Text-to-Image Text-to-Video Text-to-Speech + 42Parameters Reset Parameters < 1B 6B 12B 32B 128B > 500B < 1B > 500B Libraries PyTorch google-tensorflow TensorFlow JAX Transformers Diffusers Safetensors ONNX GGUF Transformers.js MLX Keras + 41 Apps vLLM TGI llama.cpp MLX LM LM Studio Ollama Jan + 13 Inference Providers Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale + 6 Apply filters Models Full-text search Edit filters Sort: Trending Active filters: text-generation Clear all moonshotai/Kimi-K2-Instruct-0905 Text Generation ‚Ä¢ Updated 4 days ago ‚Ä¢ 4.55k ‚Ä¢ ‚Ä¢ 297 swiss-ai/Apertus-8B-Instruct-2509 Text Generation ‚Ä¢ 8B ‚Ä¢ Updated 3 days ago ‚Ä¢ 50.7k ‚Ä¢ 256 openbmb/MiniCPM4.",
    "id": "83f8e056-1144-4d3a-8cf1-613a3c99e861",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "55k ‚Ä¢ ‚Ä¢ 297 swiss-ai/Apertus-8B-Instruct-2509 Text Generation ‚Ä¢ 8B ‚Ä¢ Updated 3 days ago ‚Ä¢ 50.7k ‚Ä¢ 256 openbmb/MiniCPM4.1-8B Text Generation ‚Ä¢ 8B ‚Ä¢ Updated 3 days ago ‚Ä¢ 204 ‚Ä¢ 236 meituan-longcat/LongCat-Flash-Chat Text Generation ‚Ä¢ 562B ‚Ä¢ Updated about 14 hours ago ‚Ä¢ 32.8k ‚Ä¢ 431 apple/FastVLM-0.5B Text Generation ‚Ä¢ 0.8B ‚Ä¢ Updated 5 days ago ‚Ä¢ 18.2k ‚Ä¢ 259 apple/FastVLM-7B Text Generation ‚Ä¢ 8B ‚Ä¢ Updated 5 days ago ‚Ä¢ 16.3k ‚Ä¢ 211 swiss-ai/Apertus-70B-Instruct-2509 Text Generation ‚Ä¢ 71B ‚Ä¢ Updated 3 days ago ‚Ä¢ 29.1k ‚Ä¢ 116 openai/gpt-oss-120b Text Generation ‚Ä¢ 120B ‚Ä¢ Updated 13 days ago ‚Ä¢ 3M ‚Ä¢ ‚Ä¢ 3.78k swiss-ai/Apertus-70B-2509 Text Generation ‚Ä¢ 71B ‚Ä¢ Updated 3 days ago ‚Ä¢ 1.89k ‚Ä¢ 87 swiss-ai/Apertus-8B-2509 Text Generation ‚Ä¢ 8B ‚Ä¢ Updated 3 days ago ‚Ä¢ 4.1k ‚Ä¢ 83 openai/gpt-oss-20b Text Generation ‚Ä¢ 22B ‚Ä¢ Updated 13 days ago ‚Ä¢ 8.93M ‚Ä¢ ‚Ä¢ 3.44k NousResearch/Hermes-4-14B Text Generation ‚Ä¢ 0.0B ‚Ä¢ Updated 5 days ago ‚Ä¢ 2.84k ‚Ä¢ 73 Nikity/lille-130m-instruct Text Generation ‚Ä¢ 0.1B ‚Ä¢ Updated 4 days ago ‚Ä¢ 1.",
    "id": "136746d2-b06f-43c5-9aec-fae8e79c9566",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "1k ‚Ä¢ 83 openai/gpt-oss-20b Text Generation ‚Ä¢ 22B ‚Ä¢ Updated 13 days ago ‚Ä¢ 8.93M ‚Ä¢ ‚Ä¢ 3.44k NousResearch/Hermes-4-14B Text Generation ‚Ä¢ 0.0B ‚Ä¢ Updated 5 days ago ‚Ä¢ 2.84k ‚Ä¢ 73 Nikity/lille-130m-instruct Text Generation ‚Ä¢ 0.1B ‚Ä¢ Updated 4 days ago ‚Ä¢ 1.64k ‚Ä¢ 88 Tesslate/WEBGEN-4B-Preview Text Generation ‚Ä¢ 4B ‚Ä¢ Updated 6 days ago ‚Ä¢ 709 ‚Ä¢ 57 Kwai-Klear/Klear-46B-A2.5B-Instruct Text Generation ‚Ä¢ 46B ‚Ä¢ Updated 1 day ago ‚Ä¢ 139 ‚Ä¢ 56 google/gemma-3-270m Text Generation ‚Ä¢ 0.3B ‚Ä¢ Updated 25 days ago ‚Ä¢ 147k ‚Ä¢ 753 deepseek-ai/DeepSeek-V3.1 Text Generation ‚Ä¢ 685B ‚Ä¢ Updated 3 days ago ‚Ä¢ 146k ‚Ä¢ ‚Ä¢ 723 nvidia/NVIDIA-Nemotron-Nano-9B-v2 Text Generation ‚Ä¢ 9B ‚Ä¢ Updated 10 days ago ‚Ä¢ 88.7k ‚Ä¢ 323 continuedev/instinct Text Generation ‚Ä¢ 8B ‚Ä¢ Updated 3 days ago ‚Ä¢ 125 ‚Ä¢ 38 apple/FastVLM-1.5B Text Generation ‚Ä¢ 2B ‚Ä¢ Updated 5 days ago ‚Ä¢ 4.93k ‚Ä¢ 54 Qwen/Qwen3-Coder-480B-A35B-Instruct Text Generation ‚Ä¢ 480B ‚Ä¢ Updated 18 days ago ‚Ä¢ 229k ‚Ä¢ ‚Ä¢ 1.",
    "id": "c2982bc1-dbf6-4009-ab0a-bc950cbc09fc",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "7k ‚Ä¢ 323 continuedev/instinct Text Generation ‚Ä¢ 8B ‚Ä¢ Updated 3 days ago ‚Ä¢ 125 ‚Ä¢ 38 apple/FastVLM-1.5B Text Generation ‚Ä¢ 2B ‚Ä¢ Updated 5 days ago ‚Ä¢ 4.93k ‚Ä¢ 54 Qwen/Qwen3-Coder-480B-A35B-Instruct Text Generation ‚Ä¢ 480B ‚Ä¢ Updated 18 days ago ‚Ä¢ 229k ‚Ä¢ ‚Ä¢ 1.17k DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf Text Generation ‚Ä¢ 21B ‚Ä¢ Updated 10 days ago ‚Ä¢ 262k ‚Ä¢ 160 Qwen/Qwen3-Coder-30B-A3B-Instruct Text Generation ‚Ä¢ 31B ‚Ä¢ Updated 18 days ago ‚Ä¢ 320k ‚Ä¢ ‚Ä¢ 559 LatitudeGames/Wayfarer-2-12B Text Generation ‚Ä¢ 12B ‚Ä¢ Updated 5 days ago ‚Ä¢ 204 ‚Ä¢ 28 ByteDance-Seed/Seed-OSS-36B-Instruct Text Generation ‚Ä¢ 36B ‚Ä¢ Updated 14 days ago ‚Ä¢ 19.2k ‚Ä¢ 405 janhq/Jan-v1-edge Text Generation ‚Ä¢ 2B ‚Ä¢ Updated 4 days ago ‚Ä¢ 157 ‚Ä¢ 27 meta-llama/Llama-3.1-8B-Instruct Text Generation ‚Ä¢ 8B ‚Ä¢ Updated Sep 25, 2024 ‚Ä¢ 8.48M ‚Ä¢ ‚Ä¢ 4.58k NousResearch/Hermes-4-70B Text Generation ‚Ä¢ 71B ‚Ä¢ Updated 6 days ago ‚Ä¢ 4.86k ‚Ä¢ ‚Ä¢ 128 nvidia/NVIDIA-Nemotron-Nano-12B-v2 Text Generation ‚Ä¢ 12B ‚Ä¢ Updated 10 days ago ‚Ä¢ 35.4k ‚Ä¢ 64 Kwai-Klear/Klear-46B-A2.",
    "id": "e0f0dc05-7cfc-49f6-865e-194c1e3b98ee",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2024 ‚Ä¢ 8.48M ‚Ä¢ ‚Ä¢ 4.58k NousResearch/Hermes-4-70B Text Generation ‚Ä¢ 71B ‚Ä¢ Updated 6 days ago ‚Ä¢ 4.86k ‚Ä¢ ‚Ä¢ 128 nvidia/NVIDIA-Nemotron-Nano-12B-v2 Text Generation ‚Ä¢ 12B ‚Ä¢ Updated 10 days ago ‚Ä¢ 35.4k ‚Ä¢ 64 Kwai-Klear/Klear-46B-A2.5B-Base Text Generation ‚Ä¢ 46B ‚Ä¢ Updated 1 day ago ‚Ä¢ 46 ‚Ä¢ 23 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Models ‚Äì Hugging Face (https://huggingface.co/models?other=vllm) === Models ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks Libraries Languages Licenses Other 1 Apps llama.cpp LM Studio Jan Backyard AI Draw Things DiffusionBee Jellybox RecurseChat Msty Sanctum Invoke JoyFusion LocalAI vLLM node-llama-cpp Ollama TGI MLX LM Docker Model Runner Lemonade Inference Providers Select all Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale SambaNova fal Featherless AI Cohere Replicate HF Inference API Misc Reset Misc vllm Inference Endpoints text-generation-inference Eval Results Merge 4-bit precision custom_code 8-bit precision text-embeddings-inference Mixture of Experts Carbon Emissions Apply filters Models 1,204 Full-text search Edit filters Sort: Trending Active filters: vllm Clear all openai/gpt-oss-120b Text Generation ‚Ä¢ 120B ‚Ä¢ Updated 13 days ago ‚Ä¢ 3M ‚Ä¢ ‚Ä¢ 3.78k openai/gpt-oss-20b Text Generation ‚Ä¢ 22B ‚Ä¢ Updated 13 days ago ‚Ä¢ 8.93M ‚Ä¢ ‚Ä¢ 3.",
    "id": "b97391df-9fd1-49b4-8c8d-8cd8f2933ff1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "204 Full-text search Edit filters Sort: Trending Active filters: vllm Clear all openai/gpt-oss-120b Text Generation ‚Ä¢ 120B ‚Ä¢ Updated 13 days ago ‚Ä¢ 3M ‚Ä¢ ‚Ä¢ 3.78k openai/gpt-oss-20b Text Generation ‚Ä¢ 22B ‚Ä¢ Updated 13 days ago ‚Ä¢ 8.93M ‚Ä¢ ‚Ä¢ 3.44k ByteDance-Seed/Seed-OSS-36B-Instruct Text Generation ‚Ä¢ 36B ‚Ä¢ Updated 14 days ago ‚Ä¢ 19.2k ‚Ä¢ 405 jinaai/jina-embeddings-v4 Visual Document Retrieval ‚Ä¢ 4B ‚Ä¢ Updated 6 days ago ‚Ä¢ 42.9k ‚Ä¢ 349 mistralai/Mistral-Small-3.2-24B-Instruct-2506 24B ‚Ä¢ Updated 18 days ago ‚Ä¢ 266k ‚Ä¢ 446 mistralai/Mistral-7B-Instruct-v0.3 7B ‚Ä¢ Updated Jul 24 ‚Ä¢ 778k ‚Ä¢ 2.07k mistralai/Mixtral-8x7B-Instruct-v0.1 47B ‚Ä¢ Updated Jul 24 ‚Ä¢ 287k ‚Ä¢ 4.55k huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated Text Generation ‚Ä¢ 21B ‚Ä¢ Updated 6 days ago ‚Ä¢ 55.7k ‚Ä¢ 156 mistralai/Voxtral-Mini-3B-2507 5B ‚Ä¢ Updated Jul 28 ‚Ä¢ 165k ‚Ä¢ 546 mistralai/Mixtral-8x7B-v0.1 47B ‚Ä¢ Updated Jul 24 ‚Ä¢ 54.6k ‚Ä¢ 1.75k mistralai/Devstral-Small-2507 24B ‚Ä¢ Updated 21 days ago ‚Ä¢ 14.",
    "id": "bfd93978-d6c3-4c8b-98ba-209f8347e250",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "7k ‚Ä¢ 156 mistralai/Voxtral-Mini-3B-2507 5B ‚Ä¢ Updated Jul 28 ‚Ä¢ 165k ‚Ä¢ 546 mistralai/Mixtral-8x7B-v0.1 47B ‚Ä¢ Updated Jul 24 ‚Ä¢ 54.6k ‚Ä¢ 1.75k mistralai/Devstral-Small-2507 24B ‚Ä¢ Updated 21 days ago ‚Ä¢ 14.7k ‚Ä¢ 327 mistralai/Ministral-8B-Instruct-2410 8B ‚Ä¢ Updated Jul 31 ‚Ä¢ 232k ‚Ä¢ 530 Jinx-org/Jinx-gpt-oss-20b Text Generation ‚Ä¢ 21B ‚Ä¢ Updated 2 days ago ‚Ä¢ 57.5k ‚Ä¢ 61 unsloth/Seed-OSS-36B-Instruct-GGUF Text Generation ‚Ä¢ 36B ‚Ä¢ Updated 15 days ago ‚Ä¢ 46.8k ‚Ä¢ 26 unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF Image-Text-to-Text ‚Ä¢ 24B ‚Ä¢ Updated 13 days ago ‚Ä¢ 41.9k ‚Ä¢ 105 huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated Text Generation ‚Ä¢ 117B ‚Ä¢ Updated 6 days ago ‚Ä¢ 6.72k ‚Ä¢ 23 mistralai/Mistral-Nemo-Instruct-2407 12B ‚Ä¢ Updated Jul 28 ‚Ä¢ 119k ‚Ä¢ 1.59k mistralai/Voxtral-Small-24B-2507 Audio-Text-to-Text ‚Ä¢ 24B ‚Ä¢ Updated Jul 28 ‚Ä¢ 8.39k ‚Ä¢ 393 mistralai/Mistral-Small-3.1-24B-Instruct-2503 24B ‚Ä¢ Updated Jul 28 ‚Ä¢ 291k ‚Ä¢ 1.31k ISTA-DASLab/Mistral-Small-3.",
    "id": "be52c9b5-dc17-45e8-b568-b263bdeebe43",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "59k mistralai/Voxtral-Small-24B-2507 Audio-Text-to-Text ‚Ä¢ 24B ‚Ä¢ Updated Jul 28 ‚Ä¢ 8.39k ‚Ä¢ 393 mistralai/Mistral-Small-3.1-24B-Instruct-2503 24B ‚Ä¢ Updated Jul 28 ‚Ä¢ 291k ‚Ä¢ 1.31k ISTA-DASLab/Mistral-Small-3.1-24B-Instruct-2503-GPTQ-4b-128g Image-Text-to-Text ‚Ä¢ 5B ‚Ä¢ Updated Apr 6 ‚Ä¢ 4.27k ‚Ä¢ 17 mistralai/Magistral-Small-2507 24B ‚Ä¢ Updated Jul 28 ‚Ä¢ 3.17k ‚Ä¢ 93 unsloth/gpt-oss-20b-BF16 Text Generation ‚Ä¢ 21B ‚Ä¢ Updated Aug 5 ‚Ä¢ 53.4k ‚Ä¢ 20 RedHatAI/gpt-oss-120b-FP8-dynamic Text Generation ‚Ä¢ 117B ‚Ä¢ Updated 13 days ago ‚Ä¢ 1.84k ‚Ä¢ 5 RedHatAI/Qwen2.5-VL-72B-Instruct-quantized.w4a16 Image-to-Text ‚Ä¢ 13B ‚Ä¢ Updated Jul 10 ‚Ä¢ 1.71k ‚Ä¢ 8 mistralai/Devstral-Small-2505 24B ‚Ä¢ Updated 21 days ago ‚Ä¢ 11.9k ‚Ä¢ 846 MiniMaxAI/MiniMax-M1-80k Text Generation ‚Ä¢ 456B ‚Ä¢ Updated Jul 7 ‚Ä¢ 723 ‚Ä¢ ‚Ä¢ 671 unsloth/Mistral-Small-3.2-24B-Instruct-2506-unsloth-bnb-4bit Image-Text-to-Text ‚Ä¢ 13B ‚Ä¢ Updated Jun 23 ‚Ä¢ 11.1k ‚Ä¢ 9 lmstudio-community/gpt-oss-20b-MLX-8bit Text Generation ‚Ä¢ 21B ‚Ä¢ Updated Aug 5 ‚Ä¢ 1.",
    "id": "94005c11-5dd3-41cd-a272-a8f9eb00bd14",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2-24B-Instruct-2506-unsloth-bnb-4bit Image-Text-to-Text ‚Ä¢ 13B ‚Ä¢ Updated Jun 23 ‚Ä¢ 11.1k ‚Ä¢ 9 lmstudio-community/gpt-oss-20b-MLX-8bit Text Generation ‚Ä¢ 21B ‚Ä¢ Updated Aug 5 ‚Ä¢ 1.05M ‚Ä¢ 38 RedHatAI/gpt-oss-20b Text Generation ‚Ä¢ 22B ‚Ä¢ Updated 3 days ago ‚Ä¢ 17 ‚Ä¢ 2 mistralai/Mixtral-8x22B-Instruct-v0.1 141B ‚Ä¢ Updated Jul 24 ‚Ä¢ 9.9k ‚Ä¢ 732 Previous 1 2 3 ... 41 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: google/embeddinggemma-300m ¬∑ Hugging Face (https://huggingface.co/google/embeddinggemma-300m) === google/embeddinggemma-300m ¬∑ Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up google / embeddinggemma-300m like 439 Follow Google 28.1k Sentence Similarity sentence-transformers Safetensors gemma3_text feature-extraction text-embeddings-inference License: gemma Model card Files Files and versions xet Community 9 Train Deploy Use this model Access EmbeddingGemma on Hugging Face This repository is publicly accessible, but you have to accept the conditions to access its files and content. To access EmbeddingGemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately. Log in or Sign Up to review the conditions and access this model content.",
    "id": "9153bb43-8b73-4236-a245-aa31fa4f3130",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "but you have to accept the conditions to access its files and content. To access EmbeddingGemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately. Log in or Sign Up to review the conditions and access this model content. EmbeddingGemma model card Model Information Description Inputs and outputs Usage Model Data Training Dataset Data Preprocessing Model Development Hardware Software Evaluation Benchmark Results Prompt Instructions Usage and Limitations Intended Usage Limitations Ethical Considerations and Risks Benefits EmbeddingGemma model card Model Page: EmbeddingGemma Resources and Technical Documentation: Responsible Generative AI Toolkit EmbeddingGemma on Kaggle EmbeddingGemma on Vertex Model Garden Terms of Use: Terms Authors: Google DeepMind Model Information Description EmbeddingGemma is a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages. The small size and on-device focus makes it possible to deploy in environments with limited resources such as mobile phones, laptops, or desktops, democratizing access to state of the art AI models and helping foster innovation for everyone. Inputs and outputs Input: Text string, such as a question, a prompt, or a document to be embedded Maximum input context length of 2048 tokens Output: Numerical vector representations of input text data Output embedding dimension size of 768, with smaller options available (512, 256, or 128) via Matryoshka Representation Learning (MRL). MRL allows users to truncate the output embedding of size 768 to their desired size and then re-normalize for efficient and accurate representation.",
    "id": "4d3841d3-1083-4ef9-a13e-09b35fa07969",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "MRL allows users to truncate the output embedding of size 768 to their desired size and then re-normalize for efficient and accurate representation. Usage These model weights are designed to be used with Sentence Transformers, using the Gemma 3 implementation from Hugging Face Transformers as the backbone. First install the Sentence Transformers library: pip install -U sentence-transformers Then you can load this model and run inference. from sentence_transformers import SentenceTransformer Download from the ü§ó Hub model = SentenceTransformer(\"google/embeddinggemma-300m\") Run inference with queries and documents query = \"Which planet is known as the Red Planet?\" documents = [ \"Venus is often called Earth's twin because of its similar size and proximity.\", \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\", \"Jupiter, the largest planet in our solar system, has a prominent red spot.\", \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\" ] query_embeddings = model.encode_query(query) document_embeddings = model.encode_document(documents) print(query_embeddings.shape, document_embeddings.shape) (768,) (4, 768) Compute similarities to determine a ranking similarities = model.similarity(query_embeddings, document_embeddings) print(similarities) tensor([[0.3011, 0.6359, 0.4930, 0.4889]]) NOTE: EmbeddingGemma activations do not support float16. Please use float32 or bfloat16 as appropriate for your hardware. Model Data Training Dataset This model was trained on a dataset of text data that includes a wide variety of sources totaling approximately 320 billion tokens. Here are the key components: Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 100 languages. Code and Technical Documents: Exposing the model to code and technical documentation helps it learn the structure and patterns of programming languages and specialized scientific content, which improves its understanding of code and technical questions.",
    "id": "9e721818-0180-4926-ad7d-dba6ed0c912b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Here are the key components: Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 100 languages. Code and Technical Documents: Exposing the model to code and technical documentation helps it learn the structure and patterns of programming languages and specialized scientific content, which improves its understanding of code and technical questions. Synthetic and Task-Specific Data: Synthetically training data helps to teach the model specific skills. This includes curated data for tasks like information retrieval, classification, and sentiment analysis, which helps to fine-tune its performance for common embedding applications. The combination of these diverse data sources is crucial for training a powerful multilingual embedding model that can handle a wide variety of different tasks and data formats. Data Preprocessing Here are the key data cleaning and filtering methods applied to the training data: CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content. Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets. Additional methods: Filtering based on content quality and safety in line with our policies. Model Development Hardware EmbeddingGemma was trained using the latest generation of Tensor Processing Unit (TPU) hardware (TPUv5e), for more details refer to the Gemma 3 model card. Software Training was done using JAX and ML Pathways. For more details refer to the Gemma 3 model card. Evaluation Benchmark Results The model was evaluated against a large collection of different datasets and metrics to cover different aspects of text understanding. Full Precision Checkpoint MTEB (Multilingual, v2) Dimensionality Mean (Task) Mean (TaskType) 768d 61.15 54.31 512d 60.71 53.89 256d 59.68 53.01 128d 58.23 51.",
    "id": "9c1e3159-c93c-4d84-8724-13b41706d226",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Evaluation Benchmark Results The model was evaluated against a large collection of different datasets and metrics to cover different aspects of text understanding. Full Precision Checkpoint MTEB (Multilingual, v2) Dimensionality Mean (Task) Mean (TaskType) 768d 61.15 54.31 512d 60.71 53.89 256d 59.68 53.01 128d 58.23 51.77 MTEB (English, v2) Dimensionality Mean (Task) Mean (TaskType) 768d 68.36 64.15 512d 67.80 63.59 256d 66.89 62.94 128d 65.09 61.56 MTEB (Code, v1) Dimensionality Mean (Task) Mean (TaskType) 768d 68.76 68.76 512d 68.48 68.48 256d 66.74 66.74 128d 62.96 62.96 QAT Checkpoints MTEB (Multilingual, v2) Quant config (dimensionality) Mean (Task) Mean (TaskType) Q4_0 (768d) 60.62 53.61 Q8_0 (768d) 60.93 53.95 Mixed Precision (768d) 60.69 53.82 MTEB (English, v2) Quant config (dimensionality) Mean (Task) Mean (TaskType) Q4_0 (768d) 67.91 63.64 Q8_0 (768d) 68.13 63.85 Mixed Precision (768d) 67.95 63.83 MTEB (Code, v1) Quant config (dimensionality) Mean (Task) Mean (TaskType) Q4_0 (768d) 67.99 67.99 Q8_0 (768d) 68.70 68.70 Mixed Precision (768d) 68.03 68.",
    "id": "5fb2acd4-4627-4a74-9f7b-973f8f20f399",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "64 Q8_0 (768d) 68.13 63.85 Mixed Precision (768d) 67.95 63.83 MTEB (Code, v1) Quant config (dimensionality) Mean (Task) Mean (TaskType) Q4_0 (768d) 67.99 67.99 Q8_0 (768d) 68.70 68.70 Mixed Precision (768d) 68.03 68.03 Note: QAT models are evaluated after quantization  Mixed Precision refers to per-channel quantization with int4 for embeddings, feedforward, and projection layers, and int8 for attention (e4_a8_f4_p4). Prompt Instructions EmbeddingGemma can generate optimized embeddings for various use cases‚Äîsuch as document retrieval, question answering, and fact verification‚Äîor for specific input types‚Äîeither a query or a document‚Äîusing prompts that are prepended to the input strings. Query prompts follow the form task: {task description} | query: where the task description varies by the use case, with the default task description being search result. Document-style prompts follow the form title: {title | \"none\"} | text: where the title is either none (the default) or the actual title of the document. Note that providing a title, if available, will improve model performance for document prompts but may require manual formatting. Use the following prompts based on your use case and input data type. These may already be available in the EmbeddingGemma configuration in your modeling framework of choice.",
    "id": "bb725825-5b10-479d-9f66-b31153fced64",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Document-style prompts follow the form title: {title | \"none\"} | text: where the title is either none (the default) or the actual title of the document. Note that providing a title, if available, will improve model performance for document prompts but may require manual formatting. Use the following prompts based on your use case and input data type. These may already be available in the EmbeddingGemma configuration in your modeling framework of choice. Use Case (task type enum) Descriptions Recommended Prompt Retrieval (Query) Used to generate embeddings that are optimized for document search or information retrieval task: search result | query: {content} Retrieval (Document) title: {title | \"none\"} | text: {content} Question Answering task: question answering | query: {content} Fact Verification task: fact checking | query: {content} Classification Used to generate embeddings that are optimized to classify texts according to preset labels task: classification | query: {content} Clustering Used to generate embeddings that are optimized to cluster texts based on their similarities task: clustering | query: {content} Semantic Similarity Used to generate embeddings that are optimized to assess text similarity. This is not intended for retrieval use cases. task: sentence similarity | query: {content} Code Retrieval Used to retrieve a code block based on a natural language query, such as sort an array or reverse a linked list. Embeddings of the code blocks are computed using retrieval_document. task: code retrieval | query: {content} Usage and Limitations These models have certain limitations that users should be aware of. Intended Usage Open embedding models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development.",
    "id": "81300a87-8694-4c24-bf8b-dee9579a5768",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Embeddings of the code blocks are computed using retrieval_document. task: code retrieval | query: {content} Usage and Limitations These models have certain limitations that users should be aware of. Intended Usage Open embedding models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development. Semantic Similarity: Embeddings optimized to assess text similarity, such as recommendation systems and duplicate detection Classification: Embeddings optimized to classify texts according to preset labels, such as sentiment analysis and spam detection Clustering: Embeddings optimized to cluster texts based on their similarities, such as document organization, market research, and anomaly detection Retrieval Document: Embeddings optimized for document search, such as indexing articles, books, or web pages for search Query: Embeddings optimized for general search queries, such as custom search Code Query: Embeddings optimized for retrieval of code blocks based on natural language queries, such as code suggestions and search Question Answering: Embeddings for questions in a question-answering system, optimized for finding documents that answer the question, such as chatbox. Fact Verification: Embeddings for statements that need to be verified, optimized for retrieving documents that contain evidence supporting or refuting the statement, such as automated fact-checking systems. Limitations Training Data The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. The scope of the training dataset determines the subject areas the model can handle effectively. Language Ambiguity and Nuance Natural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language. Ethical Considerations and Risks Risks identified and mitigations: Perpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases.",
    "id": "b48e73e5-f6e2-4b30-bea6-4a0171d7d9c2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Language Ambiguity and Nuance Natural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language. Ethical Considerations and Risks Risks identified and mitigations: Perpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases. Misuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of embeddings. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in the Gemma Prohibited Use Policy. Privacy violations: Models were trained on data filtered for removal of certain personal information and other sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques. Benefits At the time of release, this family of models provides high-performance open embedding model implementations designed from the ground up for responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown superior performance to other, comparably-sized open model alternatives. Downloads last month50,949 Safetensors Model size 303M params Tensor type F32 ¬∑ Files info Inference Providers NEW Sentence Similarity This model isn't deployed by any Inference Provider.",
    "id": "c9f01514-5cd4-43a0-86b6-23888e794f19",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Using the benchmark evaluation metrics described in this document, these models have shown superior performance to other, comparably-sized open model alternatives. Downloads last month50,949 Safetensors Model size 303M params Tensor type F32 ¬∑ Files info Inference Providers NEW Sentence Similarity This model isn't deployed by any Inference Provider. üôã 3 Ask for provider support Model tree for google/embeddinggemma-300m Finetunes 31 models Quantizations 7 models Spaces using google/embeddinggemma-300m 8 ü•á mteb/leaderboard google/mood-palette üìä akhaliq/embeddinggemma-300m üè¢ Mohaddz/Customer-classify üï∏Ô∏è bebechien/hollow-knight-helper üí¨ yogies/chat-guide üí¨ yogies/jvbot üê† fdaudens/EmbeddingGemma-Wikipedia + 3 Spaces Collections including google/embeddinggemma-300m Google's Gemma models family Collection 326 items ‚Ä¢ Updated 4 days ago ‚Ä¢ 468 EmbeddingGemma Collection 3 items ‚Ä¢ Updated 4 days ago ‚Ä¢ 62 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Models available on Fireworks ‚Äì Hugging Face (https://huggingface.co/models?inference_provider=fireworks-ai) === Models available on Fireworks ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks Libraries Languages Licenses Other 1 Tasks Text Generation Any-to-Any Image-Text-to-Text Image-to-Text Image-to-Image Text-to-Image Text-to-Video Text-to-Speech + 42Parameters Reset Parameters < 1B 6B 12B 32B 128B > 500B < 1B > 500B Libraries PyTorch google-tensorflow TensorFlow JAX Transformers Diffusers Safetensors ONNX GGUF Transformers.js MLX Keras + 41 Apps vLLM TGI llama.",
    "id": "e4cd3fad-9739-4c6b-aa23-f6828db054ba",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "js MLX Keras + 41 Apps vLLM TGI llama.cpp MLX LM LM Studio Ollama Jan + 13 Inference Providers Reset Inference Providers Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale + 6 Apply filters Models 25 Full-text search Edit filters Sort: Trending Active filters: fireworks-ai Clear all openai/gpt-oss-120b Text Generation ‚Ä¢ 120B ‚Ä¢ Updated 13 days ago ‚Ä¢ 3M ‚Ä¢ ‚Ä¢ 3.78k openai/gpt-oss-20b Text Generation ‚Ä¢ 22B ‚Ä¢ Updated 13 days ago ‚Ä¢ 8.93M ‚Ä¢ ‚Ä¢ 3.44k deepseek-ai/DeepSeek-V3.1 Text Generation ‚Ä¢ 685B ‚Ä¢ Updated 3 days ago ‚Ä¢ 146k ‚Ä¢ ‚Ä¢ 723 Qwen/Qwen3-Coder-480B-A35B-Instruct Text Generation ‚Ä¢ 480B ‚Ä¢ Updated 18 days ago ‚Ä¢ 229k ‚Ä¢ ‚Ä¢ 1.17k Qwen/Qwen3-Coder-30B-A3B-Instruct Text Generation ‚Ä¢ 31B ‚Ä¢ Updated 18 days ago ‚Ä¢ 320k ‚Ä¢ ‚Ä¢ 559 meta-llama/Llama-3.1-8B-Instruct Text Generation ‚Ä¢ 8B ‚Ä¢ Updated Sep 25, 2024 ‚Ä¢ 8.48M ‚Ä¢ ‚Ä¢ 4.58k moonshotai/Kimi-K2-Instruct Text Generation ‚Ä¢ Updated 3 days ago ‚Ä¢ 403k ‚Ä¢ ‚Ä¢ 2.14k deepseek-ai/DeepSeek-R1 Text Generation ‚Ä¢ 685B ‚Ä¢ Updated Mar 27 ‚Ä¢ 368k ‚Ä¢ ‚Ä¢ 12.7k zai-org/GLM-4.5-Air Text Generation ‚Ä¢ 110B ‚Ä¢ Updated 28 days ago ‚Ä¢ 297k ‚Ä¢ ‚Ä¢ 423 zai-org/GLM-4.5 Text Generation ‚Ä¢ 358B ‚Ä¢ Updated 28 days ago ‚Ä¢ 94.2k ‚Ä¢ ‚Ä¢ 1.",
    "id": "a6a82167-3c6e-49d7-9d0f-6d2128d6f3d0",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "14k deepseek-ai/DeepSeek-R1 Text Generation ‚Ä¢ 685B ‚Ä¢ Updated Mar 27 ‚Ä¢ 368k ‚Ä¢ ‚Ä¢ 12.7k zai-org/GLM-4.5-Air Text Generation ‚Ä¢ 110B ‚Ä¢ Updated 28 days ago ‚Ä¢ 297k ‚Ä¢ ‚Ä¢ 423 zai-org/GLM-4.5 Text Generation ‚Ä¢ 358B ‚Ä¢ Updated 28 days ago ‚Ä¢ 94.2k ‚Ä¢ ‚Ä¢ 1.29k meta-llama/Llama-4-Scout-17B-16E-Instruct Image-Text-to-Text ‚Ä¢ 109B ‚Ä¢ Updated May 22 ‚Ä¢ 762k ‚Ä¢ ‚Ä¢ 1.08k Qwen/Qwen3-235B-A22B-Instruct-2507 Text Generation ‚Ä¢ 235B ‚Ä¢ Updated 22 days ago ‚Ä¢ 97k ‚Ä¢ ‚Ä¢ 672 Qwen/Qwen3-235B-A22B-Thinking-2507 Text Generation ‚Ä¢ 235B ‚Ä¢ Updated 22 days ago ‚Ä¢ 55.2k ‚Ä¢ ‚Ä¢ 342 deepseek-ai/DeepSeek-R1-0528 Text Generation ‚Ä¢ 685B ‚Ä¢ Updated May 29 ‚Ä¢ 887k ‚Ä¢ ‚Ä¢ 2.36k meta-llama/Llama-3.1-70B-Instruct Text Generation ‚Ä¢ 71B ‚Ä¢ Updated Dec 15, 2024 ‚Ä¢ 860k ‚Ä¢ ‚Ä¢ 844 meta-llama/Llama-3.3-70B-Instruct Text Generation ‚Ä¢ 71B ‚Ä¢ Updated Dec 21, 2024 ‚Ä¢ 520k ‚Ä¢ ‚Ä¢ 2.48k Qwen/Qwen2.5-VL-32B-Instruct Image-Text-to-Text ‚Ä¢ 33B ‚Ä¢ Updated Apr 14 ‚Ä¢ 655k ‚Ä¢ ‚Ä¢ 438 Qwen/Qwen3-235B-A22B Text Generation ‚Ä¢ 235B ‚Ä¢ Updated Jul 26 ‚Ä¢ 144k ‚Ä¢ ‚Ä¢ 1.03k deepseek-ai/DeepSeek-V3 Text Generation ‚Ä¢ 685B ‚Ä¢ Updated Mar 27 ‚Ä¢ 403k ‚Ä¢ ‚Ä¢ 3.",
    "id": "72ca951d-f1f7-4dbd-aab7-70be6ee41657",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "5-VL-32B-Instruct Image-Text-to-Text ‚Ä¢ 33B ‚Ä¢ Updated Apr 14 ‚Ä¢ 655k ‚Ä¢ ‚Ä¢ 438 Qwen/Qwen3-235B-A22B Text Generation ‚Ä¢ 235B ‚Ä¢ Updated Jul 26 ‚Ä¢ 144k ‚Ä¢ ‚Ä¢ 1.03k deepseek-ai/DeepSeek-V3 Text Generation ‚Ä¢ 685B ‚Ä¢ Updated Mar 27 ‚Ä¢ 403k ‚Ä¢ ‚Ä¢ 3.97k meta-llama/Llama-3.1-405B-Instruct Text Generation ‚Ä¢ 406B ‚Ä¢ Updated Sep 25, 2024 ‚Ä¢ 28.5k ‚Ä¢ ‚Ä¢ 578 deepseek-ai/DeepSeek-V3-0324 Text Generation ‚Ä¢ 685B ‚Ä¢ Updated Mar 27 ‚Ä¢ 297k ‚Ä¢ ‚Ä¢ 3.06k meta-llama/Llama-4-Maverick-17B-128E-Instruct Image-Text-to-Text ‚Ä¢ 402B ‚Ä¢ Updated May 22 ‚Ä¢ 28.8k ‚Ä¢ ‚Ä¢ 398 Qwen/Qwen3-30B-A3B Text Generation ‚Ä¢ 31B ‚Ä¢ Updated Jul 26 ‚Ä¢ 719k ‚Ä¢ ‚Ä¢ 773 mistralai/Mixtral-8x22B-Instruct-v0.1 141B ‚Ä¢ Updated Jul 24 ‚Ä¢ 9.9k ‚Ä¢ 732 SentientAGI/Dobby-Unhinged-Llama-3.3-70B Text Generation ‚Ä¢ 71B ‚Ä¢ Updated Feb 12 ‚Ä¢ 1.21k ‚Ä¢ ‚Ä¢ 41 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Models ‚Äì Hugging Face (https://huggingface.co/models?other=lmstudio) === Models ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks Libraries Languages Licenses Other 1 Apps llama.",
    "id": "2673d8f5-3f72-4275-b232-ac3136f08d22",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "3-70B Text Generation ‚Ä¢ 71B ‚Ä¢ Updated Feb 12 ‚Ä¢ 1.21k ‚Ä¢ ‚Ä¢ 41 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Models ‚Äì Hugging Face (https://huggingface.co/models?other=lmstudio) === Models ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks Libraries Languages Licenses Other 1 Apps llama.cpp LM Studio Jan Backyard AI Draw Things DiffusionBee Jellybox RecurseChat Msty Sanctum Invoke JoyFusion LocalAI vLLM node-llama-cpp Ollama TGI MLX LM Docker Model Runner Lemonade Inference Providers Select all Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale SambaNova fal Featherless AI Cohere Replicate HF Inference API Misc Reset Misc lmstudio Inference Endpoints text-generation-inference Eval Results Merge 4-bit precision custom_code 8-bit precision text-embeddings-inference Mixture of Experts Carbon Emissions Apply filters Models 7 Full-text search Edit filters Sort: Trending Active filters: lmstudio Clear all DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters Updated Jul 27 ‚Ä¢ 132 r3b31/Qwen2-VL-7B-Captioner-Relaxed-GGUF Image-to-Text ‚Ä¢ 8B ‚Ä¢ Updated Mar 5 ‚Ä¢ 239 ‚Ä¢ 3 danlou/relay-v0.1-Mistral-Nemo-2407-GGUF Text Generation ‚Ä¢ 12B ‚Ä¢ Updated Dec 14, 2024 ‚Ä¢ 42 DavidAU/DeepHermes-3-Llama-3-8B-Preview-16.",
    "id": "0623a360-8b71-4c7a-b7ba-2af165a6151c",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "1-Mistral-Nemo-2407-GGUF Text Generation ‚Ä¢ 12B ‚Ä¢ Updated Dec 14, 2024 ‚Ä¢ 42 DavidAU/DeepHermes-3-Llama-3-8B-Preview-16.5B-Brainstorm-gguf Text Generation ‚Ä¢ 17B ‚Ä¢ Updated Mar 3 ‚Ä¢ 159 ‚Ä¢ 3 DavidAU/How-To-Use-Reasoning-Thinking-Models-and-Create-Them Text Generation ‚Ä¢ Updated Jul 27 ‚Ä¢ 9 DavidAU/How-To-Set-and-Manage-MOE-Mix-of-Experts-Model-Activation-of-Experts Text Generation ‚Ä¢ Updated Jul 27 ‚Ä¢ 8 mistralai/Devstral-Small-2505_gguf 24B ‚Ä¢ Updated Jul 24 ‚Ä¢ 1.15k ‚Ä¢ 75 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Hugging Face ‚Äì Pricing (https://huggingface.co/pricing) === Hugging Face ‚Äì Pricing Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Pricing Leveling up AI collaboration and compute. Give your personal account or your organization the most advanced platform to build AI.",
    "id": "2c7b43f6-fa33-447c-b2bf-bb0a9cf80a94",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "15k ‚Ä¢ 75 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Hugging Face ‚Äì Pricing (https://huggingface.co/pricing) === Hugging Face ‚Äì Pricing Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Pricing Leveling up AI collaboration and compute. Give your personal account or your organization the most advanced platform to build AI. PRO PRO Account Boost your personal HF experience Subscribe for $9 per month Get PRO 10√ó private storage capacity 20√ó included inference credits 8√ó ZeroGPU quota and highest queue priority Spaces Dev Mode & ZeroGPU Spaces hosting Publish blog articles on your HF profile Dataset Viewer for private datasets Show your support with a Pro badge Team Instant setup for growing teams Subscribe for $20 per user per month Get Team (via credit card) SSO and SAML support Choose data location with Storage Regions Detailed action reviews with Audit Logs Granular access control via Resource Groups Repository usage Analytics Set auth policies and default repository visibility Centralized token control and approvals Dataset Viewer for private datasets Advanced compute options for Spaces All organization members get ZeroGPU and Inference Providers PRO benefits Enterprise Custom onboarding and enterprise features Starting at $50 per user per month Contact Sales All benefits from the Team plan Managed billing with annual commitments Legal and Compliance processes Personalized support Need support to adopt the HF Hub in your organization? View our Expert Support. Hugging Face Hub free The HF Hub is the central place to explore, experiment, collaborate and build technology with Machine Learning. Join the open source Machine Learning movement! ‚Üí Sign Up Create with ML Packed with ML features, like model eval, dataset viewer and much more. Collaborate Git based and designed for collaboration at its core. Play and learn Learn by experimenting and sharing with our awesome community. Build your ML portfolio Share your work with the world and build your own ML profile. Spaces Hardware Starting at $0 Spaces are one of the most popular ways to share ML applications and demos with the world.",
    "id": "17507b46-af43-48b2-b6eb-d0ee730713a3",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "‚Üí Sign Up Create with ML Packed with ML features, like model eval, dataset viewer and much more. Collaborate Git based and designed for collaboration at its core. Play and learn Learn by experimenting and sharing with our awesome community. Build your ML portfolio Share your work with the world and build your own ML profile. Spaces Hardware Starting at $0 Spaces are one of the most popular ways to share ML applications and demos with the world. Upgrade your Spaces with our selection of custom on-demand hardware: ‚Üí Get started with Spaces Name CPU Memory Accelerator VRAM Hourly price CPU Basic 2 vCPU 16 GB - - FREE CPU Upgrade 8 vCPU 32 GB - - $0.03 Nvidia T4 - small 4 vCPU 15 GB Nvidia T4 16 GB $0.40 Nvidia T4 - medium 8 vCPU 30 GB Nvidia T4 16 GB $0.60 1x Nvidia L4 8 vCPU 30 GB Nvidia L4 24 GB $0.80 4x Nvidia L4 48 vCPU 186 GB Nvidia L4 96 GB $3.80 1x Nvidia L40S 8 vCPU 62 GB Nvidia L4 48 GB $1.80 4x Nvidia L40S 48 vCPU 382 GB Nvidia L4 192 GB $8.30 8x Nvidia L40S 192 vCPU 1534 GB Nvidia L4 384 GB $23.50 Nvidia A10G - small 4 vCPU 15 GB Nvidia A10G 24 GB $1.00 Nvidia A10G - large 12 vCPU 46 GB Nvidia A10G 24 GB $1.50 2x Nvidia A10G - large 24 vCPU 92 GB Nvidia A10G 48 GB $3.00 4x Nvidia A10G - large 48 vCPU 184 GB Nvidia A10G 96 GB $5.00 Nvidia A100 - large 12 vCPU 142 GB Nvidia A100 80 GB $2.50 Custom on demand on demand on demand on demand on demand ZeroGPU dynamic dynamic Nvidia H200 70 GB FREE Spaces Persistent Storage All Spaces get ephemeral storage for free but you can upgrade and add persistent storage at any time.",
    "id": "52c14487-b654-4823-8f5e-d25c7c2e76ce",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Name Storage Monthly price Small 20 GB $5 Medium 150 GB $25 Large 1 TB $100 Building something cool as a side project? We also offer community GPU grants. Inference Endpoints Starting at $0.033/hour Inference Endpoints (dedicated) offers a secure production solution to easily deploy any ML model on dedicated and autoscaling infrastructure, right from the HF Hub. ‚ÜíLearn more CPU instances Provider Architecture vCPUs Memory Hourly rate aws Intel Sapphire Rapids 1 2GB $0.03 2 4GB $0.07 4 8GB $0.13 8 16GB $0.27 16 32GB $0.54 aws Intel Sapphire Rapids (overcommit) 16 32GB $0.01 azure Intel Xeon 1 2GB $0.06 2 4GB $0.12 4 8GB $0.24 8 16GB $0.48 gcp Intel Sapphire Rapids 1 2GB $0.05 2 4GB $0.10 4 8GB $0.20 8 16GB $0.40 Accelerator instances Provider Architecture Topology Accelerator Memory Hourly rate aws Inf2 Neuron x1 14.5GB $0.75 x12 760GB $12.00 gcp TPU v5e 1x1 16GB $1.20 2x2 64GB $4.75 2x4 128GB $9.50 GPU instances Provider Architecture GPUs GPU Memory Hourly rate aws NVIDIA T4 1 14GB $0.50 4 56GB $3.00 aws NVIDIA L4 1 24GB $0.80 4 96GB $3.80 aws NVIDIA L40S 1 48GB $1.80 4 192GB $8.30 8 384GB $23.50 aws NVIDIA A10G 1 24GB $1.00 4 96GB $5.",
    "id": "fa3c434c-3395-4293-a860-180de6ae1a16",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "50 4 56GB $3.00 aws NVIDIA L4 1 24GB $0.80 4 96GB $3.80 aws NVIDIA L40S 1 48GB $1.80 4 192GB $8.30 8 384GB $23.50 aws NVIDIA A10G 1 24GB $1.00 4 96GB $5.00 aws NVIDIA A100 1 80GB $2.50 2 160GB $5.00 4 320GB $10.00 8 640GB $20.00 aws NVIDIA H200 1 141GB $5.00 2 282GB $10.00 4 564GB $20.00 8 1128GB $40.00 gcp NVIDIA T4 1 16GB $0.50 gcp NVIDIA L4 1 24GB $0.70 4 96GB $3.80 gcp NVIDIA A100 1 80GB $3.60 2 160GB $7.20 4 320GB $14.40 8 640GB $28.80 gcp NVIDIA H100 1 80GB $10.00 2 160GB $20.00 4 320GB $40.00 8 640GB $80.00 PRO Account PRO A monthly subscription to access powerful features.",
    "id": "72074731-4075-4dc2-978b-17d739ad1f46",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "80 gcp NVIDIA A100 1 80GB $3.60 2 160GB $7.20 4 320GB $14.40 8 640GB $28.80 gcp NVIDIA H100 1 80GB $10.00 2 160GB $20.00 4 320GB $40.00 8 640GB $80.00 PRO Account PRO A monthly subscription to access powerful features. ‚Üí Get PRO ($9/month) Inference Providers: Get 20√ó included inference credits ZeroGPU: Get 8√ó usage quota and highest priority in queues Spaces Hosting: Create ZeroGPU Spaces with H200 hardware Spaces Dev Mode: Fast iterations via SSH/VS Code for Spaces Dataset Viewer: Activate and use it on private datasets Blog Articles: Publish articles on your HF profile Features Preview: Get early access to upcoming features PRO Badge: Show your support on your profile System theme Website Models Datasets Spaces Changelog Inference Endpoints HuggingChat Company About Brand assets Terms of service Privacy Jobs Press Resources Learn Documentation Blog Forum Service Status Social GitHub Twitter LinkedIn Discord === PAGE: Tokenizers (https://huggingface.co/docs/tokenizers) === Tokenizers Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Tokenizers documentation Tokenizers Tokenizers üè° View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm Search documentation mainv0.20.3v0.13.4.rc2v0.10.0v0.9.4 EN Getting started ü§ó Tokenizers Quicktour Installation The tokenization pipeline Components Training from memory API Input Sequences Encode Inputs Tokenizer Encoding Added Tokens Models Normalizers Pre-tokenizers Post-processors Trainers Decoders Visualizer Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Tokenizers Fast State-of-the-art tokenizers, optimized for both research and production ü§ó Tokenizers provides an implementation of today‚Äôs most used tokenizers, with a focus on performance and versatility.",
    "id": "f9ab270e-313d-4e16-8856-bb7bee8d2e8a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "These tokenizers are also used in ü§ó Transformers. Main features: Train new vocabularies and tokenize, using today‚Äôs most used tokenizers. Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server‚Äôs CPU. Easy to use, but also extremely versatile. Designed for both research and production. Full alignment tracking. Even with destructive normalization, it‚Äôs always possible to get the part of the original sentence that corresponds to any token. Does all the pre-processing: Truncation, Padding, add the special tokens your model needs. < > Update on GitHub Quicktour‚Üí Tokenizers === PAGE: Qwen/Qwen-Image-Edit ¬∑ Hugging Face (https://huggingface.co/Qwen/Qwen-Image-Edit) === Qwen/Qwen-Image-Edit ¬∑ Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Qwen / Qwen-Image-Edit like 1.71k Follow Qwen 48.1k Image-to-Image Diffusers Safetensors English Chinese QwenImageEditPipeline arxiv: 2508.02324 License: apache-2.0 Model card Files Files and versions xet Community 29 Deploy Use this model Introduction Quick Start Showcase License Agreement Citation Join Us üíú Qwen Chat | ü§ó Hugging Face | ü§ñ ModelScope | üìë Tech Report | üìë Blog üñ•Ô∏è Demo | üí¨ WeChat (ÂæÆ‰ø°) | ü´® Discord | Github Introduction We are excited to introduce Qwen-Image-Edit, the image editing version of Qwen-Image. Built upon our 20B Qwen-Image model, Qwen-Image-Edit successfully extends Qwen-Image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing.",
    "id": "48bb5a91-9f0b-4a83-a5af-22ad2670da24",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Built upon our 20B Qwen-Image model, Qwen-Image-Edit successfully extends Qwen-Image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. Furthermore, Qwen-Image-Edit simultaneously feeds the input image into Qwen2.5-VL (for visual semantic control) and the VAE Encoder (for visual appearance control), achieving capabilities in both semantic and appearance editing. To experience the latest model, visit Qwen Chat and select the \"Image Editing\" feature. Key Features: Semantic and Appearance Editing: Qwen-Image-Edit supports both low-level visual appearance editing (such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged) and high-level visual semantic editing (such as IP creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency). Precise Text Editing: Qwen-Image-Edit supports bilingual (Chinese and English) text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style. Strong Benchmark Performance: Evaluations on multiple public benchmarks demonstrate that Qwen-Image-Edit achieves state-of-the-art (SOTA) performance in image editing tasks, establishing it as a powerful foundation model for image editing. Quick Start Install the latest version of diffusers pip install git+https://github.com/huggingface/diffusers The following contains a code snippet illustrating how to use the model to generate images based on text prompts: import os from PIL import Image import torch from diffusers import QwenImageEditPipeline pipeline = QwenImageEditPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\") print(\"pipeline loaded\") pipeline.to(torch.bfloat16) pipeline.to(\"cuda\") pipeline.set_progress_bar_config(disable=None) image = Image.open(\"./input.png\").convert(\"RGB\") prompt = \"Change the rabbit's color to purple, with a flash light background.\"",
    "id": "9f94c42c-10f8-4431-8246-ba78056bc41f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "inputs = { \"image\": image, \"prompt\": prompt, \"generator\": torch.manual_seed(0), \"true_cfg_scale\": 4.0, \"negative_prompt\": \" \", \"num_inference_steps\": 50, } with torch.inference_mode(): output = pipeline(inputs) output_image = output.images[0] output_image.save(\"output_image_edit.png\") print(\"image saved at\", os.path.abspath(\"output_image_edit.png\")) Showcase One of the highlights of Qwen-Image-Edit lies in its powerful capabilities for semantic and appearance editing. Semantic editing refers to modifying image content while preserving the original visual semantics. To intuitively demonstrate this capability, let's take Qwen's mascot‚ÄîCapybara‚Äîas an example: As can be seen, although most pixels in the edited image differ from those in the input image (the leftmost image), the character consistency of Capybara is perfectly preserved. Qwen-Image-Edit's powerful semantic editing capability enables effortless and diverse creation of original IP content. Furthermore, on Qwen Chat, we designed a series of editing prompts centered around the 16 MBTI personality types. Leveraging these prompts, we successfully created a set of MBTI-themed emoji packs based on our mascot Capybara, effortlessly expanding the IP's reach and expression. Moreover, novel view synthesis is another key application scenario in semantic editing. As shown in the two example images below, Qwen-Image-Edit can not only rotate objects by 90 degrees, but also perform a full 180-degree rotation, allowing us to directly see the back side of the object: Another typical application of semantic editing is style transfer. For instance, given an input portrait, Qwen-Image-Edit can easily transform it into various artistic styles such as Studio Ghibli. This capability holds significant value in applications like virtual avatar creation: In addition to semantic editing, appearance editing is another common image editing requirement. Appearance editing emphasizes keeping certain regions of the image completely unchanged while adding, removing, or modifying specific elements.",
    "id": "44cea23c-f7c6-4588-9237-3add7fe291b6",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "For instance, given an input portrait, Qwen-Image-Edit can easily transform it into various artistic styles such as Studio Ghibli. This capability holds significant value in applications like virtual avatar creation: In addition to semantic editing, appearance editing is another common image editing requirement. Appearance editing emphasizes keeping certain regions of the image completely unchanged while adding, removing, or modifying specific elements. The image below illustrates a case where a signboard is added to the scene. As shown, Qwen-Image-Edit not only successfully inserts the signboard but also generates a corresponding reflection, demonstrating exceptional attention to detail. Below is another interesting example, demonstrating how to remove fine hair strands and other small objects from an image. Additionally, the color of a specific letter \"n\" in the image can be modified to blue, enabling precise editing of particular elements. Appearance editing also has wide-ranging applications in scenarios such as adjusting a person's background or changing clothing. The three images below demonstrate these practical use cases respectively. Another standout feature of Qwen-Image-Edit is its accurate text editing capability, which stems from Qwen-Image's deep expertise in text rendering. As shown below, the following two cases vividly demonstrate Qwen-Image-Edit's powerful performance in editing English text: Qwen-Image-Edit can also directly edit Chinese posters, enabling not only modifications to large headline text but also precise adjustments to even small and intricate text elements. Finally, let's walk through a concrete image editing example to demonstrate how to use a chained editing approach to progressively correct errors in a calligraphy artwork generated by Qwen-Image: In this artwork, several Chinese characters contain generation errors. We can leverage Qwen-Image-Edit to correct them step by step. For instance, we can draw bounding boxes on the original image to mark the regions that need correction, instructing Qwen-Image-Edit to fix these specific areas. Here, we want the character \"Á®Ω\" to be correctly written within the red box, and the character \"‰∫≠\" to be accurately rendered in the blue region.",
    "id": "65ec8196-b33d-4bbe-970e-1687b4cd683e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "We can leverage Qwen-Image-Edit to correct them step by step. For instance, we can draw bounding boxes on the original image to mark the regions that need correction, instructing Qwen-Image-Edit to fix these specific areas. Here, we want the character \"Á®Ω\" to be correctly written within the red box, and the character \"‰∫≠\" to be accurately rendered in the blue region. However, in practice, the character \"Á®Ω\" is relatively obscure, and the model fails to correct it correctly in one step. The lower-right component of \"Á®Ω\" should be \"Êó®\" rather than \"Êó•\". At this point, we can further highlight the \"Êó•\" portion with a red box, instructing Qwen-Image-Edit to fine-tune this detail and replace it with \"Êó®\". Isn't it amazing? With this chained, step-by-step editing approach, we can continuously correct character errors until the desired final result is achieved. Finally, we have successfully obtained a completely correct calligraphy version of Lantingji Xu (Orchid Pavilion Preface)! In summary, we hope that Qwen-Image-Edit can further advance the field of image generation, truly lower the technical barriers to visual content creation, and inspire even more innovative applications. License Agreement Qwen-Image is licensed under Apache 2.0. Citation We kindly encourage citation of our work if you find it useful.",
    "id": "9b76e891-4fb4-4aea-a8ae-e4b6b9ad734e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Finally, we have successfully obtained a completely correct calligraphy version of Lantingji Xu (Orchid Pavilion Preface)! In summary, we hope that Qwen-Image-Edit can further advance the field of image generation, truly lower the technical barriers to visual content creation, and inspire even more innovative applications. License Agreement Qwen-Image is licensed under Apache 2.0. Citation We kindly encourage citation of our work if you find it useful. @misc{wu2025qwenimagetechnicalreport, title={Qwen-Image Technical Report}, author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu}, year={2025}, eprint={2508.02324}, archivePrefix={arXiv}, primaryClass={cs.CV}, url={https://arxiv.org/abs/2508.02324}, } Join Us If you're passionate about fundamental research, we're hiring full-time employees (FTEs) and research interns.",
    "id": "d1b3fb3f-fdd5-4a88-86a2-9cda171269eb",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Don't wait ‚Äî reach out to us at fulai.hr@alibaba-inc.com Downloads last month146,728 Inference Providers NEW fal Image-to-Image Drag image file here or click to browse from your device Browse for image (Optional) Text-guidance if the model has support for it Generate View Code Snippets Maximize Model tree for Qwen/Qwen-Image-Edit Adapters 13 models Finetunes 4 models Merges 1 model Quantizations 6 models Spaces using Qwen/Qwen-Image-Edit 100 Qwen/Qwen-Image-Edit ‚úíÔ∏èüí® multimodalart/Qwen-Image-Edit-Fast ‚úíÔ∏è linoyts/Qwen-Image-Edit-Inpaint üí° zerogpu-aoti/Qwen-Image-Edit-Relight ü•∏ aiqtech/kofaceid üåñ zerogpu-aoti/Qwen-Image-Edit-Outpaint üëÅ llamameta/nano-banana-experimental ‚ö° LPX55/Qwen-Image-Edit_Fast-Presets üåñ ginigen/Nano-Banana-PRO üåÖüèûÔ∏è zerogpu-aoti/Qwen-Image-Edit-Multi-Image üìä VirtualKimi/Nano-Banana ‚ö° umint/searchgpt + 95 Spaces + 88 Spaces Collection including Qwen/Qwen-Image-Edit Qwen-Image Collection 5 items ‚Ä¢ Updated about 14 hours ago ‚Ä¢ 1 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Hugging Face ‚Äì Pricing (https://huggingface.co/pricingendpoints) === Hugging Face ‚Äì Pricing Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Pricing Leveling up AI collaboration and compute. Give your personal account or your organization the most advanced platform to build AI.",
    "id": "34b5ac2e-d14d-4f7a-80d3-bb610fca102d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Give your personal account or your organization the most advanced platform to build AI. PRO PRO Account Boost your personal HF experience Subscribe for $9 per month Get PRO 10√ó private storage capacity 20√ó included inference credits 8√ó ZeroGPU quota and highest queue priority Spaces Dev Mode & ZeroGPU Spaces hosting Publish blog articles on your HF profile Dataset Viewer for private datasets Show your support with a Pro badge Team Instant setup for growing teams Subscribe for $20 per user per month Get Team (via credit card) SSO and SAML support Choose data location with Storage Regions Detailed action reviews with Audit Logs Granular access control via Resource Groups Repository usage Analytics Set auth policies and default repository visibility Centralized token control and approvals Dataset Viewer for private datasets Advanced compute options for Spaces All organization members get ZeroGPU and Inference Providers PRO benefits Enterprise Custom onboarding and enterprise features Starting at $50 per user per month Contact Sales All benefits from the Team plan Managed billing with annual commitments Legal and Compliance processes Personalized support Need support to adopt the HF Hub in your organization? View our Expert Support. Hugging Face Hub free The HF Hub is the central place to explore, experiment, collaborate and build technology with Machine Learning. Join the open source Machine Learning movement! ‚Üí Sign Up Create with ML Packed with ML features, like model eval, dataset viewer and much more. Collaborate Git based and designed for collaboration at its core. Play and learn Learn by experimenting and sharing with our awesome community. Build your ML portfolio Share your work with the world and build your own ML profile. Spaces Hardware Starting at $0 Spaces are one of the most popular ways to share ML applications and demos with the world.",
    "id": "52615b14-d0ff-4976-a889-7dcc7d06446a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "‚Üí Sign Up Create with ML Packed with ML features, like model eval, dataset viewer and much more. Collaborate Git based and designed for collaboration at its core. Play and learn Learn by experimenting and sharing with our awesome community. Build your ML portfolio Share your work with the world and build your own ML profile. Spaces Hardware Starting at $0 Spaces are one of the most popular ways to share ML applications and demos with the world. Upgrade your Spaces with our selection of custom on-demand hardware: ‚Üí Get started with Spaces Name CPU Memory Accelerator VRAM Hourly price CPU Basic 2 vCPU 16 GB - - FREE CPU Upgrade 8 vCPU 32 GB - - $0.03 Nvidia T4 - small 4 vCPU 15 GB Nvidia T4 16 GB $0.40 Nvidia T4 - medium 8 vCPU 30 GB Nvidia T4 16 GB $0.60 1x Nvidia L4 8 vCPU 30 GB Nvidia L4 24 GB $0.80 4x Nvidia L4 48 vCPU 186 GB Nvidia L4 96 GB $3.80 1x Nvidia L40S 8 vCPU 62 GB Nvidia L4 48 GB $1.80 4x Nvidia L40S 48 vCPU 382 GB Nvidia L4 192 GB $8.30 8x Nvidia L40S 192 vCPU 1534 GB Nvidia L4 384 GB $23.50 Nvidia A10G - small 4 vCPU 15 GB Nvidia A10G 24 GB $1.00 Nvidia A10G - large 12 vCPU 46 GB Nvidia A10G 24 GB $1.50 2x Nvidia A10G - large 24 vCPU 92 GB Nvidia A10G 48 GB $3.00 4x Nvidia A10G - large 48 vCPU 184 GB Nvidia A10G 96 GB $5.00 Nvidia A100 - large 12 vCPU 142 GB Nvidia A100 80 GB $2.50 Custom on demand on demand on demand on demand on demand ZeroGPU dynamic dynamic Nvidia H200 70 GB FREE Spaces Persistent Storage All Spaces get ephemeral storage for free but you can upgrade and add persistent storage at any time.",
    "id": "448579fa-5385-491b-af17-0507595dc871",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Name Storage Monthly price Small 20 GB $5 Medium 150 GB $25 Large 1 TB $100 Building something cool as a side project? We also offer community GPU grants. Inference Endpoints Starting at $0.033/hour Inference Endpoints (dedicated) offers a secure production solution to easily deploy any ML model on dedicated and autoscaling infrastructure, right from the HF Hub. ‚ÜíLearn more CPU instances Provider Architecture vCPUs Memory Hourly rate aws Intel Sapphire Rapids 1 2GB $0.03 2 4GB $0.07 4 8GB $0.13 8 16GB $0.27 16 32GB $0.54 aws Intel Sapphire Rapids (overcommit) 16 32GB $0.01 azure Intel Xeon 1 2GB $0.06 2 4GB $0.12 4 8GB $0.24 8 16GB $0.48 gcp Intel Sapphire Rapids 1 2GB $0.05 2 4GB $0.10 4 8GB $0.20 8 16GB $0.40 Accelerator instances Provider Architecture Topology Accelerator Memory Hourly rate aws Inf2 Neuron x1 14.5GB $0.75 x12 760GB $12.00 gcp TPU v5e 1x1 16GB $1.20 2x2 64GB $4.75 2x4 128GB $9.50 GPU instances Provider Architecture GPUs GPU Memory Hourly rate aws NVIDIA T4 1 14GB $0.50 4 56GB $3.00 aws NVIDIA L4 1 24GB $0.80 4 96GB $3.80 aws NVIDIA L40S 1 48GB $1.80 4 192GB $8.30 8 384GB $23.50 aws NVIDIA A10G 1 24GB $1.00 4 96GB $5.",
    "id": "d5851dab-8363-45d4-af50-8154662d24d2",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "50 4 56GB $3.00 aws NVIDIA L4 1 24GB $0.80 4 96GB $3.80 aws NVIDIA L40S 1 48GB $1.80 4 192GB $8.30 8 384GB $23.50 aws NVIDIA A10G 1 24GB $1.00 4 96GB $5.00 aws NVIDIA A100 1 80GB $2.50 2 160GB $5.00 4 320GB $10.00 8 640GB $20.00 aws NVIDIA H200 1 141GB $5.00 2 282GB $10.00 4 564GB $20.00 8 1128GB $40.00 gcp NVIDIA T4 1 16GB $0.50 gcp NVIDIA L4 1 24GB $0.70 4 96GB $3.80 gcp NVIDIA A100 1 80GB $3.60 2 160GB $7.20 4 320GB $14.40 8 640GB $28.80 gcp NVIDIA H100 1 80GB $10.00 2 160GB $20.00 4 320GB $40.00 8 640GB $80.00 PRO Account PRO A monthly subscription to access powerful features.",
    "id": "7e517710-9374-4ca8-9743-e5981140cd2b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "80 gcp NVIDIA A100 1 80GB $3.60 2 160GB $7.20 4 320GB $14.40 8 640GB $28.80 gcp NVIDIA H100 1 80GB $10.00 2 160GB $20.00 4 320GB $40.00 8 640GB $80.00 PRO Account PRO A monthly subscription to access powerful features. ‚Üí Get PRO ($9/month) Inference Providers: Get 20√ó included inference credits ZeroGPU: Get 8√ó usage quota and highest priority in queues Spaces Hosting: Create ZeroGPU Spaces with H200 hardware Spaces Dev Mode: Fast iterations via SSH/VS Code for Spaces Dataset Viewer: Activate and use it on private datasets Blog Articles: Publish articles on your HF profile Features Preview: Get early access to upcoming features PRO Badge: Show your support on your profile System theme Website Models Datasets Spaces Changelog Inference Endpoints HuggingChat Company About Brand assets Terms of service Privacy Jobs Press Resources Learn Documentation Blog Forum Service Status Social GitHub Twitter LinkedIn Discord === PAGE: allenai (Ai2) (https://huggingface.co/allenai) === allenai (Ai2) Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Ai2 Enterprise non-profit Verified https://allenai.org/ allen_ai allenai Activity Feed Follow 4,008 AI & ML interests Building breatkthrough AI to solve the world's biggest problems. Recent Activity jakep-allenai updated a dataset 12 minutes ago allenai/olmOCR-bench elynnwu updated a model about 1 hour ago allenai/ACE2-EAMv3 hqfang published a dataset 2 days ago allenai/libero View all activity Articles Introducing the Open Chain of Thought Leaderboard Apr 23, 2024 ‚Ä¢ 35 Team members 229 +195 +182 +161 +151 +131 Organization Card Community About org cards Collections 24 MolmoAct Data Mixture All datasets for the MolmoAct (Multimodal Open Language Model for Action) release.",
    "id": "565b2806-8081-44df-ae20-35962bd5f2c1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "allenai/MolmoAct-Dataset Viewer ‚Ä¢ Updated 5 days ago ‚Ä¢ 1.11M ‚Ä¢ 20.8k ‚Ä¢ 18 allenai/MolmoAct-Pretraining-Mixture Viewer ‚Ä¢ Updated 22 days ago ‚Ä¢ 24.1M ‚Ä¢ 16.8k ‚Ä¢ 6 allenai/MolmoAct-Midtraining-Mixture Viewer ‚Ä¢ Updated 22 days ago ‚Ä¢ 5.93M ‚Ä¢ 17.1k ‚Ä¢ 2 allenai/libero Viewer ‚Ä¢ Updated 12 days ago ‚Ä¢ 521k ‚Ä¢ 18 MolmoAct All models for the MolmoAct (Multimodal Open Language Model for Action) release. MolmoAct: Action Reasoning Models that can Reason in Space Paper ‚Ä¢ 2508.07917 ‚Ä¢ Published 28 days ago ‚Ä¢ 42 allenai/MolmoAct-7B-D-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 6 days ago ‚Ä¢ 1.52k ‚Ä¢ 31 allenai/MolmoAct-7B-O-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 6 days ago ‚Ä¢ 182 ‚Ä¢ 5 allenai/MolmoAct-7B-D-Pretrain-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 6 days ago ‚Ä¢ 895 ‚Ä¢ 8 MolmoAct Data Mixture All datasets for the MolmoAct (Multimodal Open Language Model for Action) release. allenai/MolmoAct-Dataset Viewer ‚Ä¢ Updated 5 days ago ‚Ä¢ 1.11M ‚Ä¢ 20.8k ‚Ä¢ 18 allenai/MolmoAct-Pretraining-Mixture Viewer ‚Ä¢ Updated 22 days ago ‚Ä¢ 24.1M ‚Ä¢ 16.8k ‚Ä¢ 6 allenai/MolmoAct-Midtraining-Mixture Viewer ‚Ä¢ Updated 22 days ago ‚Ä¢ 5.93M ‚Ä¢ 17.1k ‚Ä¢ 2 allenai/libero Viewer ‚Ä¢ Updated 12 days ago ‚Ä¢ 521k ‚Ä¢ 18 MolmoAct All models for the MolmoAct (Multimodal Open Language Model for Action) release.",
    "id": "53d0f379-4d79-4f44-8070-aaa93ead8e27",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "MolmoAct: Action Reasoning Models that can Reason in Space Paper ‚Ä¢ 2508.07917 ‚Ä¢ Published 28 days ago ‚Ä¢ 42 allenai/MolmoAct-7B-D-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 6 days ago ‚Ä¢ 1.52k ‚Ä¢ 31 allenai/MolmoAct-7B-O-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 6 days ago ‚Ä¢ 182 ‚Ä¢ 5 allenai/MolmoAct-7B-D-Pretrain-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 6 days ago ‚Ä¢ 895 ‚Ä¢ 8 View 24 collections spaces 13 Sort: Recently updated pinned Running 7 AstaBench Leaderboard ü•á View benchmark leaderboards allenai 10 days ago pinned Running 395 Reward Bench Leaderboard üìê Display and analyze reward model evaluation results allenai 27 days ago pinned Running 2 HREF Leaderboard üìê Browse and search HREF leaderboard data allenai Jul 24 pinned Running 89 Zebra Logic Bench ü¶ì Render a leaderboard for model evaluation allenai Apr 2 pinned Running 3 SUPER Leaderboard ü§ñ allenai Feb 10 pinned Running 53 ZeroEval Leaderboard üìä Embed and use ZeroEval for evaluation tasks allenai Nov 22,",
    "id": "812cee2c-5c76-48e9-be1a-fd6a01380b0a",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "2024 View 13 Spaces models 786 Sort: Recently updated allenai/ACE2-EAMv3 Updated about 1 hour ago ‚Ä¢ 5 ‚Ä¢ 2 allenai/MolmoAct-7B-D-Captioner-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 3 days ago ‚Ä¢ 9 allenai/Flex-public-7B-1T 7B ‚Ä¢ Updated 6 days ago ‚Ä¢ 6 ‚Ä¢ 2 allenai/MolmoAct-7B-D-Pretrain-RT-1-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 6 days ago ‚Ä¢ 639 ‚Ä¢ 4 allenai/MolmoAct-7B-D-Pretrain-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 6 days ago ‚Ä¢ 895 ‚Ä¢ 8 allenai/MolmoAct-7B-O-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 6 days ago ‚Ä¢ 182 ‚Ä¢ 5 allenai/MolmoAct-7B-D-0812 Robotics ‚Ä¢ 8B ‚Ä¢ Updated 6 days ago ‚Ä¢ 1.52k ‚Ä¢ 31 allenai/OLMoASR Audio-Text-to-Text ‚Ä¢ Updated 11 days ago ‚Ä¢ 55 allenai/olmOCR-7B-0725 Image-Text-to-Text ‚Ä¢ 8B ‚Ä¢ Updated 13 days ago ‚Ä¢ 11.4k ‚Ä¢ 56 allenai/olmOCR-7B-0725-FP8 Image-to-Text ‚Ä¢ 8B ‚Ä¢ Updated 20 days ago ‚Ä¢ 62.3k ‚Ä¢ 12 View 786 models datasets 283 Sort: Recently updated allenai/dpo-base-100k-qwq-judge-random-rejected Viewer ‚Ä¢ Updated 4 days ago ‚Ä¢ 100k ‚Ä¢ 105 ‚Ä¢ 1 allenai/dpo-base-100k-qwq-judge-remix Viewer ‚Ä¢ Updated 5 days ago ‚Ä¢ 100k ‚Ä¢ 106 allenai/MolmoAct-Dataset Viewer ‚Ä¢ Updated 5 days ago ‚Ä¢ 1.11M ‚Ä¢ 20.",
    "id": "caea5505-f533-40ac-863a-968281431e0b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "11M ‚Ä¢ 20.8k ‚Ä¢ 18 allenai/dpo-base-100k-gemma3-judge Viewer ‚Ä¢ Updated 5 days ago ‚Ä¢ 100k ‚Ä¢ 116 allenai/dpo-base-100k-qwq-judge Viewer ‚Ä¢ Updated 5 days ago ‚Ä¢ 100k ‚Ä¢ 149 allenai/MoNaCo_Benchmark Updated 11 days ago ‚Ä¢ 264 ‚Ä¢ 18 allenai/asta-bench-submissions Updated 12 days ago ‚Ä¢ 27 ‚Ä¢ 1 allenai/libero Viewer ‚Ä¢ Updated 12 days ago ‚Ä¢ 521k ‚Ä¢ 18 allenai/asta-bench-results Viewer ‚Ä¢ Updated 13 days ago ‚Ä¢ 70 ‚Ä¢ 394 allenai/asta-bench Updated 14 days ago ‚Ä¢ 535 ‚Ä¢ 1 View 283 datasets System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: huggingface (Hugging Face) (https://huggingface.co/huggingface) === huggingface (Hugging Face) Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Hugging Face Enterprise company Verified https://huggingface.co huggingface huggingface Activity Feed Follow 57,900 AI & ML interests The AI community building the future. Recent Activity mishig updated a Space about 1 hour ago huggingface/inference-playground reach-vb new activity about 3 hours ago huggingface/transformers-metadata:Update with commit 94ae1ba5b55e79ba766582de8a199d8ccf24a021 See: https://github.com/huggingface/transformers/commit/94ae1ba5b55e79ba766582de8a199d8ccf24a021 reach-vb new activity about 3 hours ago huggingface/transformers-metadata:thoth-guardian-pr-branch View all activity Articles TimeScope: How Long Can Your Video Large Multimodal Model Go? Jul 23 ‚Ä¢ 39 Yay!",
    "id": "2be07b69-9bff-4ab4-bc45-d342b9adc7f9",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Jul 23 ‚Ä¢ 39 Yay! Organizations can now publish blog Articles Jan 20 ‚Ä¢ 48 Team members 209 +175 +162 +141 +131 +111 Organization Card Community About org cards üëã Hi! We are on a mission to democratize good machine learning, one commit at a time. If that sounds like something you should be doing, why don't you join us! For press enquiries, you can ‚úâÔ∏è contact our team here. Collections 1 DistilBERT release Original DistilBERT model, checkpoints obtained from using teacher-student learning from the original BERT checkpoints. distilbert/distilbert-base-cased Fill-Mask ‚Ä¢ 0.1B ‚Ä¢ Updated May 6, 2024 ‚Ä¢ 129k ‚Ä¢ ‚Ä¢ 52 distilbert/distilbert-base-uncased Fill-Mask ‚Ä¢ 0.1B ‚Ä¢ Updated May 6, 2024 ‚Ä¢ 11.9M ‚Ä¢ ‚Ä¢ 752 distilbert/distilbert-base-multilingual-cased Fill-Mask ‚Ä¢ 0.1B ‚Ä¢ Updated May 6, 2024 ‚Ä¢ 1.21M ‚Ä¢ 211 distilbert/distilbert-base-uncased-finetuned-sst-2-english Text Classification ‚Ä¢ 0.1B ‚Ä¢ Updated Dec 19, 2023 ‚Ä¢ 3.27M ‚Ä¢ ‚Ä¢ 823 DistilBERT release Original DistilBERT model, checkpoints obtained from using teacher-student learning from the original BERT checkpoints. distilbert/distilbert-base-cased Fill-Mask ‚Ä¢ 0.1B ‚Ä¢ Updated May 6, 2024 ‚Ä¢ 129k ‚Ä¢ ‚Ä¢ 52 distilbert/distilbert-base-uncased Fill-Mask ‚Ä¢ 0.1B ‚Ä¢ Updated May 6, 2024 ‚Ä¢ 11.9M ‚Ä¢ ‚Ä¢ 752 distilbert/distilbert-base-multilingual-cased Fill-Mask ‚Ä¢ 0.1B ‚Ä¢ Updated May 6, 2024 ‚Ä¢ 1.",
    "id": "96592c74-dc7e-4107-a323-c4e2cae85b92",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "1B ‚Ä¢ Updated May 6, 2024 ‚Ä¢ 129k ‚Ä¢ ‚Ä¢ 52 distilbert/distilbert-base-uncased Fill-Mask ‚Ä¢ 0.1B ‚Ä¢ Updated May 6, 2024 ‚Ä¢ 11.9M ‚Ä¢ ‚Ä¢ 752 distilbert/distilbert-base-multilingual-cased Fill-Mask ‚Ä¢ 0.1B ‚Ä¢ Updated May 6, 2024 ‚Ä¢ 1.21M ‚Ä¢ 211 distilbert/distilbert-base-uncased-finetuned-sst-2-english Text Classification ‚Ä¢ 0.1B ‚Ä¢ Updated Dec 19, 2023 ‚Ä¢ 3.27M ‚Ä¢ ‚Ä¢ 823 spaces 30 Sort: Recently updated pinned Running 17 Enterprise Hub üîí Why you need it, how to get it huggingface Jun 27 pinned Running on CPU Upgrade 16 Hugging Face Fast üèé Test your internet bandwidth huggingface Jun 12 pinned Running 101 Number Tokenization Blog üìà Explore how tokenization affects arithmetic in LLMs huggingface Dec 14, 2024 Running on CPU Upgrade 208 Inference Playground üîã Set theme for Hugging Face Playground huggingface about 1 hour ago Running 484 AI Deadlines ‚ö° Manage project deadlines with AI assistance huggingface about 6 hours ago Running on CPU Upgrade 21 Openapi ü¶Ä Hub API Documentation huggingface Jun 18 View 30 Spaces models 16 Sort: Recently updated huggingface/time-series-transformer-tourism-monthly Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated May 24 ‚Ä¢ 3.4k ‚Ä¢ 22 huggingface/timesfm-tourism-monthly 0.0B ‚Ä¢ Updated Mar 31 ‚Ä¢ 22 ‚Ä¢ 2 huggingface/CodeBERTa-language-id Text Classification ‚Ä¢ Updated Mar 29, 2024 ‚Ä¢ 1.31k ‚Ä¢ ‚Ä¢ 63 huggingface/falcon-40b-gptq Text Generation ‚Ä¢ 7B ‚Ä¢ Updated Jun 14,",
    "id": "07960fc3-5066-4b8e-b46f-f6fd69b01cbc",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "0B ‚Ä¢ Updated May 24 ‚Ä¢ 3.4k ‚Ä¢ 22 huggingface/timesfm-tourism-monthly 0.0B ‚Ä¢ Updated Mar 31 ‚Ä¢ 22 ‚Ä¢ 2 huggingface/CodeBERTa-language-id Text Classification ‚Ä¢ Updated Mar 29, 2024 ‚Ä¢ 1.31k ‚Ä¢ ‚Ä¢ 63 huggingface/falcon-40b-gptq Text Generation ‚Ä¢ 7B ‚Ä¢ Updated Jun 14, 2023 ‚Ä¢ 45 ‚Ä¢ 13 huggingface/autoformer-tourism-monthly Updated May 24, 2023 ‚Ä¢ 4.02k ‚Ä¢ 10 huggingface/distilbert-base-uncased-finetuned-mnli Text Classification ‚Ä¢ 0.1B ‚Ä¢ Updated Mar 22, 2023 ‚Ä¢ 210 ‚Ä¢ 2 huggingface/informer-tourism-monthly Updated Feb 24, 2023 ‚Ä¢ 4.3k ‚Ä¢ 6 huggingface/the-no-branch-repo Text-to-Image ‚Ä¢ Updated Feb 10, 2023 ‚Ä¢ 8 ‚Ä¢ 6 huggingface/CodeBERTa-small-v1 Fill-Mask ‚Ä¢ Updated Jun 27, 2022 ‚Ä¢ 63.1k ‚Ä¢ ‚Ä¢ 84 huggingface/test-model-repo Updated Nov 19, 2021 ‚Ä¢ 2 View 16 models datasets 34 Sort: Recently updated huggingface/diffusers-metadata Viewer ‚Ä¢ Updated about 7 hours ago ‚Ä¢ 78 ‚Ä¢ 1.13k ‚Ä¢ 8 huggingface/documentation-images Viewer ‚Ä¢ Updated 4 days ago ‚Ä¢ 55 ‚Ä¢ 2.46M ‚Ä¢ 80 huggingface/transformers-metadata Viewer ‚Ä¢ Updated 4 days ago ‚Ä¢ 1.47k ‚Ä¢ 1.08k ‚Ä¢ 28 huggingface/policy-docs Viewer ‚Ä¢ Updated Jul 19 ‚Ä¢ 26 ‚Ä¢ 4.08k ‚Ä¢ 13 huggingface/badges Viewer ‚Ä¢ Updated Jul 17 ‚Ä¢ 1 ‚Ä¢ 472k ‚Ä¢ 46 huggingface/cats-image Viewer ‚Ä¢ Updated Jun 20 ‚Ä¢ 1 ‚Ä¢ 10.",
    "id": "7767e5bf-294a-412b-a5e8-16dcdac6d2a6",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "46M ‚Ä¢ 80 huggingface/transformers-metadata Viewer ‚Ä¢ Updated 4 days ago ‚Ä¢ 1.47k ‚Ä¢ 1.08k ‚Ä¢ 28 huggingface/policy-docs Viewer ‚Ä¢ Updated Jul 19 ‚Ä¢ 26 ‚Ä¢ 4.08k ‚Ä¢ 13 huggingface/badges Viewer ‚Ä¢ Updated Jul 17 ‚Ä¢ 1 ‚Ä¢ 472k ‚Ä¢ 46 huggingface/cats-image Viewer ‚Ä¢ Updated Jun 20 ‚Ä¢ 1 ‚Ä¢ 10.5k ‚Ä¢ 2 huggingface/label-files Updated Jun 12 ‚Ä¢ 1.55k ‚Ä¢ 19 huggingface/electricity-production Viewer ‚Ä¢ Updated May 22 ‚Ä¢ 397 ‚Ä¢ 63 ‚Ä¢ 1 huggingface/gemini-results-new-cs-papers-2025-02-11-100-230-updated Viewer ‚Ä¢ Updated Feb 17 ‚Ä¢ 130 ‚Ä¢ 124 huggingface/wiki_dpr Updated Feb 5 ‚Ä¢ 193 View 34 datasets System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Hugging Face - Documentation (https://huggingface.co/docs) === Hugging Face - Documentation Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Documentation Hub & Client Libraries Hub Host Git-based models, datasets, and Spaces on the HF Hub Hub Python Library Python client to interact with the Hugging Face Hub Huggingface.js JavaScript libraries for Hugging Face with built-in TS types Tasks Explore demos, models, and datasets for any ML tasks Dataset viewer API for metadata, stats,",
    "id": "0119b600-9551-4331-bbdc-214a0712c22f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "co/docs) === Hugging Face - Documentation Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Documentation Hub & Client Libraries Hub Host Git-based models, datasets, and Spaces on the HF Hub Hub Python Library Python client to interact with the Hugging Face Hub Huggingface.js JavaScript libraries for Hugging Face with built-in TS types Tasks Explore demos, models, and datasets for any ML tasks Dataset viewer API for metadata, stats, and content of HF Hub datasets Deployment & Inference Inference Providers Call 200k+ models hosted by our 10+ Inference partners Inference Endpoints (dedicated) Deploy models on dedicated & fully managed infrastructure on HF Deploying on AWS Train/deploy models from Hugging Face to AWS with DLCs Text Generation Inference Serve language models with TGI optimized toolkit Text Embeddings Inference Serve embeddings models with TEI optimized toolkit Microsoft Azure Deploy Hugging Face models on Microsoft Azure Core ML Libraries Transformers State-of-the-art AI models for PyTorch Diffusers State-of-the-art Diffusion models in PyTorch Datasets Access & share datasets for any ML tasks Transformers.js State-of-the-art ML running directly in your browser Tokenizers Fast tokenizers optimized for research & production Evaluate Evaluate and compare models performance timm State-of-the-art vision models: layers, optimizers, and utilities Sentence Transformers Embeddings, Retrieval, and Reranking Training & Optimization PEFT Parameter-efficient finetuning for large language models Accelerate Train PyTorch models with multi-GPU, TPU, mixed precision Optimum Optimize HF Transformers for faster training/inference AWS Trainium & Inferentia Train/deploy Transformers/Diffusers on AWS TRL Train transformers LMs with reinforcement learning Safetensors Safe way to store/distribute neural network weights Bitsandbytes Optimize and quantize models with bitsandbytes Lighteval All-in-one toolkit to evaluate LLMs across multiple backends Collaboration & Extras Gradio Build ML demos and web apps with a few lines of Python Trackio A lightweight, local-first,",
    "id": "608f87cb-1f0d-4fba-9e41-82e030598458",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "local-first, and free experiment tracking Python library smolagents Smol library to build great agents in Python LeRobot Making AI for Robotics more accessible with end-to-end learning AutoTrain AutoTrain API and UI for seamless model training Chat UI Open source chat frontend powering HuggingChat Leaderboards Create custom Leaderboards on Hugging Face Argilla Collaboration tool for building high-quality datasets Distilabel Framework for synthetic data generation and AI feedback Community Blog Learn Discord Forum Github System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: amazon (Amazon) (https://huggingface.co/amazon) === amazon (Amazon) Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Amazon Enterprise company https://aws.amazon.com/ aws Activity Feed Request to join this org Follow 3,399 AI & ML interests Scalable Artificial Intelligence Recent Activity yinsong1986 new activity 15 days ago amazon/MistralLite:Update README.md LahariChowtoori updated a Space about 1 month ago amazon/README LahariChowtoori new activity about 1 month ago amazon/README:Q2 Updates to org card View all activity Team members 21 Organization Card Community About org cards Innovating with machine learning on AWS On AWS, you can access performant infrastructure, deployment resources, data governance solutions, and AWS Deep Learning Containers (DLCs) with optimized open source frameworks, so that you can focus on your machine learning tasks. Build and Scale AI/ML on AWS AWS offers a comprehensive suite of AI/ML tools and services that support every stage of the machine learning lifecycle. From model development and training to deployment and inference, AWS provides cutting-edge solutions such as Amazon SageMaker AI, a fully-managed service for end-to-end development and deployment of models, Amazon Bedrock for building and scaling generative AI applications with foundation models ‚Äî including AgentCore for deploying AI agents securely at scale with purpose-built infrastructure and multi-framework support, custom AI accelerator chips such as AWS Trainium for training and AWS Inferentia for inference, and pre-configured environments to streamline your ML workflows.",
    "id": "b44f636e-4c4d-4af8-a0e4-148d6643364b",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Additionally, you can explore the Registry of Open Data to explore and access datasets for your AI/ML projects. Whether you're working on large language models, generative AI, computer vision, time-series forecasting, or natural language processing ‚Äî scaling your projects on AWS is easy. Learn more about these services and others: AWS GPUs for Machine Learning AWS Deep Learning AMIs Artificial Intelligence on AWS Building AI Agents with AWS AWS offers Strands Agents, an open source SDK that simplifies the development and deployment of AI agents. Supporting models from Amazon Bedrock, Meta's Llama, and other providers ‚Äî Strands Agents enables developers to build production ready multi-agent systems with minimal code. Introduction to Strands Agents Blog Strands Agents Documentation Strands Agents on GitHub AWS & Hugging Face Collaboration AWS and Hugging Face are working together to simplify and accelerate the adoption of advanced machine learning models. This collaboration offers streamlined training using Hugging Face Deep Learning Containers with SageMaker AI distributed training libraries, simplifying workflows with the SageMaker AI Python SDK for efficient model training. Deployment is made effortless through the Hugging Face Inference toolkit and DLCs, allowing users to deploy trained models on the Hugging Face Hub. Amazon SageMaker AI facilitates the creation of scalable endpoints with built-in monitoring and enterprise-level security. This joint effort empowers teams to move quickly from experimentation to production, leveraging cutting-edge models and scalable infrastructure to drive innovation in machine learning projects.",
    "id": "add23b8a-fd2e-44e5-bebc-4aa083816034",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Deployment is made effortless through the Hugging Face Inference toolkit and DLCs, allowing users to deploy trained models on the Hugging Face Hub. Amazon SageMaker AI facilitates the creation of scalable endpoints with built-in monitoring and enterprise-level security. This joint effort empowers teams to move quickly from experimentation to production, leveraging cutting-edge models and scalable infrastructure to drive innovation in machine learning projects. Learn about Hugging Face on AWS Learn about Hugging Face on Amazon SageMaker AI Reference documentation for using Hugging Face with Amazon SageMaker AI Community Forum on Hugging Face Hugging Face Hub on AWS Marketplace Connect, Learn, and Grow with AWS Stay connected with the latest AWS AI/ML and open source developments: AWS Open Source AWS on GitHub AWS on LinkedIn Open Source At AWS Amazon Science Amazon Science on GitHub Amazon Science on LinkedIn Code and Datasets from Amazon Researchers AWS Developers AWS Machine Learning Blog Generative AI on AWS Data and Machine Learning on AWS ML Specialist Training & Resources Amazon-run organizations on Hugging Face AutoGluon AWS Inferentia and Trainium Amazon Science Let's innovate together! üéâüöÄ What is Agentic AI? Building AI Agents with Strands : Part 1 - Creating Your First Agent Intelligent Code at your Fingertips: Introducing an agentic coding experience in your IDE Build a Multi-Agent System with LangGraph and Mistral on AWS Collections 2 Chronos-Bolt‚ö°Ô∏è Models Chronos-Bolt pretrained models for time series forecasting.",
    "id": "2a304193-518b-49fb-a069-a9a132ae2eaa",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "üéâüöÄ What is Agentic AI? Building AI Agents with Strands : Part 1 - Creating Your First Agent Intelligent Code at your Fingertips: Introducing an agentic coding experience in your IDE Build a Multi-Agent System with LangGraph and Mistral on AWS Collections 2 Chronos-Bolt‚ö°Ô∏è Models Chronos-Bolt pretrained models for time series forecasting. amazon/chronos-bolt-tiny Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 938k ‚Ä¢ 23 amazon/chronos-bolt-mini Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 283k ‚Ä¢ 8 amazon/chronos-bolt-small Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 770k ‚Ä¢ 14 amazon/chronos-bolt-base Time Series Forecasting ‚Ä¢ 0.2B ‚Ä¢ Updated Feb 17 ‚Ä¢ 1.03M ‚Ä¢ 63 Chronos Models & Datasets Collection of artifacts related to Chronos pretrained models for time series forecasting. amazon/chronos-bolt-tiny Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 938k ‚Ä¢ 23 amazon/chronos-bolt-mini Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 283k ‚Ä¢ 8 amazon/chronos-bolt-small Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 770k ‚Ä¢ 14 amazon/chronos-bolt-base Time Series Forecasting ‚Ä¢ 0.2B ‚Ä¢ Updated Feb 17 ‚Ä¢ 1.03M ‚Ä¢ 63 Chronos-Bolt‚ö°Ô∏è Models Chronos-Bolt pretrained models for time series forecasting.",
    "id": "2cd2d663-cd85-41e5-9b5e-03b12ae7c358",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "amazon/chronos-bolt-tiny Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 938k ‚Ä¢ 23 amazon/chronos-bolt-mini Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 283k ‚Ä¢ 8 amazon/chronos-bolt-small Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 770k ‚Ä¢ 14 amazon/chronos-bolt-base Time Series Forecasting ‚Ä¢ 0.2B ‚Ä¢ Updated Feb 17 ‚Ä¢ 1.03M ‚Ä¢ 63 Chronos Models & Datasets Collection of artifacts related to Chronos pretrained models for time series forecasting. amazon/chronos-bolt-tiny Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 938k ‚Ä¢ 23 amazon/chronos-bolt-mini Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 283k ‚Ä¢ 8 amazon/chronos-bolt-small Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 770k ‚Ä¢ 14 amazon/chronos-bolt-base Time Series Forecasting ‚Ä¢ 0.2B ‚Ä¢ Updated Feb 17 ‚Ä¢ 1.03M ‚Ä¢ 63 models 20 Sort: Recently updated amazon/bort Fill-Mask ‚Ä¢ Updated May 29 ‚Ä¢ 290 ‚Ä¢ 18 amazon/chronos-bolt-tiny Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 938k ‚Ä¢ 23 amazon/chronos-bolt-mini Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 283k ‚Ä¢ 8 amazon/chronos-bolt-small Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 770k ‚Ä¢ 14 amazon/chronos-bolt-base Time Series Forecasting ‚Ä¢ 0.2B ‚Ä¢ Updated Feb 17 ‚Ä¢ 1.03M ‚Ä¢ 63 amazon/chronos-t5-large Time Series Forecasting ‚Ä¢ 0.",
    "id": "3b9a4335-acbf-4c51-8fb7-9cbed4504bb5",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 283k ‚Ä¢ 8 amazon/chronos-bolt-small Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 770k ‚Ä¢ 14 amazon/chronos-bolt-base Time Series Forecasting ‚Ä¢ 0.2B ‚Ä¢ Updated Feb 17 ‚Ä¢ 1.03M ‚Ä¢ 63 amazon/chronos-t5-large Time Series Forecasting ‚Ä¢ 0.7B ‚Ä¢ Updated Feb 17 ‚Ä¢ 219k ‚Ä¢ 158 amazon/chronos-t5-base Time Series Forecasting ‚Ä¢ 0.2B ‚Ä¢ Updated Feb 17 ‚Ä¢ 1.17M ‚Ä¢ 34 amazon/chronos-t5-small Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 24M ‚Ä¢ 126 amazon/chronos-t5-mini Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 113k ‚Ä¢ 17 amazon/chronos-t5-tiny Time Series Forecasting ‚Ä¢ 0.0B ‚Ä¢ Updated Feb 17 ‚Ä¢ 553k ‚Ä¢ 113 View 20 models datasets 2 Sort: Recently updated amazon/CodePrefBench Preview ‚Ä¢ Updated Nov 25, 2024 ‚Ä¢ 42 ‚Ä¢ 1 amazon/AmazonQAC Viewer ‚Ä¢ Updated Nov 19, 2024 ‚Ä¢ 396M ‚Ä¢ 2.92k ‚Ä¢ 15 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Changelog - Hugging Face (https://huggingface.co/changelog) === Changelog - Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Changelog Keep track of latest changes on the Hugging Face Hub Jul 30, 25 Upvote 175 +170 Jul 30, 25 Introducing HF Jobs: Run scalable compute jobs on Hugging Face Upvote 175 +170 Hugging Face Jobs lets you effortlessly run compute tasks on our infrastructure‚Äîfrom simple scripts to large-scale workloads‚Äîwith a simple CLI.",
    "id": "4fff47a1-a419-4513-8d3c-9275d86430b0",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "25 Upvote 175 +170 Jul 30, 25 Introducing HF Jobs: Run scalable compute jobs on Hugging Face Upvote 175 +170 Hugging Face Jobs lets you effortlessly run compute tasks on our infrastructure‚Äîfrom simple scripts to large-scale workloads‚Äîwith a simple CLI. Whether you need CPUs, or high-end GPUs, Jobs provides instant access to the hardware you need, billed by the second. Quick Start Examples: Run Python code directly: hf jobs run python:3.12 python -c \"print('Hello from the cloud!')\" Use GPUs without any setup: hf jobs run --flavor=t4-small ubuntu nvidia-smi Jul 28, 25 Upvote 87 +82 Jul 28, 25 Trending Papers Upvote 87 +82 The Daily Papers page now includes Trending Papers, showcasing the most popular and impactful research papers from the community, along with their corresponding code implementations on GitHub. Trending Papers are ranked based on recent GitHub star activity. Jul 25, 25 Upvote 86 +81 Jul 25, 25 Introducing a better Hugging Face CLI Upvote 86 +81 We‚Äôve renamed huggingface-cli to hf and overhauled the command structure for speed and clarity. The new CLI now uses the format hf <resource> <action>, so commands like hf auth login, hf repo create, or hf download Qwen/Qwen3-0.6B are now consistent and intuitive. Migration is easy, the old CLI still works and will gently point you to the new commands. You can learn more by reading the full announcement in this blog article. Jul 23, 25 Upvote 47 +42 Jul 23, 25 JSON Support in the Dataset Viewer Upvote 47 +42 You can now view JSON cells directly in the Dataset Viewer, making it far easier to expand nested structures, copy useful snippets, and understand your data. This feature is particularly useful for tool calling datasets like interstellarninja/hermes_reasoning_tool_use and Salesforce/APIGen-MT-5k.",
    "id": "01af8566-f238-41d6-bd21-c01e8a5e5ac3",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Jul 23, 25 Upvote 47 +42 Jul 23, 25 JSON Support in the Dataset Viewer Upvote 47 +42 You can now view JSON cells directly in the Dataset Viewer, making it far easier to expand nested structures, copy useful snippets, and understand your data. This feature is particularly useful for tool calling datasets like interstellarninja/hermes_reasoning_tool_use and Salesforce/APIGen-MT-5k. Jul 18, 25 Upvote 88 +83 Jul 18, 25 Inference Providers now fully support OpenAI-compatible API Upvote 88 +83 Our Inference Providers service now fully supports the OpenAI-compatible API, making it easy to integrate with existing workflows. Plus, you can now specify the provider name directly in the model path for greater flexibility. This means you can effortlessly switch between different inference providers while using the familiar OpenAI client‚Äîjust point to router.huggingface.co/v1 and include the provider in the model name. This update simplifies integration while maintaining full compatibility with OpenAI's SDK. Try it now with supported providers like novita, groq, together, and more! Previous 1 2 3 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Privacy Policy ‚Äì Hugging Face (https://huggingface.co/privacy) === Privacy Policy ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Terms of Service Privacy Policy Content Policy Code of Conduct Hugging Face Privacy Policy üóì Effective Date: March 28, 2023 We have implemented this Privacy Policy because your privacy is important to us. This Privacy Policy (the ‚ÄúPolicy‚Äù) describes the type of information that Hugging Face, Inc. (the ‚ÄúCompany‚Äù) gathers from users of the Hugging Face services (the ‚ÄúServices‚Äù), and how the Company uses that information. The Policy is part of the Company Terms of Use. The Policy applies to all users of the Services (‚ÄúUsers‚Äù). The Company occasionally collects Personal Information from Users.",
    "id": "329c88d6-449f-4e0b-85f2-ce60f2ceb4f0",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "This Privacy Policy (the ‚ÄúPolicy‚Äù) describes the type of information that Hugging Face, Inc. (the ‚ÄúCompany‚Äù) gathers from users of the Hugging Face services (the ‚ÄúServices‚Äù), and how the Company uses that information. The Policy is part of the Company Terms of Use. The Policy applies to all users of the Services (‚ÄúUsers‚Äù). The Company occasionally collects Personal Information from Users. ‚ÄúPersonal Information‚Äù means any information that can, alone or associated with other information, be used to identify an individual, for instance by reference of a name, a username, an email address, an IP address, a photograph or others. By using the Services, you consent to the terms of the Policy and to our processing of Personal Information in the manner and for the purposes set forth herein. If you do not agree with the Policy, please do not use the Services. The Company reserves the right, at its sole discretion, to change the Policy at any time, which change will be effective 10 days following posting the revision to the Policy on the Hugging Face website (the ‚ÄúWebsite‚Äù). Your continued use of the Services 10 days following such posting means you accept those changes. 1. INFORMATION WE COLLECT The Company collects the following information, some of which might be Personal Information. A. Information you provide directly The Company collects information directly provided by Users as part of using the Services, such as: information provided as part of setting up an account on the Website: email address, password, username, full name, and other optional information such as an avatar, your interests, or usernames to your third-party social networks, payment information provided, if you decide to upgrade your user or organization account: credit card information, other information and materials that you decide to post on the Website (e.g., the discussion forum, or other), communications between you and the Company, as part of using the Services. At any time during your use of the Services, you may decide to share some information or content publicly or privately.",
    "id": "49f7dc67-0a0f-4e02-a6b5-89c6514ac8e1",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "At any time during your use of the Services, you may decide to share some information or content publicly or privately. If you decide to share your information or content publicly, and if you decide to include Personal Information, you understand that anyone may view this information. If you decide to keep your information private and control the access to it, you understand that only the users that you authorize will view this information. The Company also reserves the right to access this information with your consent, or without your consent only for the purposes of pursuing legitimate interests such as maintaining security on its Services or complying with any legal or regulatory obligations. B. Information we collect from third parties We may collect Information from third parties that help us deliver the Services or process information. C. Information we automatically collect from your use of the Services The Company automatically records information from your use of the Services such as: information about your Use of the Services, your session (date, location), your IP address, information from cookies, especially your login information, your preferences, information about your device: type, model, version, operating system, browser D. Cookies We use cookies only for the purposes of delivering, updating, monitoring, improving the Services, and maintaining security on our Services by detecting, preventing and responding to any type of threats or incidents. We may collect Information through those cookies. If you do not wish to accept these cookies and you decide to disable them, you will not be able to access and use the Services. E. ‚ÄúDo Not Track‚Äù On September 27, 2013, California enacted A.B. 370, amending the California Online Privacy Protection Act to require website operators to disclose how they respond to \"Do Not Track Signals\"; and whether third parties collect personally identifiable information about users when they use online services. The Company honors \"do not track\" signals and does not track, use cookies, or use advertising when a ‚Äúdo not track‚Äù mechanism is in place. The Company does not authorize the collection of personally identifiable information from our users for third party use through advertising technologies without separate member consent.",
    "id": "c91cad5d-40b1-46e6-a2d1-2dd8427afca4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The Company honors \"do not track\" signals and does not track, use cookies, or use advertising when a ‚Äúdo not track‚Äù mechanism is in place. The Company does not authorize the collection of personally identifiable information from our users for third party use through advertising technologies without separate member consent. California Civil Code Section 1798.83 also permits customers who are California residents to request certain information regarding Our disclosure of Personal Information to third parties for direct marketing purposes. To make such a request, please send an email to privacy@huggingface.co. Please note that the Company is only required to respond to one request per customer each year. 2. USE OF INFORMATION Purposes of the use of Information The Company may use information from Users for the following purposes: to deliver the Services, which may include the creation of Your account, the display of Your profile or Your content, or if applicable the upgrading of Your account to a paid account, to operate and improve the Services by providing you with more effective customer service, making the Services easier to use by eliminating the need for you to enter the same information repeatedly; performing research and analysis aimed at improving the Services, or other products and technologies of the Company; automatically updating the Services; diagnosing or fixing problems with the Services, to conduct analysis or research on the Services or any topics related to it, for business operations or scientific purposes, to communicate with you, especially through the sending of welcome emails, information on technical service issues, security announcements, information of new services available, legal notices, response to your requests, or any other information that we think might interest or be relevant to you, to ensure and maintain security on our Services or Website, which may include detecting, preventing, investigating or otherwise addressing fraud or security issues, to protect against harm to the rights, property or safety of the Company, our Users, yourself or the public, to enforce any applicable terms of service or agreement, including investigations of potential violations thereof, to comply with any applicable law, regulation, legal process or governmental requests.",
    "id": "97a78707-320e-4a0f-ae68-c70b312acd83",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "B. Grounds for the use of Information Pursuant to applicable data protection laws, and especially the European Union‚Äôs General Data Protection Regulation (EU) 2016/679 (the ‚ÄúGDPR‚Äù), Hugging Face remains under an obligation to notify the Users about the legal basis on which their Personal Information is processed. Consent By creating an account on the Website and by using the Services, you consent to disclose information, some of which might be personal, and to our processing of such Personal Information in the manner and for the purposes set forth in this Policy. Agreement If you or your organization enter into an agreement with Hugging Face, either by simply using the Services and abiding by the terms and conditions available on the Website, or by executing another separate agreement, you also consent to our processing of your Personal Information as pursuant the obligations of such an agreement. Legitimate Interests Apart from the above cases, Hugging Face will use the information collected from you to pursue legitimate interests such as legal or regulatory compliance, security control, business operations, scientific research, or any other interest reasonably held as legitimate. 3. SHARING OF INFORMATION The Company will not sell, rent or lease your Personal Information except as provided for by this Policy. The Company may also share other information as provided by this Policy. A. Affiliates The Company may share User Information and Personal information collected by the Services with businesses that are legally part of the same group as the Company, or that become part of that group in the event of a change of control, merger, acquisition or sale (‚ÄúAffiliates‚Äù). B. Third Party Service Providers The Company may occasionally hire other companies to provide limited services on its behalf, such as providing customer support, hosting websites, processing transactions, or performing statistical analysis of its Services. Those companies will be permitted to obtain only the Personal Information they need to deliver the relevant service. They will be required to maintain the confidentiality of the information and are prohibited from using it for any other purpose. Please refer to the list of Third Party Service Providers below.",
    "id": "74992ea9-9d66-4023-b77a-a64d046532f8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "B. Third Party Service Providers The Company may occasionally hire other companies to provide limited services on its behalf, such as providing customer support, hosting websites, processing transactions, or performing statistical analysis of its Services. Those companies will be permitted to obtain only the Personal Information they need to deliver the relevant service. They will be required to maintain the confidentiality of the information and are prohibited from using it for any other purpose. Please refer to the list of Third Party Service Providers below. C. With your consent At any time during your use of the Services, or upon explicit request from us, you may consent to the disclosure of your information. D. For security and safety purposes In the event of any fraud, security threats or incidents, we reserve the right to disclose your information without your consent for the purposes of ensuring and maintaining security on our Website and for all of our Users, and detecting, preventing, investigating or otherwise addressing fraud or security issues. Similarly, we reserve the right to disclose your information without your consent for the purpose of protecting against harm to the rights, property or safety of the Company, our Users, yourself or the public. E. For legal or regulatory purposes We also reserve the right to disclose your information without your consent to comply with any applicable law, regulation, legal process or governmental requests. G. Anonymous Information The Company may use Anonymous Information (as defined below) or disclose it to third party service providers, to provide and improve the Services and other products or technologies of the Company. The Company may disclose Anonymous Information (with or without compensation) to third parties, including advertisers and partners, for purposes including, but not limited to, targeting advertisements. \"Anonymous Information\" means information which does not enable identification of an individual User, such as aggregated information about use of the Services. 4. YOUR RIGHTS A. Access your Information You may be entitled under data protection laws to access and review Personal Information the Company holds related to you. You may access, modify or delete the Information we collected by editing your profile or controlling the content that you share at any time.",
    "id": "bbdaeb9c-3173-4585-9d76-9c1c904b391f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "\"Anonymous Information\" means information which does not enable identification of an individual User, such as aggregated information about use of the Services. 4. YOUR RIGHTS A. Access your Information You may be entitled under data protection laws to access and review Personal Information the Company holds related to you. You may access, modify or delete the Information we collected by editing your profile or controlling the content that you share at any time. If you have any other request, all such communications regarding access to Personal Information should be addressed to: privacy@huggingface.co. Such inquiries should be clearly marked as data protection queries and you should indicate if the request is time sensitive. B. Data retention We retain your Information for as long as necessary to deliver the Services, to comply with any applicable legal requirements, to maintain security and prevent incidents and, in general, to pursue our legitimate interests. You may decide to cancel your account and your content at any time by editing your profile. If you wish to request the erasure of all of your Personal Information that we process, you may do so by sending a written request to privacy@huggingface.co. 5. DATA SECURITY The security of your Personal Information is important to us. The Company follows generally accepted industry standards, including the use of appropriate administrative, physical and technical safeguards, to protect Personal Information. However, no method of transmission over the Internet, or method of electronic storage, is fully secure. Therefore, while the Company strives to use commercially acceptable means to protect Personal Information, the Company cannot guarantee its absolute security or confidentiality. If you have any questions about security, you can contact the Company at privacy@huggingface.co. In the event of an incident affecting your Personal Information, we will act promptly acceptable means to identify and address the incident, and to notify you. Please be aware that certain Personal Information and other information provided by you in connection with your use of the Services may be stored on your device (even if that Information is not collected by the Company). You are solely responsible for maintaining the security of your device from unauthorized access.",
    "id": "f7480f52-c235-4aa5-99d3-4a85862bb7ee",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "In the event of an incident affecting your Personal Information, we will act promptly acceptable means to identify and address the incident, and to notify you. Please be aware that certain Personal Information and other information provided by you in connection with your use of the Services may be stored on your device (even if that Information is not collected by the Company). You are solely responsible for maintaining the security of your device from unauthorized access. Similarly, you also remain responsible for maintaining the confidentiality of your password or any other information that should reasonably be held confidential. 6. LOCATION OF PROCESSING AND DATA TRANSFERS The Company and its servers are located in the United States. Personal Information collected by the Services may be stored and processed in the United States or any other country in which the Company or its affiliates, subsidiaries or agents maintain facilities. By using the Services, you consent to any such transfer of information outside of your country. The Company may transfer your Personal Information to affiliated companies for the purpose of storing or processing such information on its behalf. Such information may be transferred to other countries around the world. The Company requires that these parties agree to process such information in compliance with the Policy. In particular, if you provide Personal Information, it may be transferred to and processed on computers in the U.S. and other countries. We strive to take appropriate safeguards to ensure that your Personal Information will remain protected in a manner consistent with standard applicable data protection laws. If you have any other question, please contact the Company at: privacy@huggingface.co. 7. CHILDREN‚ÄôS PRIVACY The Services are neither directed to nor structured to attract children under the age of 13 years. Accordingly, the Company does not intend to collect Personal Information from anyone it knows to be under 13 years of age. The Company will direct potential users under 13 years of age not to use the Services. If the Company learns that Personal Information of persons less than 13 years of age has been collected without verifiable parental consent, the Company will take the appropriate steps to delete this information.",
    "id": "011bea4a-bdda-4e32-b19d-7210051f7407",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Accordingly, the Company does not intend to collect Personal Information from anyone it knows to be under 13 years of age. The Company will direct potential users under 13 years of age not to use the Services. If the Company learns that Personal Information of persons less than 13 years of age has been collected without verifiable parental consent, the Company will take the appropriate steps to delete this information. To make such a request, please contact the Company at: privacy@huggingface.co. 8. COMMUNICATIONS AND CAN-SPAM ACT The Company may collect your email address in order to send information and respond to inquiries and/or other requests or questions. The Company does not use false or misleading subjects or email addresses. The Company reasonably identifies advertisements and includes in its communications the physical address of its business location. The Company honors opt-out/unsubscribe requests. Users may follow the instructions at the bottom of an email from the Company in order to unsubscribe from correspondence. 9. CONTACT US If you have questions about this Policy, please contact privacy@huggingface.co. The main establishment in the European Union is Hugging Face, SAS, a French soci√©t√© par actions simplifi√©e √† associ√© unique registered in the Paris Trade and Companies Register under the number 822 168 043, and whose headquarters are located on 9 rue des Colonnes, 75002 Paris, France. The designation of this main establishment in the European Union gives full authority to the French Data Protection Agency, la Commission Nationale de l'Informatique et des Libert√©s (CNIL) per the General Data Protection Regulation (GDPR). 10. LIST OF THIRD-PARTY SERVICE PROVIDERS We may share your information with Third-Party Service Providers, and when we do, we ensure that they access your information in compliance with applicable data protection laws. The following list respects the format Subprocessor | Description of Subprocessing | Location of Subprocessing. Discourse | Forum | United States AWS SES | Emails | United States Amazon Web Services, Inc.",
    "id": "3b70caf8-2e7b-472a-81d9-69b59466630d",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "10. LIST OF THIRD-PARTY SERVICE PROVIDERS We may share your information with Third-Party Service Providers, and when we do, we ensure that they access your information in compliance with applicable data protection laws. The following list respects the format Subprocessor | Description of Subprocessing | Location of Subprocessing. Discourse | Forum | United States AWS SES | Emails | United States Amazon Web Services, Inc. | Hosting/Infrastructure | United States Stripe | Payment | United States MongoDB Atlas | Hosting/Infrastructure | United States Google Cloud Platform | Hosting/Infrastructure | United States/EMEA GitHub | Hosting code | United States OVHCloud | Hosting/Infrastructure | France Slack Technologies | Communication | United States Hugging Face SAS | All of the above | France System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Image-to-Image Models ‚Äì Hugging Face (https://huggingface.co/models?pipeline_tag=image-to-image) === Image-to-Image Models ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks 1 Libraries Languages Licenses Other Tasks Reset Tasks Text Generation Any-to-Any Image-Text-to-Text Image-to-Text Image-to-Image Text-to-Image Text-to-Video Text-to-Speech + 42Parameters Reset Parameters < 1B 6B 12B 32B 128B > 500B < 1B > 500B Libraries PyTorch google-tensorflow TensorFlow JAX Transformers Diffusers Safetensors ONNX GGUF Transformers.js MLX Keras + 41 Apps vLLM TGI llama.cpp MLX LM LM Studio Ollama Jan + 13 Inference Providers Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale + 6 Apply filters Models 1,518 Full-text search Edit filters Sort: Trending Active filters: image-to-image Clear all Qwen/Qwen-Image-Edit Image-to-Image ‚Ä¢ Updated 15 days ago ‚Ä¢ 147k ‚Ä¢ ‚Ä¢ 1.",
    "id": "f29603e3-62d7-4f88-908d-9637f2c905df",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "cpp MLX LM LM Studio Ollama Jan + 13 Inference Providers Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale + 6 Apply filters Models 1,518 Full-text search Edit filters Sort: Trending Active filters: image-to-image Clear all Qwen/Qwen-Image-Edit Image-to-Image ‚Ä¢ Updated 15 days ago ‚Ä¢ 147k ‚Ä¢ ‚Ä¢ 1.71k black-forest-labs/FLUX.1-Kontext-dev Image-to-Image ‚Ä¢ Updated Jun 27 ‚Ä¢ 401k ‚Ä¢ ‚Ä¢ 2.25k peteromallet/Qwen-Image-Edit-InStyle Image-to-Image ‚Ä¢ Updated 6 days ago ‚Ä¢ 1.84k ‚Ä¢ 46 chflame163/kontext_hires Image-to-Image ‚Ä¢ Updated 4 days ago ‚Ä¢ ‚Ä¢ 16 drbaph/Qwen-Image-Edit-Remove-Clothing-LoRA Image-to-Image ‚Ä¢ Updated about 18 hours ago ‚Ä¢ 1.8k ‚Ä¢ 17 QuantStack/Qwen-Image-Edit-GGUF Image-to-Image ‚Ä¢ 20B ‚Ä¢ Updated 7 days ago ‚Ä¢ 168k ‚Ä¢ 184 TencentARC/IC-Custom Image-to-Image ‚Ä¢ Updated 8 days ago ‚Ä¢ 122 ‚Ä¢ 15 InstantX/Qwen-Image-ControlNet-Union Image-to-Image ‚Ä¢ Updated 13 days ago ‚Ä¢ 5.59k ‚Ä¢ 58 flymy-ai/qwen-image-edit-inscene-lora Image-to-Image ‚Ä¢ Updated 15 days ago ‚Ä¢ 2.89k ‚Ä¢ 60 stepfun-ai/Step1X-Edit-v1p2-preview Image-to-Image ‚Ä¢ Updated about 10 hours ago ‚Ä¢ 8 ‚Ä¢ 7 ovedrive/qwen-image-edit-4bit Image-to-Image ‚Ä¢ Updated 14 days ago ‚Ä¢ 5.",
    "id": "afdfaee9-7114-4ebc-91c1-29b5587413b4",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "59k ‚Ä¢ 58 flymy-ai/qwen-image-edit-inscene-lora Image-to-Image ‚Ä¢ Updated 15 days ago ‚Ä¢ 2.89k ‚Ä¢ 60 stepfun-ai/Step1X-Edit-v1p2-preview Image-to-Image ‚Ä¢ Updated about 10 hours ago ‚Ä¢ 8 ‚Ä¢ 7 ovedrive/qwen-image-edit-4bit Image-to-Image ‚Ä¢ Updated 14 days ago ‚Ä¢ 5.67k ‚Ä¢ 24 thedeoxen/refcontrol-flux-kontext-reference-pose-lora Image-to-Image ‚Ä¢ Updated 12 days ago ‚Ä¢ ‚Ä¢ 35 Kim2091/UltraSharpV2 Image-to-Image ‚Ä¢ Updated Jun 5 ‚Ä¢ 52 ilkerzgi/Overlay-Kontext-Dev-LoRA Image-to-Image ‚Ä¢ Updated Jul 16 ‚Ä¢ 16k ‚Ä¢ ‚Ä¢ 180 bullerwins/FLUX.1-Kontext-dev-GGUF Image-to-Image ‚Ä¢ 12B ‚Ä¢ Updated Jun 26 ‚Ä¢ 33.5k ‚Ä¢ 218 nunchaku-tech/nunchaku-flux.1-kontext-dev Image-to-Image ‚Ä¢ Updated 16 days ago ‚Ä¢ 32.1k ‚Ä¢ 34 manycore-research/SpatialGen-1.0 Image-to-Image ‚Ä¢ Updated 12 days ago ‚Ä¢ 197 ‚Ä¢ 15 stabilityai/stable-diffusion-xl-refiner-1.0 Image-to-Image ‚Ä¢ Updated Sep 25, 2023 ‚Ä¢ 553k ‚Ä¢ 1.96k jasperai/Flux.1-dev-Controlnet-Upscaler Image-to-Image ‚Ä¢ Updated Mar 22 ‚Ä¢ 10.5k ‚Ä¢ 815 QuantStack/FLUX.1-Kontext-dev-GGUF Image-to-Image ‚Ä¢ 12B ‚Ä¢ Updated Jul 24 ‚Ä¢ 27.9k ‚Ä¢ 118 peteromallet/Flux-Kontext-InScene Image-to-Image ‚Ä¢ Updated Jul 19 ‚Ä¢ 4.92k ‚Ä¢ ‚Ä¢ 166 thedeoxen/FLUX.",
    "id": "21944a84-e9e1-4c88-892c-d236bc83b7c8",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "1-dev-Controlnet-Upscaler Image-to-Image ‚Ä¢ Updated Mar 22 ‚Ä¢ 10.5k ‚Ä¢ 815 QuantStack/FLUX.1-Kontext-dev-GGUF Image-to-Image ‚Ä¢ 12B ‚Ä¢ Updated Jul 24 ‚Ä¢ 27.9k ‚Ä¢ 118 peteromallet/Flux-Kontext-InScene Image-to-Image ‚Ä¢ Updated Jul 19 ‚Ä¢ 4.92k ‚Ä¢ ‚Ä¢ 166 thedeoxen/FLUX.1-Kontext-dev-reference-depth-fusion-LORA Image-to-Image ‚Ä¢ Updated 19 days ago ‚Ä¢ 2.47k ‚Ä¢ ‚Ä¢ 49 stabilityai/stable-diffusion-2-inpainting Image-to-Image ‚Ä¢ Updated Jul 5, 2023 ‚Ä¢ 147k ‚Ä¢ 599 starsfriday/Kontext-Remover-General-LoRA Image-to-Image ‚Ä¢ Updated Jul 28 ‚Ä¢ 1.2k ‚Ä¢ ‚Ä¢ 69 Owen777/Kontext-Style-Loras Image-to-Image ‚Ä¢ Updated Jul 26 ‚Ä¢ 5.37k ‚Ä¢ 112 HighCWu/FLUX.1-Kontext-dev-bnb-hqq-4bit Image-to-Image ‚Ä¢ Updated Jul 5 ‚Ä¢ 32.1k ‚Ä¢ 11 nunchaku-tech/nunchaku-flux.1-fill-dev Image-to-Image ‚Ä¢ Updated 16 days ago ‚Ä¢ 5.94k ‚Ä¢ 9 nunchaku-tech/nunchaku-flux.1-depth-dev Image-to-Image ‚Ä¢ Updated 16 days ago ‚Ä¢ 1.74k ‚Ä¢ 4 Arm/neural-super-sampling Image-to-Image ‚Ä¢ Updated 12 days ago ‚Ä¢ 17 calcuis/qwen-image-edit-gguf Image-to-Image ‚Ä¢ 20B ‚Ä¢ Updated 5 days ago ‚Ä¢ 9.7k ‚Ä¢ 14 Previous 1 2 3 ... 51 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Text Generation Inference (https://huggingface.",
    "id": "5b4111a6-e7b9-4bd9-88a6-a8a785c10f15",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "74k ‚Ä¢ 4 Arm/neural-super-sampling Image-to-Image ‚Ä¢ Updated 12 days ago ‚Ä¢ 17 calcuis/qwen-image-edit-gguf Image-to-Image ‚Ä¢ 20B ‚Ä¢ Updated 5 days ago ‚Ä¢ 9.7k ‚Ä¢ 14 Previous 1 2 3 ... 51 Next System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: Text Generation Inference (https://huggingface.co/docs/text-generation-inference) === Text Generation Inference Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up text-generation-inference documentation Text Generation Inference text-generation-inference üè° View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm Search documentation main EN Getting started Text Generation Inference Quick Tour Supported Models Using TGI with Nvidia GPUs Using TGI with AMD GPUs Using TGI with Intel Gaudi Using TGI with AWS Trainium and Inferentia Using TGI with Google TPUs Using TGI with Intel GPUs Installation from source Multi-backend support Internal Architecture Usage Statistics Tutorials Consuming TGI Preparing Model for Serving Serving Private & Gated Models Using TGI CLI Non-core Model Serving Safety Using Guidance, JSON, tools Visual Language Models Monitoring TGI with Prometheus and Grafana Train Medusa Backends Neuron Gaudi TensorRT-LLM Llamacpp Reference All TGI CLI options Exported Metrics API Reference Conceptual Guides V3 update, caching and chunking Streaming Quantization Tensor Parallelism PagedAttention Safetensors Flash Attention Speculation (Medusa,",
    "id": "32df2454-de5f-4760-8132-91ccc8243a6e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "JSON, tools Visual Language Models Monitoring TGI with Prometheus and Grafana Train Medusa Backends Neuron Gaudi TensorRT-LLM Llamacpp Reference All TGI CLI options Exported Metrics API Reference Conceptual Guides V3 update, caching and chunking Streaming Quantization Tensor Parallelism PagedAttention Safetensors Flash Attention Speculation (Medusa, ngram) How Guidance Works (via outlines) LoRA (Low-Rank Adaptation) External Resources Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Text Generation Inference Text Generation Inference (TGI) is a toolkit for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and T5. Text Generation Inference implements many optimizations and features, such as: Simple launcher to serve most popular LLMs Production ready (distributed tracing with Open Telemetry, Prometheus metrics) Tensor Parallelism for faster inference on multiple GPUs Token streaming using Server-Sent Events (SSE) Continuous batching of incoming requests for increased total throughput Optimized transformers code for inference using Flash Attention and Paged Attention on the most popular architectures Quantization with bitsandbytes and GPT-Q Safetensors weight loading Watermarking with A Watermark for Large Language Models Logits warper (temperature scaling, top-p, top-k, repetition penalty) Stop sequences Log probabilities Fine-tuning Support: Utilize fine-tuned models for specific tasks to achieve higher accuracy and performance. Guidance: Enable function calling and tool-use by forcing the model to generate structured outputs based on your own predefined output schemas.",
    "id": "f961abd4-fa9d-4689-a7ba-b5f7814d497f",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Guidance: Enable function calling and tool-use by forcing the model to generate structured outputs based on your own predefined output schemas. Text Generation Inference is used in production by multiple projects, such as: Hugging Chat, an open-source interface for open-access models, such as Open Assistant and Llama OpenAssistant, an open-source community effort to train LLMs in the open nat.dev, a playground to explore and compare LLMs.",
    "id": "43e7fe36-89d5-46dc-b153-d6b37187954e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Guidance: Enable function calling and tool-use by forcing the model to generate structured outputs based on your own predefined output schemas. Text Generation Inference is used in production by multiple projects, such as: Hugging Chat, an open-source interface for open-access models, such as Open Assistant and Llama OpenAssistant, an open-source community effort to train LLMs in the open nat.dev, a playground to explore and compare LLMs. < > Update on GitHub Quick Tour‚Üí Text Generation Inference === PAGE: Models ‚Äì Hugging Face (https://huggingface.co/models?other=jan) === Models ‚Äì Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Edit Models filters Main Tasks Libraries Languages Licenses Other 1 Apps llama.cpp LM Studio Jan Backyard AI Draw Things DiffusionBee Jellybox RecurseChat Msty Sanctum Invoke JoyFusion LocalAI vLLM node-llama-cpp Ollama TGI MLX LM Docker Model Runner Lemonade Inference Providers Select all Together AI Cerebras Fireworks Nebius AI Novita Groq Hyperbolic Nscale SambaNova fal Featherless AI Cohere Replicate HF Inference API Misc Reset Misc jan Inference Endpoints text-generation-inference Eval Results Merge 4-bit precision custom_code 8-bit precision text-embeddings-inference Mixture of Experts Carbon Emissions Apply filters Models 0 Full-text search Edit filters Sort: Trending Active filters: jan Clear all System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs === PAGE: swiss-ai/Apertus-70B-Instruct-2509 ¬∑ Hugging Face (https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509) === swiss-ai/Apertus-70B-Instruct-2509 ¬∑ Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up swiss-ai / Apertus-70B-Instruct-2509 like 116 Follow Swiss AI Initiative 625 Text Generation Transformers Safetensors apertus multilingual compliant swiss-ai conversational License: apache-2.0 Model card Files Files and versions xet Community 8 Train Deploy Use this model You need to agree to share your contact information to access this model This repository is publicly accessible, but you have to accept the conditions to access its files and content.",
    "id": "75b6a834-ea45-466a-b710-bb85d60f05d9",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Apertus LLM Acceptable Use Policy (1.0 | September 1, 2025)\"Agreement\" The Swiss National AI Institute (SNAI) is a partnership between the two Swiss Federal Institutes of Technology, ETH Zurich and EPFL. By using the Apertus LLM you agree to indemnify, defend, and hold harmless ETH Zurich and EPFL against any third-party claims arising from your use of Apertus LLM. The training data and the Apertus LLM may contain or generate information that directly or indirectly refers to an identifiable individual (Personal Data). You process Personal Data as independent controller in accordance with applicable data protection law. SNAI will regularly provide a file with hash values for download which you can apply as an output filter to your use of our Apertus LLM. The file reflects data protection deletion requests which have been addressed to SNAI as the developer of the Apertus LLM. It allows you to remove Personal Data contained in the model output. We strongly advise downloading and applying this output filter from SNAI every six months following the release of the model. Log in or Sign Up to review the conditions and access this model content. Apertus Table of Contents Model Summary Key features How to use Long context processing Agentic Usage Deployment Evaluation Training Model Software & hardware Open resources Limitations Legal Aspects Contact Citation Apertus Table of Contents Model Summary How to use Evaluation Training Limitations Legal Aspects Model Summary Apertus is a 70B and 8B parameter language model designed to push the boundaries of fully-open multilingual and transparent models. The model supports over 1000 languages and long context, it uses only fully compliant and open training data, and achieves comparable performance to models trained behind closed doors. The model is a decoder-only transformer, pretrained on 15T tokens with a staged curriculum of web, code and math data. The model uses a new xIELU activation function and is trained from scratch with the AdEMAMix optimizer. Post-training included supervised fine-tuning and alignment via QRPO.",
    "id": "0966dab6-ea3d-4fe7-90ee-85f478e8cf18",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "The model supports over 1000 languages and long context, it uses only fully compliant and open training data, and achieves comparable performance to models trained behind closed doors. The model is a decoder-only transformer, pretrained on 15T tokens with a staged curriculum of web, code and math data. The model uses a new xIELU activation function and is trained from scratch with the AdEMAMix optimizer. Post-training included supervised fine-tuning and alignment via QRPO. Key features Fully open model: open weights + open data + full training details including all data and training recipes Massively Multilingual: 1811 natively supported languages Compliant Apertus is trained while respecting opt-out consent of data owners (even retrospectivey), and avoiding memorization of training data For more details refer to our technical report How to use The modeling code for Apertus is available in transformers v4.56.0, so make sure to upgrade your transformers version. You can also load the model with the latest vLLM which uses transformers as a backend. pip install -U transformers from transformers import AutoModelForCausalLM, AutoTokenizer model_name = \"swiss-ai/Apertus-70B-Instruct-2509\" device = \"cuda\" for GPU usage or \"cpu\" for CPU usage load the tokenizer and the model tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, ).to(device) prepare the model input prompt = \"Give me a brief explanation of gravity in simple terms.\"",
    "id": "76afa9c4-b946-42b8-a32b-238ba08c6475",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "messages_think = [ {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages_think, tokenize=False, add_generation_prompt=True, ) model_inputs = tokenizer([text], return_tensors=\"pt\", add_special_tokens=False).to(model.device) Generate the output generated_ids = model.generate(**model_inputs, max_new_tokens=32768) Get and decode the output output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :] print(tokenizer.decode(output_ids, skip_special_tokens=True)) We recommend setting temperature=0.8 and top_p=0.9 in the sampling parameters. Long context processing Apertus by default supports a context length up to 65,536 tokens. Agentic Usage Apertus supports tool use Deployment Deployment of the models is directly supported by the newest versions of Transformers, vLLM, SGLang, and also for running on-device with MLX, Evaluation Pretraining Evaluation: Performance (%) of Apertus models on general language understanding tasks (higher is better) compared to other pretrained models. Model Avg ARC HellaSwag WinoGrande XNLI XCOPA PIQA Fully Open Models Apertus-8B 65.8 72.7 59.8 70.6 45.2 66.5 79.8 Apertus-70B 67.5 70.6 64.0 73.3 45.3 69.8 81.9 OLMo2-7B 64.0 72.9 60.4 74.5 40.4 55.2 80.9 OLMo2-32B 67.7 76.2 66.7 78.6 42.9 60.1 82.1 EuroLLM-1.7B 54.8 57.2 44.9 58.1 40.7 55.7 72.4 EuroLLM-9B 62.8 67.9 57.",
    "id": "09fb60a0-d984-4e60-b612-09d64379faeb",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "4 55.2 80.9 OLMo2-32B 67.7 76.2 66.7 78.6 42.9 60.1 82.1 EuroLLM-1.7B 54.8 57.2 44.9 58.1 40.7 55.7 72.4 EuroLLM-9B 62.8 67.9 57.9 68.8 41.5 61.1 79.6 SmolLM2-1.7B 58.5 66.1 52.4 65.6 37.6 52.3 77.0 SmolLM3-3B 61.6 68.6 56.4 68.1 40.5 58.2 77.7 Poro-34B 61.7 65.7 57.9 70.6 41.6 56.0 78.5 Open-Weight Models Llama3.1-8B 65.4 71.6 60.0 73.4 45.3 61.8 80.1 Llama3.1-70B 67.3 74.4 56.5 79.4 44.3 66.7 82.3 Qwen2.5-7B 64.4 69.6 60.1 72.8 43.3 61.7 78.7 Qwen2.5-72B 69.8 76.2 67.5 78.0 46.9 68.2 82.0 Qwen3-32B 67.8 75.6 64.0 73.8 44.4 67.9 80.9 Llama4-Scout-16x17B 67.9 74.7 66.8 73.2 43.5 67.7 81.",
    "id": "143ae94b-35ea-410b-9814-beeb4b83f942",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "8 76.2 67.5 78.0 46.9 68.2 82.0 Qwen3-32B 67.8 75.6 64.0 73.8 44.4 67.9 80.9 Llama4-Scout-16x17B 67.9 74.7 66.8 73.2 43.5 67.7 81.2 GPT-OSS-20B 58.1 67.0 41.5 66.5 37.4 60.4 75.6 Many additional benchmark evaluations, for pretraining and posttraining phases, multilingual evaluations in around hundred languages, and long context evaluations are provided in Section 5 of the Apertus_Tech_Report.pdf Training Model Architecture: Transformer decoder Pretraining tokens: 15T Precision: bfloat16 Software & hardware GPUs: 4096 GH200 Training Framework: Megatron-LM ... Open resources All elements used in the training process are made openly available Training data reconstruction scripts: github.com/swiss-ai/pretrain-data The training intermediate checkpoints are available on the different branches of this same repository Limitations Apertus can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content. Legal Aspects EU AI Act Transparency Documentation and Code of Practice Apertus_EU_Public_Summary.pdf Apertus_EU_Code_of_Practice.pdf Data Protection and Copyright Requests For removal requests of personally identifiable information (PII) or of copyrighted content, please contact the respective dataset owners or us directly llm-privacy-requests@swiss-ai.org llm-copyright-requests@swiss-ai.org Output Filter for PII Currently no output filter is provided.",
    "id": "aad717cc-0d0d-4a85-858e-779a74e0611e",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  },
  {
    "text": "Legal Aspects EU AI Act Transparency Documentation and Code of Practice Apertus_EU_Public_Summary.pdf Apertus_EU_Code_of_Practice.pdf Data Protection and Copyright Requests For removal requests of personally identifiable information (PII) or of copyrighted content, please contact the respective dataset owners or us directly llm-privacy-requests@swiss-ai.org llm-copyright-requests@swiss-ai.org Output Filter for PII Currently no output filter is provided. Please check this site regularly for an output filter that can be used on top of the Apertus LLM. The filter reflects data protection deletion requests which have been addressed to us as the developer of the Apertus LLM. It allows you to remove Personal Data contained in the model output. We strongly advise downloading and applying this output filter from this site every six months. Contact To contact us, please send an email to llm-requests@swiss-ai.org Citation @misc{swissai2025apertus, title={{Apertus: Democratizing Open and Compliant LLMs for Global Language Environments}}, author={Apertus Team}, year={2025}, howpublished={\\url{https://huggingface.co/swiss-ai/Apertus-70B-2509}} } Downloads last month29,064 Safetensors Model size 70.6B params Tensor type BF16 ¬∑ Chat template Files info Inference Providers NEW Text Generation This model isn't deployed by any Inference Provider. üôã 10 Ask for provider support Model tree for swiss-ai/Apertus-70B-Instruct-2509 Base model swiss-ai/Apertus-70B-2509 Finetuned (2) this model Finetunes 2 models Collection including swiss-ai/Apertus-70B-Instruct-2509 Apertus LLM Collection 4 items ‚Ä¢ Updated 6 days ago ‚Ä¢ 213 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs",
    "id": "ab5860ec-2e5e-4863-939a-5e4cf7d5f895",
    "metadata": {
      "filename": "Website: https://huggingface.co",
      "file_hash": "78c9f4b04c0dadd88efddd2d3b482b2c",
      "source": "website",
      "processed_at": "2025-09-08T23:01:36.343673"
    }
  }
]